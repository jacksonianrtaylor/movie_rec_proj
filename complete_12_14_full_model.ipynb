{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data source\n",
    "\n",
    "* Download the data needed for this jupyter notebook from kaggle and store it in a new folder (the-movies-dataset) in the current directory.\n",
    "\n",
    "\n",
    "* Upon running this cell, the user will be asked for their username and key which can be found in a fresh api token from kaggle.\n",
    "\n",
    "* Instructions to get api token to authenticate the data request (Note: kaggle account required):\n",
    "    1. Sign into kaggle.\n",
    "    2. Go to the 'Account' tab of your user profile and select 'Create New Token'. \n",
    "    3. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
    "\n",
    "* If the folder has been created and the files are already in that folder, than this cell does nothing and requires no credentials.\n",
    "\n",
    "* Data Source Information: https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Kaggle Key:Downloading the-movies-dataset.zip to ./the-movies-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228M/228M [00:23<00:00, 10.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Raw Data\n",
    "\n",
    "Combining certain data from the necessary csv files into a single dataframe (complete_df).\n",
    "\n",
    "* Rows are removed from each dataframe when they do not have sufficent data for a column or the data from a column does not exist.\n",
    "* This kind of row removal is done before multiple copies of the same movie data becomes present in multple rows, to save time and space.\n",
    "* Iteration through rows of a dataframe at this level is inefficient compared to list iteration.\n",
    "* This is why the dataframes are converted into lists before iteration and then back again to dataframes, so the merge function can be applied to combine the data into a single dataframe (complete_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3276\n",
      "Minutes taken: 0.6035405238469441\n",
      "        userId    id rating               title  \\\n",
      "6566765      1  1246    5.0        Rocky Balboa   \n",
      "6880303      1  2959    4.0      License to Wed   \n",
      "2083077      1  2762    4.5  Young and Innocent   \n",
      "1492304      1  1968    4.0       Fools Rush In   \n",
      "2638962      1   147    4.5       The 400 Blows   \n",
      "\n",
      "                                                                                                genres  \\\n",
      "6566765                                                                  [{'id': 18, 'name': 'Drama'}]   \n",
      "6880303                                                                 [{'id': 35, 'name': 'Comedy'}]   \n",
      "2083077                                     [{'id': 18, 'name': 'Drama'}, {'id': 80, 'name': 'Crime'}]   \n",
      "1492304  [{'id': 18, 'name': 'Drama'}, {'id': 35, 'name': 'Comedy'}, {'id': 10749, 'name': 'Romance'}]   \n",
      "2638962                                                                  [{'id': 18, 'name': 'Drama'}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                production_companies  \\\n",
      "6566765                                                                                                  [{'name': 'Columbia Pictures', 'id': 5}, {'name': 'Revolution Studios', 'id': 497}, {'name': 'Rogue Marble', 'id': 696}, {'name': 'Metro-Goldwyn-Mayer (MGM)', 'id': 8411}]   \n",
      "6880303  [{'name': 'Village Roadshow Pictures', 'id': 79}, {'name': 'Robert Simonds Productions', 'id': 3929}, {'name': 'Warner Bros.', 'id': 6194}, {'name': 'Phoenix Pictures', 'id': 11317}, {'name': 'Underground', 'id': 49326}, {'name': 'Proposal Productions', 'id': 49327}]   \n",
      "2083077                                                                                                                                                                                                                [{'name': 'Gaumont British Picture Corporation', 'id': 4978}]   \n",
      "1492304                                                                                                                                                                                                                                     [{'name': 'Columbia Pictures', 'id': 5}]   \n",
      "2638962                                                                                                                                 [{'name': 'Les Films du Carrosse', 'id': 53}, {'name': 'Sédif Productions', 'id': 10897}, {'name': 'The Criterion Collection', 'id': 10932}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       keywords  \\\n",
      "6566765  [{'id': 276, 'name': 'philadelphia'}, {'id': 396, 'name': 'transporter'}, {'id': 1721, 'name': 'fight'}, {'id': 2038, 'name': \"love of one's life\"}, {'id': 2416, 'name': 'publicity'}, {'id': 2792, 'name': 'boxer'}, {'id': 2968, 'name': 'grave'}, {'id': 3393, 'name': 'tombstone'}, {'id': 3586, 'name': 'tv station'}, {'id': 4487, 'name': 'boxing match'}, {'id': 4610, 'name': 'comeback'}, {'id': 4613, 'name': 'training'}, {'id': 5167, 'name': 'restaurant owner'}, {'id': 5378, 'name': 'world champion'}, {'id': 5379, 'name': 'challenger'}, {'id': 5380, 'name': 'virtual fight'}, {'id': 6066, 'name': 'defeat'}, {'id': 6067, 'name': 'victory'}, {'id': 10163, 'name': 'cancer'}, {'id': 155464, 'name': 'over-the-hill fighter'}]   \n",
      "6880303                                                                                                                                                                                                                                                                                                                                             [{'id': 1605, 'name': 'new love'}, {'id': 2856, 'name': 'ten commandments'}, {'id': 3582, 'name': 'bride'}, {'id': 3583, 'name': 'bridegroom'}, {'id': 6038, 'name': 'marriage'}, {'id': 6192, 'name': 'relation'}, {'id': 6281, 'name': 'partnership'}, {'id': 6704, 'name': 'civil registry office'}, {'id': 10093, 'name': 'priest'}, {'id': 13027, 'name': 'wedding'}, {'id': 14765, 'name': 'church'}]   \n",
      "2083077                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [{'id': 769, 'name': 'falsely accused'}, {'id': 1655, 'name': 'country house'}, {'id': 9826, 'name': 'murder'}, {'id': 9937, 'name': 'suspense'}]   \n",
      "1492304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [{'id': 828, 'name': 'waitress'}, {'id': 1463, 'name': 'culture clash'}, {'id': 9799, 'name': 'romantic comedy'}, {'id': 13149, 'name': 'pregnancy'}]   \n",
      "2638962                                                                                                                                                                                                                                                                                                                                                                      [{'id': 6930, 'name': 'fondling'}, {'id': 10183, 'name': 'independent film'}, {'id': 155518, 'name': 'nouvelle vague'}, {'id': 170268, 'name': 'skipping school'}, {'id': 170272, 'name': 'mise en scene'}, {'id': 170273, 'name': 'fingerprinting'}, {'id': 170279, 'name': '\\xa0mugshot'}, {'id': 170286, 'name': 'strict teacher'}, {'id': 170293, 'name': 'montmartre paris'}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        cast  \\\n",
      "6566765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [{'cast_id': 24, 'character': 'Rocky Balboa', 'credit_id': '52fe42e9c3a36847f802c61b', 'gender': 2, 'id': 16483, 'name': 'Sylvester Stallone', 'order': 0, 'profile_path': '/gnmwOa46C2TP35N7ARSzboTdx2u.jpg'}, {'cast_id': 25, 'character': 'Paulie', 'credit_id': '52fe42e9c3a36847f802c61f', 'gender': 2, 'id': 4521, 'name': 'Burt Young', 'order': 1, 'profile_path': '/rbSsSkQ72FoGcvwIHUxQWJ92I3W.jpg'}, {'cast_id': 26, 'character': 'Rocky Jr.', 'credit_id': '52fe42e9c3a36847f802c623', 'gender': 2, 'id': 16501, 'name': 'Milo Ventimiglia', 'order': 2, 'profile_path': '/maJeS6bA6ku21rSRceISQtwHL2h.jpg'}, {'cast_id': 27, 'character': 'Marie', 'credit_id': '52fe42e9c3a36847f802c627', 'gender': 1, 'id': 16502, 'name': 'Geraldine Hughes', 'order': 3, 'profile_path': '/bTXux3EJq25Fh2ixbet6MjdG3Fb.jpg'}, {'cast_id': 28, 'character': 'Steps', 'credit_id': '52fe42e9c3a36847f802c62b', 'gender': 2, 'id': 16503, 'name': 'James Francis Kelly III', 'order': 4, 'profile_path': '/iZyTQ2UlwNXrqLqPeNHbofFXubP.jpg'}, {'cast_id': 29, 'character': 'Duke', 'credit_id': '52fe42e9c3a36847f802c62f', 'gender': 2, 'id': 16504, 'name': 'Tony Burton', 'order': 5, 'profile_path': '/ue54hK217thXeQMzN4qUYXLImLd.jpg'}, {'cast_id': 30, 'character': 'L.C.', 'credit_id': '52fe42e9c3a36847f802c633', 'gender': 2, 'id': 16505, 'name': 'A. J. Benza', 'order': 6, 'profile_path': '/5hVinC6C1ZyD7c8EmZFTiEaF7vH.jpg'}, {'cast_id': 31, 'character': 'Adrian', 'credit_id': '52fe42e9c3a36847f802c637', 'gender': 1, 'id': 3094, 'name': 'Talia Shire', 'order': 7, 'profile_path': '/liNfrVB3eZFBOjcUGltISCucews.jpg'}, {'cast_id': 32, 'character': 'Martin', 'credit_id': '52fe42e9c3a36847f802c63b', 'gender': 2, 'id': 16506, 'name': 'Henry G. Sanders', 'order': 8, 'profile_path': '/2SU75g2CAIzGWbgfIlNvKZQhYTZ.jpg'}, {'cast_id': 33, 'character': \"Mason 'The Line' Dixon\", 'credit_id': '52fe42e9c3a36847f802c63f', 'gender': 2, 'id': 16507, 'name': 'Antonio Tarver', 'order': 9, 'profile_path': '/kJEljjHwBvrjoxqcSVntXlejgl1.jpg'}, {'cast_id': 34, 'character': 'Spider Rico', 'credit_id': '52fe42e9c3a36847f802c643', 'gender': 2, 'id': 16508, 'name': 'Pedro Lovell', 'order': 10, 'profile_path': None}, {'cast_id': 35, 'character': 'Isabel', 'credit_id': '52fe42e9c3a36847f802c647', 'gender': 1, 'id': 16509, 'name': 'Ana Gerena', 'order': 11, 'profile_path': None}, {'cast_id': 36, 'character': 'Angie', 'credit_id': '52fe42e9c3a36847f802c64b', 'gender': 1, 'id': 16510, 'name': 'Angela Boyd', 'order': 12, 'profile_path': None}, {'cast_id': 37, 'character': 'Bar Thug', 'credit_id': '52fe42e9c3a36847f802c64f', 'gender': 0, 'id': 16511, 'name': 'Louis Giansante', 'order': 13, 'profile_path': None}, {'cast_id': 38, 'character': \"Lucky's Bartender\", 'credit_id': '52fe42e9c3a36847f802c653', 'gender': 0, 'id': 16512, 'name': 'Maureen Schilling', 'order': 14, 'profile_path': None}, {'cast_id': 40, 'character': 'X-Cell', 'credit_id': '5761db05c3a3682f20000302', 'gender': 2, 'id': 98298, 'name': 'Lahmard J. Tate', 'order': 15, 'profile_path': '/4WcFReePSxyGQJWV5wXGNfY0Y7o.jpg'}]   \n",
      "6880303                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [{'cast_id': 18, 'character': 'Reverend Frank', 'credit_id': '52fe4376c3a36847f8056039', 'gender': 2, 'id': 2157, 'name': 'Robin Williams', 'order': 0, 'profile_path': '/sojtJyIV3lkUeThD7A2oHNm8183.jpg'}, {'cast_id': 19, 'character': 'Sadie Jones', 'credit_id': '52fe4376c3a36847f805603d', 'gender': 1, 'id': 16855, 'name': 'Mandy Moore', 'order': 1, 'profile_path': '/15sDtRpe301tZWrRYV31wjMuFpx.jpg'}, {'cast_id': 20, 'character': 'Ben Murphy', 'credit_id': '52fe4376c3a36847f8056041', 'gender': 2, 'id': 17697, 'name': 'John Krasinski', 'order': 2, 'profile_path': '/nOWwdZURikW22qo6OUSGFCTukgc.jpg'}, {'cast_id': 21, 'character': 'Carlisle', 'credit_id': '52fe4376c3a36847f8056045', 'gender': 2, 'id': 29020, 'name': 'Eric Christian Olsen', 'order': 3, 'profile_path': '/clbouet8o9IJlUd8WILD0lzHAtG.jpg'}, {'cast_id': 22, 'character': 'Lindsey Jones', 'credit_id': '52fe4376c3a36847f8056049', 'gender': 1, 'id': 15286, 'name': 'Christine Taylor', 'order': 4, 'profile_path': '/99OssnGmgGjduXFA7syxjNqt9tQ.jpg'}, {'cast_id': 23, 'character': 'Choir Boy', 'credit_id': '52fe4376c3a36847f805604d', 'gender': 2, 'id': 216, 'name': 'Josh Flitter', 'order': 5, 'profile_path': '/6RCA8tDWBxIVk9N3IqUjJEAzYGv.jpg'}, {'cast_id': 24, 'character': 'Joel', 'credit_id': '52fe4376c3a36847f8056051', 'gender': 2, 'id': 11827, 'name': 'DeRay Davis', 'order': 6, 'profile_path': '/w2JYPRLwXhNCpxpJc2v4UQYyMv8.jpg'}, {'cast_id': 25, 'character': 'Mr. Jones', 'credit_id': '52fe4376c3a36847f8056055', 'gender': 2, 'id': 21368, 'name': 'Peter Strauss', 'order': 7, 'profile_path': '/ufx1trct43k7UcT4DpoIMPZXi5A.jpg'}, {'cast_id': 26, 'character': 'Grandma Jones', 'credit_id': '52fe4376c3a36847f8056059', 'gender': 1, 'id': 6465, 'name': 'Grace Zabriskie', 'order': 8, 'profile_path': '/ibBabuSM1UyPYFFo0wBXhGbqElk.jpg'}, {'cast_id': 27, 'character': 'Mrs. Jones', 'credit_id': '52fe4376c3a36847f805605d', 'gender': 1, 'id': 29021, 'name': 'Roxanne Hart', 'order': 9, 'profile_path': '/yWGMW6HdhUGT2oIcQ4jmnkw7ZAM.jpg'}, {'cast_id': 28, 'character': 'Shelly', 'credit_id': '5586ee469251417f6f0059c8', 'gender': 1, 'id': 125167, 'name': 'Mindy Kaling', 'order': 10, 'profile_path': '/Agpd4tJyZ95hk74RifjnfnJpn9U.jpg'}, {'cast_id': 30, 'character': 'Expectant Father', 'credit_id': '56c3467cc3a36847c5001f66', 'gender': 2, 'id': 1368801, 'name': 'David Quinlan', 'order': 11, 'profile_path': '/2m75rrBhvOTtdUS9jlKW8GOHCBV.jpg'}, {'cast_id': 31, 'character': 'Judith', 'credit_id': '58e26093c3a36872f600dcf2', 'gender': 1, 'id': 113867, 'name': 'Angela Kinsey', 'order': 12, 'profile_path': '/omLdRLdwMLliVeVIualEnWVhm1a.jpg'}]   \n",
      "2083077                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [{'cast_id': 18, 'character': 'Erica Burgoyne', 'credit_id': '52fe436bc3a36847f8052cd5', 'gender': 1, 'id': 27939, 'name': 'Nova Pilbeam', 'order': 0, 'profile_path': '/l6oHJaRYrVxsvoTSmMS5wIXaei5.jpg'}, {'cast_id': 19, 'character': 'Robert Tisdall', 'credit_id': '52fe436bc3a36847f8052cd9', 'gender': 0, 'id': 27940, 'name': 'Derrick De Marney', 'order': 1, 'profile_path': '/7VRZ7K0EZ50haOlbVr7DHZ5550O.jpg'}, {'cast_id': 20, 'character': 'Col. Burgoyne', 'credit_id': '52fe436bc3a36847f8052cdd', 'gender': 2, 'id': 27929, 'name': 'Percy Marmont', 'order': 2, 'profile_path': '/p3DIyvlxx6B0SVIxcDaPUPlEV0U.jpg'}, {'cast_id': 21, 'character': 'Old Will', 'credit_id': '52fe436bc3a36847f8052ce1', 'gender': 2, 'id': 27941, 'name': 'Edward Rigby', 'order': 3, 'profile_path': '/B7GJ0jPtODqZVgVtZHPtvZl2tO.jpg'}, {'cast_id': 22, 'character': 'Ericas Tante Margaret', 'credit_id': '52fe436bc3a36847f8052ce5', 'gender': 1, 'id': 14304, 'name': 'Mary Clare', 'order': 4, 'profile_path': '/lAdEwCGiSUj9CCMPB4L9X4oujLe.jpg'}, {'cast_id': 23, 'character': 'Det. Insp. Kent', 'credit_id': '52fe436bc3a36847f8052ce9', 'gender': 2, 'id': 7383, 'name': 'John Longden', 'order': 5, 'profile_path': '/rsCoUEx2ThNIz12fBR6vPncCICk.jpg'}, {'cast_id': 24, 'character': 'Guy', 'credit_id': '52fe436bc3a36847f8052ced', 'gender': 2, 'id': 27942, 'name': 'George Curzon', 'order': 6, 'profile_path': None}, {'cast_id': 25, 'character': 'Ericas Onkel Basil', 'credit_id': '52fe436bc3a36847f8052cf1', 'gender': 2, 'id': 14303, 'name': 'Basil Radford', 'order': 7, 'profile_path': '/9STo7Tgdutplo78ZtyeINGWkXUk.jpg'}, {'cast_id': 26, 'character': 'Christine Clay', 'credit_id': '52fe436bc3a36847f8052cf5', 'gender': 1, 'id': 27943, 'name': 'Pamela Carme', 'order': 8, 'profile_path': None}, {'cast_id': 27, 'character': 'Detective Sergeant Miller', 'credit_id': '52fe436bc3a36847f8052cf9', 'gender': 2, 'id': 27944, 'name': 'George Merritt', 'order': 9, 'profile_path': None}, {'cast_id': 28, 'character': 'Henry Briggs', 'credit_id': '52fe436bc3a36847f8052cfd', 'gender': 2, 'id': 27945, 'name': 'J.H. Roberts', 'order': 10, 'profile_path': None}, {'cast_id': 29, 'character': \"Truckfahrer bei Tom's Hat\", 'credit_id': '52fe436bc3a36847f8052d01', 'gender': 2, 'id': 27946, 'name': 'Jerry Verno', 'order': 11, 'profile_path': None}, {'cast_id': 30, 'character': 'Police Sergeant Ruddock', 'credit_id': '52fe436bc3a36847f8052d05', 'gender': 2, 'id': 27947, 'name': 'H.F. Maltby', 'order': 12, 'profile_path': None}, {'cast_id': 31, 'character': 'Police Constable', 'credit_id': '52fe436bc3a36847f8052d09', 'gender': 2, 'id': 27948, 'name': 'John Miller', 'order': 13, 'profile_path': None}]   \n",
      "1492304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [{'cast_id': 2, 'character': 'Alex Whitman', 'credit_id': '52fe4327c3a36847f803e629', 'gender': 2, 'id': 14408, 'name': 'Matthew Perry', 'order': 0, 'profile_path': '/oSKEEDXDNnwWdQ68qfDVD6Q7Pxp.jpg'}, {'cast_id': 3, 'character': 'Isabel Fuentes', 'credit_id': '52fe4327c3a36847f803e62d', 'gender': 1, 'id': 3136, 'name': 'Salma Hayek', 'order': 1, 'profile_path': '/u5mg73xKVqm8oT93HoMmsgQHyoK.jpg'}, {'cast_id': 4, 'character': 'Jeff', 'credit_id': '52fe4327c3a36847f803e631', 'gender': 2, 'id': 4602, 'name': 'Jon Tenney', 'order': 2, 'profile_path': '/fiG1bW6DX1szsRDPIYjfIKPQ0kV.jpg'}, {'cast_id': 5, 'character': 'Lanie', 'credit_id': '52fe4327c3a36847f803e635', 'gender': 1, 'id': 6751, 'name': 'Siobhan Fallon', 'order': 3, 'profile_path': '/wVFa8GiY0xdOLFsvGygy9RMtcBc.jpg'}, {'cast_id': 16, 'character': 'Great Grandma', 'credit_id': '52fe4327c3a36847f803e675', 'gender': 1, 'id': 20360, 'name': 'Angelina Torres', 'order': 4, 'profile_path': None}, {'cast_id': 17, 'character': 'Richard Whitman', 'credit_id': '52fe4327c3a36847f803e679', 'gender': 2, 'id': 20361, 'name': 'John Bennett Perry', 'order': 5, 'profile_path': '/bzFhwuXsdZiOHRtBgz4XVELIFYO.jpg'}, {'cast_id': 18, 'character': 'Nan Whitman', 'credit_id': '52fe4327c3a36847f803e67d', 'gender': 1, 'id': 20362, 'name': 'Jill Clayburgh', 'order': 6, 'profile_path': '/twrfhIvbqHuJ7nXVpehvU6nyi6R.jpg'}, {'cast_id': 19, 'character': 'Cathy Stewart', 'credit_id': '52fe4327c3a36847f803e681', 'gender': 1, 'id': 20363, 'name': 'Suzanne Snyder', 'order': 7, 'profile_path': '/90FrTcjJudpeIYUjUzlO6XAmvnt.jpg'}, {'cast_id': 20, 'character': 'Amalia', 'credit_id': '52fe4327c3a36847f803e685', 'gender': 0, 'id': 13029, 'name': 'Anne Betancourt', 'order': 8, 'profile_path': '/6UU5P4DzjJTSBFztIu1nALT2tk0.jpg'}, {'cast_id': 21, 'character': 'Juan Fuentes', 'credit_id': '52fe4327c3a36847f803e689', 'gender': 2, 'id': 4511, 'name': 'Mark Adair-Rios', 'order': 9, 'profile_path': '/rX4d1e5jlF5P73qynjjUzJslB0c.jpg'}, {'cast_id': 22, 'character': 'Judd Marshall', 'credit_id': '52fe4327c3a36847f803e68d', 'gender': 2, 'id': 4171, 'name': 'Stanley DeSantis', 'order': 10, 'profile_path': '/4cHxkhTd7oklyNkdva9WJp0FLrX.jpg'}, {'cast_id': 23, 'character': 'Antonio Fuentes', 'credit_id': '52fe4327c3a36847f803e691', 'gender': 0, 'id': 4665, 'name': 'Josh Cruze', 'order': 11, 'profile_path': '/v3QrQzH0uGV9pd1dNR5Ue6a74qO.jpg'}, {'cast_id': 24, 'character': 'Petra', 'credit_id': '52fe4327c3a36847f803e695', 'gender': 0, 'id': 4666, 'name': 'Angela Lanza', 'order': 12, 'profile_path': '/zmf6TMWMVCdnuUfpgdnioaICk1L.jpg'}, {'cast_id': 25, 'character': 'Phil', 'credit_id': '52fe4327c3a36847f803e699', 'gender': 2, 'id': 4445, 'name': 'Chris Bauer', 'order': 13, 'profile_path': '/3KYVMaGkWTEDQ0T9lsu85pVbP4T.jpg'}, {'cast_id': 26, 'character': 'Chuy', 'credit_id': '577e438f925141440c000d63', 'gender': 0, 'id': 115874, 'name': 'Carlos Gómez', 'order': 14, 'profile_path': '/nBxwoMv1zrhNXyEjYXbcdmAdmF0.jpg'}]   \n",
      "2638962  [{'cast_id': 6, 'character': 'Antoine Doinel', 'credit_id': '52fe421ec3a36847f8005661', 'gender': 2, 'id': 1653, 'name': 'Jean-Pierre Léaud', 'order': 0, 'profile_path': '/dzkPODapVe4CSubEqI9ytTCqnZ7.jpg'}, {'cast_id': 7, 'character': 'Gilberte Doinel', 'credit_id': '52fe421ec3a36847f8005665', 'gender': 1, 'id': 1654, 'name': 'Claire Maurier', 'order': 1, 'profile_path': '/cP1n7zMsMKr77xJeR3CncomxEZ0.jpg'}, {'cast_id': 8, 'character': 'Julien Doinel', 'credit_id': '52fe421ec3a36847f8005669', 'gender': 0, 'id': 1655, 'name': 'Albert Rémy', 'order': 2, 'profile_path': '/6b8eyIXAV6oA5eX6ltc3hF7ZB3d.jpg'}, {'cast_id': 10, 'character': 'Mr. Bigey', 'credit_id': '52fe421ec3a36847f8005673', 'gender': 2, 'id': 1658, 'name': 'Georges Flamant', 'order': 3, 'profile_path': '/lQwmtPsFWME63x5M7IRF6g8bLrR.jpg'}, {'cast_id': 11, 'character': 'René', 'credit_id': '52fe421ec3a36847f8005677', 'gender': 0, 'id': 1659, 'name': 'Patrick Auffay', 'order': 4, 'profile_path': None}, {'cast_id': 12, 'character': 'Director of the school', 'credit_id': '52fe421ec3a36847f800567b', 'gender': 0, 'id': 1660, 'name': 'Robert Beauvais', 'order': 5, 'profile_path': None}, {'cast_id': 13, 'character': 'Mme Bigey', 'credit_id': '52fe421ec3a36847f800567f', 'gender': 0, 'id': 1661, 'name': 'Yvonne Claudie', 'order': 6, 'profile_path': None}, {'cast_id': 14, 'character': 'English Teacher', 'credit_id': '52fe421ec3a36847f8005683', 'gender': 0, 'id': 1662, 'name': 'Pierre Repp', 'order': 7, 'profile_path': '/1AUhiNGBAR0C6AU9iK1IXBs3QTz.jpg'}, {'cast_id': 17, 'character': 'French Teacher', 'credit_id': '52fe421ec3a36847f8005693', 'gender': 0, 'id': 1656, 'name': 'Guy Decomble', 'order': 8, 'profile_path': '/34iexAuqI1asyFounbSXSCFphen.jpg'}, {'cast_id': 20, 'character': 'Betrand Mauricet', 'credit_id': '52fe421ec3a36847f8005697', 'gender': 0, 'id': 1077237, 'name': 'Daniel Couturier', 'order': 9, 'profile_path': None}, {'cast_id': 21, 'character': 'Child', 'credit_id': '52fe421ec3a36847f800569b', 'gender': 0, 'id': 1077238, 'name': 'François Nocher', 'order': 10, 'profile_path': None}, {'cast_id': 22, 'character': 'Child', 'credit_id': '52fe421ec3a36847f800569f', 'gender': 2, 'id': 150939, 'name': 'Richard Kanayan', 'order': 11, 'profile_path': '/vCMDk3ifj2vJKZYCISXT3K6DYXF.jpg'}, {'cast_id': 23, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056a3', 'gender': 0, 'id': 1077239, 'name': 'Renaud Fontanarosa', 'order': 12, 'profile_path': None}, {'cast_id': 24, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056a7', 'gender': 0, 'id': 1077240, 'name': 'Michel Girard', 'order': 13, 'profile_path': None}, {'cast_id': 25, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056ab', 'gender': 0, 'id': 71997, 'name': 'Serge Moati', 'order': 14, 'profile_path': '/wccRQKHrX61sH4WlOtM1KBP4qaq.jpg'}, {'cast_id': 26, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056af', 'gender': 0, 'id': 1077241, 'name': 'Bernard Abbou', 'order': 15, 'profile_path': None}, {'cast_id': 27, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056b3', 'gender': 0, 'id': 1077242, 'name': 'Jean-François Bergouignan', 'order': 16, 'profile_path': None}, {'cast_id': 28, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056b7', 'gender': 0, 'id': 1077243, 'name': 'Michel Lesignor', 'order': 17, 'profile_path': None}, {'cast_id': 31, 'character': 'Man in Street', 'credit_id': '5457f0a1c3a3683993000156', 'gender': 2, 'id': 24299, 'name': 'Jean-Claude Brialy', 'order': 18, 'profile_path': '/g3kkYcAvq90tALMErxmdAIcIXsE.jpg'}, {'cast_id': 32, 'character': 'Woman with Dog', 'credit_id': '5457f0bec3a36839a0000144', 'gender': 1, 'id': 14812, 'name': 'Jeanne Moreau', 'order': 19, 'profile_path': '/uHJnVwCzehEoz0mIlwN7xkymql8.jpg'}, {'cast_id': 33, 'character': 'Man in Funfair', 'credit_id': '5457f0d3c3a368399300015b', 'gender': 2, 'id': 34613, 'name': 'Philippe de Broca', 'order': 20, 'profile_path': '/yrvmXE2SJBX659r2Y7eWwlmwfYd.jpg'}, {'cast_id': 34, 'character': 'Man in Funfair', 'credit_id': '5457f0e5c3a368399d00014c', 'gender': 0, 'id': 1650, 'name': 'François Truffaut', 'order': 21, 'profile_path': '/apCCV99N3FqB5NsEPqOzetlkprL.jpg'}]   \n",
      "\n",
      "                                                                               tagline  \\\n",
      "6566765                                                  It ain't over 'til it's over.   \n",
      "6880303                                   First came love... then came Reverend Frank.   \n",
      "2083077                                                          A Brilliant Melodrama   \n",
      "1492304  What if finding the love of your life meant changing the life that you loved?   \n",
      "2638962                                            Angel faces hell-bent for violence.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        overview  \n",
      "6566765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              When he loses a highly publicized virtual boxing match to ex-champ Rocky Balboa, reigning heavyweight titleholder, Mason Dixon retaliates by challenging Rocky to a nationally televised, 10-round exhibition bout. To the surprise of his son and friends, Rocky agrees to come out of retirement and face an opponent who's faster, stronger and thirty years his junior.  \n",
      "6880303                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Newly engaged, Ben and Sadie can't wait to start their life together and live happily ever after. However Sadie's family church's Reverend Frank won't bless their union until they pass his patented, \"foolproof\" marriage prep course consisting of outrageous classes, outlandish homework assignments and some outright invasion of privacy.  \n",
      "2083077  Derrick De Marney finds himself in a 39 Steps situation when he is wrongly accused of murder. While a fugitive from the law, De Marney is helped by heroine Nova Pilbeam, who three years earlier had played the adolescent kidnap victim in Hitchcock's The Man Who Knew Too Much. The obligatory \"fish out of water\" scene, in which the principals are briefly slowed down by a banal everyday event, occurs during a child's birthday party. The actual villain, whose identity is never in doubt (Hitchcock made thrillers, not mysteries) is played by George Curzon, who suffers from a twitching eye. Curzon's revelation during an elaborate nightclub sequence is a Hitchcockian tour de force, the sort of virtuoso sequence taken for granted in these days of flexible cameras and computer enhancement, but which in 1937 took a great deal of time, patience and talent to pull off. Released in the US as The Girl Was Young, Young and Innocent was based on a novel by Josephine Tey.  \n",
      "1492304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Alex Whitman (Matthew Perry) is a designer from New York City who is sent to Las Vegas to supervise the construction of a nightclub that his firm has been hired to build. Alex is a straight-laced WASP-ish type who, while enjoying a night on the town, meets Isabel Fuentes (Salma Hayek), a free-spirited Mexican-American photographer. Alex and Isabel are overtaken by lust at first sight and end up sp  \n",
      "2638962                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     For young Parisian boy Antoine Doinel, life is one difficult situation after another. Surrounded by inconsiderate adults, including his neglectful parents, Antoine spends his days with his best friend, Rene, trying to plan for a better life. When one of their schemes goes awry, Antoine ends up in trouble with the law, leading to even more conflicts with unsympathetic authority figures.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_df = pd.read_csv('./the-movies-dataset/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={'genres':\"string\",\"id\":\"string\",\"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\",\n",
    "                                    \"production_companies\" :\"string\"})[[\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"]]\n",
    "movies_df.dropna(inplace = True)\n",
    "movies_lst = [row for row in movies_df.values.tolist() if not (row[0][len(row[0])  - 2:] == \"[]\" or row[5][len(row[5]) - 2:] == \"[]\")]\n",
    "movies_df = pd.DataFrame(movies_lst, columns = (\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"), dtype = str)\n",
    "\n",
    "\n",
    "\n",
    "ratings_df = pd.read_csv('./the-movies-dataset/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"),\n",
    "                       dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})[[\"userId\", \"movieId\", \"rating\"]]\n",
    "ratings_df.rename(columns={\"movieId\": \"id\"}, inplace = True)\n",
    "ratings_df.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Question: What if the removal of duplicate movie ids per user was processed here instead of the cell below???\n",
    "# Answer: The duplicate removal function can be ran here,...\n",
    "# but the complete_list in the cell below can also be iterated over with relative complexity in order to remove duplicates.\n",
    "# The iteration in the next cell also populates the gap list...\n",
    "# which is critical to be ran directly before the function that determines bounds for users rated movies.\n",
    "# So, omitting the no duplicate function in this cell and making it run in the next cell avoids redundant iteration.\n",
    "\n",
    "\n",
    "# Question: What if the test and train ratings bounds was enforced here instead of the cell below???\n",
    "# Answer: The merge functions below needs to be executed before determining test and train users, because merge will remove rows and ratings from users...\n",
    "# before enforcing the users to be in a certain bounds for the number of their ratings. \n",
    "# The current timing of this function will ensure that the final users are within the set train or test bounds.\n",
    "\n",
    "\n",
    "keywords_df = pd.read_csv('./the-movies-dataset/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})[[\"id\", \"keywords\"]]\n",
    "keywords_df.dropna(inplace = True)\n",
    "keywords_lst = [row for row in keywords_df.values.tolist() if not (row[1][len(row[1])  - 2:] == \"[]\")]\n",
    "keywords_df = pd.DataFrame(keywords_lst, columns = (\"id\", \"keywords\"), dtype = str)\n",
    "\n",
    "\n",
    "credits_df = pd.read_csv(\"./the-movies-dataset/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})[[\"cast\", \"id\"]]\n",
    "credits_df.dropna(inplace = True)\n",
    "credits_lst = [row for row in credits_df.values.tolist() if (not row[0][len(row[0])  - 2:] == \"[]\")]\n",
    "credits_df = pd.DataFrame(credits_lst, columns = (\"cast\", \"id\"), dtype = str)\n",
    "\n",
    "\n",
    "# Default merge is inner: This only keeps movies that have the id existing in both dataframes.\n",
    "complete_df =  pd.merge(movies_df, ratings_df, on =\"id\")\n",
    "complete_df =  pd.merge(complete_df,keywords_df, on =\"id\")\n",
    "complete_df  = pd.merge(complete_df,credits_df, on =\"id\")\n",
    "\n",
    "\n",
    "complete_df.sort_values(by = 'userId', inplace = True)\n",
    "\n",
    "\n",
    "# Master dataframe: For each (user id, movie id) row combination there is the combined movie data from movies_df, ratings_df, keywords_df, and credits_df for the movie id in question.\n",
    "# The columns are reordered.\n",
    "complete_df  = complete_df.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "print(len(complete_df[\"id\"].unique()))\n",
    "\n",
    "# For testing:\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "print(complete_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# Tested on personal machine:\n",
    "# Old run with dataframe iteration (old code): 1 minute and 5.7 seconds\n",
    "# New run with list conversion before iteration (current code): 37.1 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Selection\n",
    "1. Select data from users that have a number of ratings within a certain bounds.\n",
    "2. Select a random subset of this data and simplify it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete number of users: 260788\n",
      "met\n",
      "met\n",
      "met\n",
      "1000\n",
      "800\n",
      "800\n",
      "Minutes taken: 0.7150333484013875\n",
      "36113\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import hist\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "SEED_INT = 1\n",
    "# Seed for consistent results across runtimes:\n",
    "random.seed(SEED_INT)\n",
    "\n",
    "\n",
    "def populate_names(item):\n",
    "    \"\"\"Extract names from the syntax of certain data entries:\"\"\"\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    index = 0\n",
    "    for item in jsons:\n",
    "        if(index == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        index += 1\n",
    "    return names\n",
    "\n",
    "\n",
    "def provide_data(row):\n",
    "    \"\"\"Extract data from row of complete_list:\"\"\"\n",
    "    movie_data = []\n",
    "    movie_data.append(int(row[0]))\n",
    "    movie_data.append(int(row[1]))\n",
    "    movie_data.append(float(row[2]))\n",
    "    movie_data.append(row[3])  \n",
    "\n",
    "    movie_data.append(populate_names(row[4]))\n",
    "    movie_data.append(populate_names(row[5]))\n",
    "    movie_data.append(populate_names(row[6]))\n",
    "    movie_data.append(populate_names(row[7]))\n",
    "\n",
    "    movie_data.append(str(row[8]))\n",
    "    movie_data.append(str(row[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "# The list of rows with users id, the users rating for the movie, and raw data for the movie:\n",
    "# Note: It is sorted by user_id.\n",
    "complete_list = complete_df.values.tolist()\n",
    "\n",
    "print(\"Complete number of users:\", len(list(complete_df[\"userId\"].unique()))) # 260788\n",
    "\n",
    "# The complete list of user rows without ratings of the same movie more than once for a given user:\n",
    "complete_list_no_dups = []\n",
    "\n",
    "# Distinquish the user the row belongs to:\n",
    "last_id = complete_list[0][0]\n",
    "\n",
    "# The set of movies that a user has rated:\n",
    "# It is used to omit later ratings of a movie that the user has already rated.\n",
    "movie_set = set()\n",
    "\n",
    "# The number of rows of movie data a single user takes up for each user:\n",
    "gaps = []\n",
    "\n",
    "# Appended to gaps when all of a users rows of movie data have been counted:\n",
    "gap_len = 0\n",
    "\n",
    "\n",
    "# Populates gaps and complete_list_no_dups by omitting movies that already have a rating in respect to each user:\n",
    "# Note: This code is faster than using dataframe methods.\n",
    "# Example: Filter data by user and then remove duplicate movie ids for each user.\n",
    "# This avoids slow dataframe iteration, but the filter method is also slow.\n",
    "for row in complete_list:\n",
    "    if last_id != row[0]:\n",
    "        movie_set= set()\n",
    "        complete_list_no_dups.append(row)\n",
    "        movie_set.add(row[1])\n",
    "        gaps.append(gap_len)\n",
    "        gap_len = 1\n",
    "    else:\n",
    "        if row[1] not in movie_set:\n",
    "            complete_list_no_dups.append(row)\n",
    "            gap_len+=1\n",
    "            movie_set.add(row[1])\n",
    "    last_id = row[0]\n",
    "\n",
    "# Add the last gap_len:\n",
    "gaps.append(gap_len)\n",
    "\n",
    "\n",
    "\n",
    "full_index = 0 \n",
    "bounds = [] \n",
    "\n",
    "for user_index in range(len(gaps)):\n",
    "    bounds.append([full_index, full_index+gaps[user_index]])\n",
    "    full_index+=gaps[user_index]    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#LOOK: rundown of process\n",
    "#LOOK: these are the types of user categories\n",
    "#users that train the svd to predict for train and test users\n",
    "#train users\n",
    "#test users\n",
    "\n",
    "#test and train users should have the same range of ratings\n",
    "#but there should be more train users than test users\n",
    "#users that train the svd to predict for train and test users should...\n",
    "#have a different rating range\n",
    "\n",
    "#there are 2 features to train the final model\n",
    "#against the target ratings of the train users\n",
    "#feature 1: svd prediction from train users\n",
    "#feature 2: average rating for the train users\n",
    "\n",
    "#there are 2 features to test the final model\n",
    "#against the target ratings of the test users\n",
    "#feature 1: svd prediction from test users\n",
    "#feature 2: average rating for the test users\n",
    "\n",
    "\n",
    "#These set the rating requirements for test and train users.\n",
    "    \n",
    "\n",
    "SVD_USER_RATING_LB = 20\n",
    "SVD_USER_RATING_UB = 30\n",
    "USER_RATING_LB = 5\n",
    "USER_RATING_UB = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(bounds)\n",
    "no_svd_users = 1000\n",
    "train_users = 800\n",
    "test_users = 800\n",
    "last_index = -1\n",
    "bounds_svd_users = []\n",
    "bounds_train_users = []\n",
    "bounds_test_users = []\n",
    "\n",
    "\n",
    "index = 0\n",
    "for item in bounds:\n",
    "    if item[1]-item[0] >=SVD_USER_RATING_LB and item[1]-item[0] <=SVD_USER_RATING_UB:\n",
    "        bounds_svd_users.append(item)\n",
    "        if len(bounds_svd_users) == no_svd_users:\n",
    "            last_index = index\n",
    "            print(\"met\")\n",
    "            break\n",
    "    index+=1\n",
    "\n",
    "for item in bounds[last_index+1:]:\n",
    "    if item[1]-item[0] >=USER_RATING_LB and item[1]-item[0] <=USER_RATING_UB:\n",
    "        bounds_train_users.append(item)\n",
    "        if len(bounds_train_users) == train_users:\n",
    "            last_index = index\n",
    "            print(\"met\")\n",
    "            break\n",
    "    index+=1\n",
    "\n",
    "for item in bounds[last_index+1:]:\n",
    "    if item[1]-item[0] >=USER_RATING_LB and item[1]-item[0] <=USER_RATING_UB:\n",
    "        bounds_test_users.append(item)\n",
    "        if len(bounds_test_users) == test_users:\n",
    "            last_index = index\n",
    "            print(\"met\")\n",
    "            break\n",
    "    index+=1\n",
    "\n",
    "\n",
    "print(len(bounds_svd_users))\n",
    "print(len(bounds_train_users))\n",
    "print(len(bounds_test_users))\n",
    "\n",
    "# Transformed data of the selected train users and test users (in that order):\n",
    "sampled_data = []\n",
    "\n",
    "\n",
    "for bound in bounds_svd_users:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        sampled_data.append(movie_data)\n",
    "\n",
    "\n",
    "for bound in bounds_train_users:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        sampled_data.append(movie_data)\n",
    "\n",
    "\n",
    "\n",
    "for bound in bounds_test_users:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        sampled_data.append(movie_data)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "\n",
    "\n",
    "\n",
    "#this shows that there are 3 extra added lines\n",
    "#nof ratings: 36233\n",
    "#last index in csv: 36236\n",
    "print(len(sampled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data\n",
    "\n",
    "Save selected data in constructed_data.csv file so that cells below it can run without running this cell and above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "final_directory = os.path.join(current_directory, 'constructed_data')\n",
    "if not os.path.exists(final_directory):\n",
    "   os.makedirs(final_directory)\n",
    "\n",
    "with open(\"constructed_data/constructed_data_2.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    writer.writerows(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "This is the starting cell to run if the data is already saved to the constructed_data.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36113\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructed_data/constructed_data_2.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n",
    "\n",
    "\n",
    "print(len(data_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format and re-sample Data:\n",
    "\n",
    "Format the data into a list of movie data rows for each movie rated for the user for each user. Then, select a subset of that data for each user type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798\n",
      "1000\n",
      "800\n",
      "799\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m user_to_data_svd \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(user_to_data_svd, \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     44\u001b[0m user_to_data_train \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(user_to_data_train, \u001b[38;5;241m800\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m user_to_data_test \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_to_data_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# old_to_new  = dict()\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# last_index = 0\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# cnt = 0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#         row[0] = cnt\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#     cnt+=1\u001b[39;00m\n",
      "File \u001b[1;32mC:\\python\\lib\\random.py:482\u001b[0m, in \u001b[0;36mRandom.sample\u001b[1;34m(self, population, k, counts)\u001b[0m\n\u001b[0;32m    480\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[1;32m--> 482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    483\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[0;32m    484\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "SEED_INT = 1\n",
    "\n",
    "random.seed(SEED_INT)\n",
    "\n",
    "user_to_data_svd = []\n",
    "user_to_data_train= []\n",
    "user_to_data_test = []\n",
    "\n",
    "user_id = data_list[0][0]\n",
    "ratings = []\n",
    "user_index = 0\n",
    "\n",
    "\n",
    "#LOOK: What if two diiferen user have the same id???\n",
    "\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        if(user_index<1000 and user_index>=0):\n",
    "            user_to_data_svd.append(ratings)\n",
    "        elif(user_index<1800 and user_index>=1000):\n",
    "            user_to_data_train.append(ratings)\n",
    "        elif(user_index<2600 and user_index>=1800):\n",
    "            user_to_data_test.append(ratings)         \n",
    "        user_id = row[0]\n",
    "        ratings = [row]\n",
    "        user_index+=1\n",
    "    else:\n",
    "        ratings.append(row)\n",
    "\n",
    "\n",
    "\n",
    "print(len(user_to_data_test))\n",
    "user_to_data_test.append(ratings)\n",
    "\n",
    "print(len(user_to_data_svd))\n",
    "print(len(user_to_data_train))\n",
    "print(len(user_to_data_test))\n",
    "\n",
    "#sample and relabel user and movie indices\n",
    "user_to_data_svd = random.sample(user_to_data_svd, 1000)\n",
    "user_to_data_train = random.sample(user_to_data_train, 800)\n",
    "user_to_data_test = random.sample(user_to_data_test, 800)\n",
    "\n",
    "\n",
    "# old_to_new  = dict()\n",
    "# last_index = 0\n",
    "# cnt = 0\n",
    "\n",
    "# for user in user_to_data_svd:\n",
    "#     for row in user: \n",
    "#         if(row[1] in old_to_new.keys()):\n",
    "#             row[1] = old_to_new[row[1]]\n",
    "#         else:\n",
    "#             old_to_new[row[1]] = last_index\n",
    "#             row[1] = last_index\n",
    "#             last_index+=1      \n",
    "#         row[0] = cnt\n",
    "#     cnt+=1\n",
    "\n",
    "# for user in user_to_data_train:\n",
    "#     for row in user: \n",
    "#         if(row[1] in old_to_new.keys()):\n",
    "#             row[1] = old_to_new[row[1]]\n",
    "#         else:\n",
    "#             old_to_new[row[1]] = last_index\n",
    "#             row[1] = last_index\n",
    "#             last_index+=1      \n",
    "#         row[0] = cnt\n",
    "#     cnt+=1\n",
    "\n",
    "\n",
    "# for user in user_to_data_test:\n",
    "#     for row in user: \n",
    "#         if(row[1] in old_to_new.keys()):\n",
    "#             row[1] = old_to_new[row[1]]\n",
    "#         else:\n",
    "#             old_to_new[row[1]] = last_index\n",
    "#             row[1] = last_index\n",
    "#             last_index+=1      \n",
    "#         row[0] = cnt\n",
    "#     cnt+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features and Target values\n",
    "\n",
    "* The train and test version of feature 1,2, and 3 are populated and in the final cell some subset of (feature 1, 2 and 3) is used to train and test the final model.\n",
    "* The target values are ratings for each user from the randomly selected movie that they rated. They are also either train or test ratings used to train or test the the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jackson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple averages: 1.1700601084224824\n",
      "svd: 1.1357558208614607\n",
      "Minutes taken: 0.096286141872406\n",
      "Feature_1 to target comparison (train):\n",
      "[3.75, 4.125, 3.5, 4.285714285714286, 4.0]\n",
      "[3.0, 3.5, 3.0, 2.0, 2.5]\n",
      "Feature_1 to target comparison (test):\n",
      "[4.25, 3.6, 4.4, 3.125, 2.9]\n",
      "[5.0, 4.0, 3.0, 3.0, 1.5]\n",
      "Feature_2 to target comparison (train):\n",
      "[0, 0, 0, 0, 0]\n",
      "[3.0, 3.5, 3.0, 2.0, 2.5]\n",
      "Feature_2 to target comparison (test):\n",
      "[0, 0, 0, 0, 0]\n",
      "[5.0, 4.0, 3.0, 3.0, 1.5]\n",
      "Feature_3 to target comparison (train):\n",
      "[3.9121507348162496, 3.0763482288056334, 2.9380413797164193, 2.9999786566113946, 1.848619859222548]\n",
      "[3.0, 3.5, 3.0, 2.0, 2.5]\n",
      "Feature_3 to target comparison (test):\n",
      "[4.131802246727004, 3.773238138944804, 3.500691499818452, 3.868073347015997, 3.5830268255595463]\n",
      "[5.0, 4.0, 3.0, 3.0, 1.5]\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import random\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Number of selected components for each element of the svd factorization.\n",
    "N_VALUE_1 = 10\n",
    "N_VALUE_2 = 15\n",
    "\n",
    "SEED_INT = 1\n",
    "# Seed for consistent results across runtimes:\n",
    "random.seed(SEED_INT)\n",
    "\n",
    "\n",
    "class user_type_vars():\n",
    "    \"\"\"Each of the variables in this class represent the data structures for a user type.\"\"\"\n",
    "    def __init__(self):\n",
    "        # For each user, a dictionary of movie_id to the movies rating for each movie the user watched:\n",
    "        self.user_to_movie_id_to_rating = [] \n",
    "\n",
    "        # For each user, a random choice of movie_id from all the movies the user watched to represent the target movie:\n",
    "        self.user_to_target_movie_id = [] \n",
    "\n",
    "        # For each user, this is the index of the users target movie in the order of train.movies_in_order.\n",
    "        # (train_users only)\n",
    "        self.user_to_target_index_full = [] \n",
    "\n",
    "        # For each user, includes ratings for all the movies in the entire train set in the order of movies_in_order:\n",
    "        # Missing ratings and target movie ratings are set to that movies average rating.\n",
    "        # (train_users only)\n",
    "        self.user_to_ratings_full = [] \n",
    "\n",
    "        # For each user, includes ratings for all the movies in the entire train set in the order of movies_in_order:\n",
    "        # The movies mean rating is subtracted from each rating.\n",
    "        # Missing ratings and target movie ratings are set to zero.\n",
    "        # (train_users only)\n",
    "        self.user_to_ratings_full_transform = []\n",
    "\n",
    "        # This is a set of every unique target movie for the train set.\n",
    "        # This is used to check if a movie is in the train set but only as a target movie.\n",
    "        # (train_users only)\n",
    "        self.target_movies = set()\n",
    "\n",
    "        # For every movie watched by the user_type, a list of ratings:\n",
    "        self.movie_id_to_ratings = dict()\n",
    "\n",
    "        # All the movie ids in the order of where they appear first in the list of user ratings (either train or test users): \n",
    "        self.movies_in_order = OrderedSet()\n",
    "\n",
    "        # Model input features x for each user:\n",
    "        self.feature_1 = []\n",
    "        self.feature_2 = []\n",
    "        self.feature_3 = []\n",
    "\n",
    "        # Model output feature y for each user:\n",
    "        self.user_to_target_rating  = [] \n",
    "\n",
    "\n",
    "# \"user_type_vars\" can represent a group of train users and a group of test users.\n",
    "train_users = user_type_vars()\n",
    "test_users = user_type_vars()\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#LOOK: make sure the first param is removed if proven useless\n",
    "#LOOK: What if there were two train sets (one for content based filtering and one for svd)\n",
    "def load_feature_1_and_2(train_test, target_movies, movies_in_order, user_to_data, movie_id_to_ratings, user_to_movie_id_to_rating, user_to_target_movie_id, user_to_target_rating, feature_1, feature_2):\n",
    "    \"\"\"\n",
    "    This is ran once to be used to populate features 1 and 2 for the train_users...\n",
    "    and ran again to be used to populate features 1 and 2 for the test_users.\n",
    "    It also populates the train and test version of these variables: target_movies, movies_in_order, movie_id_to_ratings,...\n",
    "    user_to_movie_id_to_rating, user_to_target_movie_id, user_to_target_rating.\n",
    "    These variables are used in the load_feature_3 function.\n",
    "    \"\"\" \n",
    "    # \"overall_rating_sum\" and \"overall_rating_count\" are used to calculate the overall train rating.\n",
    "    # The overall_average_train (which is overall_rating_sum/overall_rating_count) is only set to the output of the \"train\" function call.\n",
    "    # This is used to fill in ratings for movies that are only target movies for a certain set of users.\n",
    "    # This is because in practice and full application of the program/model, the target movie ratings are unknown (they need to be predicted).\n",
    "    overall_rating_sum = 0\n",
    "    overall_rating_count = 0\n",
    "\n",
    "    for i in range(len(user_to_data)):\n",
    "        movie_id_to_words = dict()\n",
    "        movie_id_to_rating = dict()\n",
    "        index = 0\n",
    "        rand_index = random.randint(0, len(user_to_data[i])-1)\n",
    "        for movie_data in user_to_data[i]:\n",
    "            if index == rand_index:    \n",
    "                target_movies.add(movie_data[1])\n",
    "                user_to_target_movie_id.append(movie_data[1])\n",
    "            else:\n",
    "                # The program should train and test while simulating the possibility of...\n",
    "                # some new movies having no existing rating data in the database.\n",
    "                # This is critical training and testing if the resulting model were to realistically be tried on completely new data...\n",
    "                # where target ratings are unknown (they need to be predicted).\n",
    "                \n",
    "                # This is why target ratings are omitted from movie_id_to_ratings.\n",
    "                # The same logic stands for overall_average_train which is formed by overall_rating_sum and overall_rating_count.\n",
    "                overall_rating_sum += float(movie_data[2])\n",
    "                overall_rating_count += 1\n",
    "\n",
    "                if movie_data[1] in movie_id_to_ratings.keys():\n",
    "                    movie_id_to_ratings[movie_data[1]].append(float(movie_data[2]))\n",
    "                else:\n",
    "                    movie_id_to_ratings[movie_data[1]] = [float(movie_data[2])]\n",
    "\n",
    "            movie_string = \"\"\n",
    "\n",
    "            # Use this code to apply all the text data and combine in to a single list of words (repeats allowed).\n",
    "            # for j in range (3,len(movie_data)):\n",
    "            #     if(j!= len(movie_data)-1):\n",
    "            #         movie_string+= movie_data[j]+\" \"\n",
    "            #     else:\n",
    "            #         movie_string+= movie_data[j]\n",
    "\n",
    "\n",
    "            # Use this truncated code to only include the genre column strings.\n",
    "            movie_string = movie_data[4]\n",
    "\n",
    "            # Lematization and conversion to lists of words:\n",
    "            cleaned_string = remove_stopwords(movie_string)\n",
    "            cleaned_list = [wnl.lemmatize(word) for word in cleaned_string.split(\" \")]\n",
    "            cleaned_list = [word[:-1] for word in cleaned_list if word.endswith(\".\")] + [word for word in cleaned_list if not word.endswith(\".\")]\n",
    "\n",
    "            movie_id_to_words[movie_data[1]] = cleaned_list\n",
    "            movie_id_to_rating[movie_data[1]] = float(movie_data[2])\n",
    "            movies_in_order.add(movie_data[1])\n",
    "            index+=1\n",
    "\n",
    "        user_to_movie_id_to_rating.append(movie_id_to_rating)\n",
    "\n",
    "        # Question: This function assumes that all movies have their corpus information somewhere in selected portion of the \"the-movies-dataset\",...\n",
    "        # inorder to make a word count vector.\n",
    "        # What happens if with the application of this program/model, a completely new movie to the data is found???\n",
    "        # Answer: Then feature_2 would not function as a model parameter and feature 3 would give worse performance, but feature 1 should still work fine.\n",
    "        # Note: For the runtime of this notebook, since the train and test data all come from \"the-movies-dataset\" there is no risk of this happening.\n",
    "\n",
    "        # The current users set of words from all the movies they rated:\n",
    "        users_words_in_order = OrderedSet()\n",
    "        for movie_id in movie_id_to_words.keys():\n",
    "            users_words_in_order.update(movie_id_to_words[movie_id])\n",
    "\n",
    "        # List of word counts in the order of users_words_in_order for each movie (excluding target movie):\n",
    "        word_counts = []\n",
    "\n",
    "        # Word counts in the order of users_words_in_order for the target movie:\n",
    "        target_word_counts = [] \n",
    "\n",
    "\n",
    "        # Populate words_counts and target_word_counts\n",
    "        # for movie_id in movie_id_to_words.keys():\n",
    "        #     if movie_id != user_to_target_movie_id[-1]:\n",
    "        #         temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "        #         temp_list = []\n",
    "        #         for word in users_words_in_order:\n",
    "        #             if word in temp_dict.keys():\n",
    "        #                 temp_list.append(temp_dict[word])\n",
    "        #             else:\n",
    "        #                 temp_list.append(0)  \n",
    "        #         word_counts.append(temp_list)  \n",
    "        #     else:\n",
    "        #         temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "        #         temp_list = []\n",
    "        #         for word in users_words_in_order:\n",
    "        #             if word in temp_dict.keys():\n",
    "        #                 temp_list.append(temp_dict[word])             \n",
    "        #             else:\n",
    "        #                 temp_list.append(0) \n",
    "        #         target_word_counts = temp_list\n",
    "\n",
    "\n",
    "        # Question: What if a list comprehension method was used instead of the above method???\n",
    "        # Answer: The performance benefits remain indifferent even when the corpus columns are maxed out.\n",
    "        # This code portion is not a bottleneck.\n",
    "\n",
    "        # Populate words_counts and target_word_counts\n",
    "        for movie_id in movie_id_to_words.keys():\n",
    "            temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                word_counts.append([(lambda x : temp_dict[x] if x in temp_dict.keys() else 0)(word) for word in users_words_in_order])  \n",
    "            else:\n",
    "                target_word_counts = [(lambda x : temp_dict[x] if x in temp_dict.keys() else 0)(word) for word in users_words_in_order]\n",
    "\n",
    "\n",
    "        # Construct the normalized tf-idf before applying cossine similairity/linear kernel: \n",
    "        # This places value on terms that are un-common in alot of documents,...\n",
    "        # while still placing value on how common they are in the document at hand.\n",
    "        # In this case documents are word counts for the corpuses of a single movie the user rated.\n",
    "        # This should lead to a more powerful quantifier for cossine similairity/linear kernel between documents.\n",
    "\n",
    "        complete_word_counts = word_counts.copy()\n",
    "        complete_word_counts.append(target_word_counts)\n",
    "        transformed_word_counts = TfidfTransformer().fit_transform(complete_word_counts).toarray()\n",
    "\n",
    "\n",
    "        # Populate ratings without the target rating:\n",
    "        ratings = []\n",
    "        for movie_id in movie_id_to_rating.keys():\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                ratings.append(movie_id_to_rating[movie_id])\n",
    "            else:\n",
    "                # Add the target movie rating for the current user:\n",
    "                # Each user has only one target movie rating.\n",
    "                user_to_target_rating.append(movie_id_to_rating[movie_id])\n",
    "    \n",
    "\n",
    "        #LOOK: What if the number of train ratings was randomly reduced to the number of test ratings here\n",
    "        #would it make a difference???\n",
    "        #Answer: The results are worse because using all train data makes the average a better estimator\n",
    "        # if(train_test):\n",
    "        #     nof = random.randint(4, 9)\n",
    "        #     ratings = random.sample(ratings,nof)\n",
    "\n",
    "        def predict():\n",
    "            \"Use the word counts and ratings to add predictions to feature_1 list and feature_2 list:\"\n",
    "            # Pred_1 is unweighted average of all of the users movies.\n",
    "            pred_1 = sum(ratings) / len(ratings)\n",
    "            # Pred_2 is weighted average of all the users movies (weights are based on (cossine similarity or linear kernel)),...\n",
    "            # unless denominator is zero (see below).\n",
    "            pred_2 = 0\n",
    "\n",
    "            # cosine_sim = linear_kernel(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "            # cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            # numerator = 0\n",
    "            # denominator = 0\n",
    "            # pred_2 = pred_1\n",
    "\n",
    "            #This prediciton is based on the premise that rating of the closest movie in terms #of textual data is a better prediction than the average of all the users ratings\n",
    "\n",
    "            # highest_sim = -1\n",
    "            # closest_rating = -1\n",
    "            # for i in range(len(ratings)):\n",
    "            #     if cosine_sim[i]> highest_sim:\n",
    "            #         highest_sim = cosine_sim[i]\n",
    "            #         closest_rating = ratings[i]\n",
    "            # pred_2 = closest_rating\n",
    "\n",
    "\n",
    "            #This prediction is based on the premise that movies that are closer...\n",
    "            #in terms of textual data to the target movie shoud contribute more of there #weighted rating towards the the prediction of the target movies rating\n",
    "\n",
    "            # for i in range(len(ratings)):\n",
    "            #     numerator += cosine_sim[i]*ratings[i]\n",
    "            #     denominator += cosine_sim[i]\n",
    "    \n",
    "            # # In case of potential division by zero:\n",
    "            # if denominator != 0:\n",
    "            #     pred_2 = numerator/denominator\n",
    "\n",
    "\n",
    "            #LOOK: What about k-means clustering and then a weighted average for movies in the same cluster as the\n",
    "            #target movie\n",
    "\n",
    "            #LOOK: also make sure to do weigthed average of the movies in the same clusetr as the target if there are any\n",
    "\n",
    "            # k_means_preds = KMeans(n_clusters=4, random_state = SEED_INT, n_init=\"auto\").fit_predict(transformed_word_counts)\n",
    "            # numerator = 0\n",
    "            # denominator = 0\n",
    "            # for i in range(len(k_means_preds)-1):\n",
    "            #     if k_means_preds[len(k_means_preds)-1] == k_means_preds[i]:\n",
    "            #         numerator += ratings[i]\n",
    "            #         denominator+=1\n",
    "\n",
    "            # if denominator != 0:\n",
    "            #     pred_2 = numerator/denominator\n",
    "\n",
    "            return (pred_1, pred_2)\n",
    "        \n",
    "        predictions = predict()\n",
    "\n",
    "        feature_1.append(predictions[0])\n",
    "        feature_2.append(predictions[1])\n",
    "            \n",
    "        \n",
    "    return overall_rating_sum/overall_rating_count\n",
    "\n",
    "# Populate train data (feature 1 and feature 2):\n",
    "# The overall_average_train, which is overall_rating_sum/overall_rating_count, is only set to the output of the \"train\" function call.\n",
    "# This is used to fill in ratings for movies that are only target movies for a certain set of users.\n",
    "overall_average_train = load_feature_1_and_2(True, train_users.target_movies, train_users.movies_in_order, user_to_data_train, train_users.movie_id_to_ratings, train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id, train_users.user_to_target_rating, train_users.feature_1, train_users.feature_2)\n",
    "\n",
    "# Populate test data (feature 1 and feature 2):\n",
    "load_feature_1_and_2(False, set(), test_users.movies_in_order, user_to_data_test, test_users.movie_id_to_ratings, test_users.user_to_movie_id_to_rating, \n",
    "               test_users.user_to_target_movie_id,\n",
    "               test_users.user_to_target_rating, test_users.feature_1, test_users.feature_2)\n",
    "\n",
    "\n",
    "def pre_svd(movie_id_to_average_rating, movies_in_order, user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, \n",
    "               user_to_movie_id_to_rating, user_to_target_movie_id):\n",
    "    \"\"\"\n",
    "    Populate the lists user_to_ratings_full and user_to_ratings_full_transform:\n",
    "    User_to_ratings_full_transform is used for svd because it includes entries from all movies in movies_in_order...\n",
    "    and transforms the data in user_to_ratings_full by subtracting the movie rating mean.\n",
    "    This means that the transformed value at the indices of unwatched movies and index coresponding to target movies are zero.\n",
    "    \"\"\"\n",
    "    for i in range(len(user_to_movie_id_to_rating)):\n",
    "        ratings = []\n",
    "        transformed_ratings = []\n",
    "\n",
    "        # The index of the target movie within the entire \"movies_in_order\" ordered set:\n",
    "        index = 0\n",
    "\n",
    "        for movie_id in movies_in_order:\n",
    "            if movie_id == user_to_target_movie_id[i]:\n",
    "                user_to_target_index_full.append(index)\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                transformed_ratings.append(0) \n",
    "            elif movie_id in user_to_movie_id_to_rating[i].keys():\n",
    "                ratings.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "                transformed_ratings.append(user_to_movie_id_to_rating[i][movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            else:\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                transformed_ratings.append(0)\n",
    "            index +=1\n",
    "        # \"user_to_ratings_full\" is just for demonstration.\n",
    "        user_to_ratings_full.append(ratings)\n",
    "        # \"user_to_ratings_full_transform\" is used.\n",
    "        # Per movie averages have been subtracted (data is ready for svd).\n",
    "        user_to_ratings_full_transform.append(transformed_ratings)\n",
    "\n",
    "\n",
    "def svd_full(user_to_ratings_full_transform, n, movie_id_to_average_rating):\n",
    "    \"\"\"\n",
    "    1. Get the svd of the user_to_ratings_full_transform \n",
    "    2. Truncate each factor to n components\n",
    "    3. Multiply the truncated components together (U X s) X V \n",
    "    4. Scale back the values to the orginal rating scale (1-5) and return result\n",
    "    \"\"\"\n",
    "    U, S, V = np.linalg.svd(user_to_ratings_full_transform, full_matrices=False)\n",
    "    \n",
    "    # Simplify factors to n components:\n",
    "    U=U[:,0:n]\n",
    "    S=np.diag(S)\n",
    "    S=S[0:n,0:n]\n",
    "    V=V[0:n,:]\n",
    "\n",
    "    # Reconstruct to a new array:\n",
    "    US = np.dot(U,S)\n",
    "    USV = np.dot(US,V)\n",
    "\n",
    "    # This tranforms the UsV row by row into the original rating scale (1-5).\n",
    "    USV = USV + np.tile(list(movie_id_to_average_rating.values()), (USV.shape[0],1))\n",
    "\n",
    "    # Be consistent with data structures:\n",
    "    return list(USV)\n",
    "\n",
    "\n",
    "\n",
    "def load_feature_3():\n",
    "    \"\"\"\n",
    "    Populate feature_3 with a method loosely outlined here:\n",
    "    1. Find the average ratings for movies \n",
    "    2. Pre_svd writes a rating for every movie for every user as well as a transformed version of those rating using the averages found above\n",
    "    3. Then use the output of the svd_full function by row for user and by column for the target movie rating prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Every movie ever seen by any user in either the test and train sets:\n",
    "    all_movies_in_order = train_users.movies_in_order|test_users.movies_in_order\n",
    "\n",
    "\n",
    "    # When a movie has a number of target ratings and non-target ratings, then only the non-target ratings are used...\n",
    "    # to form the movies average rating.\n",
    "\n",
    "    # There is a difference between non-target ratings between \"movie_id_to_average_rating_train\" and \"movie_id_to_average_rating_full\".\n",
    "    # \"movie_id_to_average_rating_train\" considers the train set and \"movie_id_to_average_rating_full\" considers the train and test set.\n",
    "\n",
    "    # When a movie has only target ratings in either the train of full dataset,...\n",
    "    # instead of using the mean of the actual target ratings for \"movie_id_to_average_rating_train\" or \"movie_id_to_average_rating_full\",...\n",
    "    # the movies average rating takes on the value of overall_average_train.\n",
    "    # This is used to simlulate the potential application of this model when there are movies to be rated for a new user that have no ratings in the existing data.\n",
    "\n",
    "    # The code below deliniates two different averages for valid movies, a train average and a train+test or full average.\n",
    "    # The train average is used to normalize the ratings of the movies for train users in the first pre_svd call.\n",
    "    # The train+test averages are used to normalize the ratings of the movies for train+test users in the second pre_svd call.\n",
    "\n",
    "\n",
    "    #LOOK: need to try different solvers\n",
    "\n",
    "    movie_id_to_average_rating_train = dict()\n",
    "    movie_id_to_average_rating_full = dict()\n",
    "\n",
    "    for movie in all_movies_in_order:\n",
    "        temp = 0\n",
    "        if(movie in train_users.movie_id_to_ratings and movie in test_users.movie_id_to_ratings):\n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_train[movie] = temp/len(train_users.movie_id_to_ratings[movie]) \n",
    "\n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_full[movie] = temp/(len(train_users.movie_id_to_ratings[movie])+len(test_users.movie_id_to_ratings[movie]))\n",
    "\n",
    "        elif(movie in train_users.movie_id_to_ratings):\n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_train[movie] = temp/len(train_users.movie_id_to_ratings[movie])\n",
    "            movie_id_to_average_rating_full[movie] = movie_id_to_average_rating_train[movie]\n",
    "\n",
    "        elif(movie in test_users.movie_id_to_ratings):        \n",
    "            if(movie in train_users.target_movies):\n",
    "                movie_id_to_average_rating_train[movie] = overall_average_train\n",
    "\n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_full[movie] = temp/len(test_users.movie_id_to_ratings[movie])\n",
    "        else:\n",
    "            if(movie in train_users.target_movies):\n",
    "                movie_id_to_average_rating_train[movie] = overall_average_train\n",
    "                movie_id_to_average_rating_full[movie] = overall_average_train\n",
    "            else:\n",
    "                movie_id_to_average_rating_full[movie] = overall_average_train\n",
    "   \n",
    "\n",
    "    # Note: The three variables below are mirrored in the train_users object.\n",
    "    # These variables are for the full set \"all_movies_in_order\".\n",
    "\n",
    "    # All the ratings for the movies corresponding to the order of the ordered set, \"all_movies_in_order\", for each user.\n",
    "    full_user_to_ratings_full = []\n",
    "    # All the transformed ratings for the movies corresponding to the order of the ordered set, \"all_movies_in_order\", for each user.\n",
    "    full_user_to_ratings_full_transform = [] \n",
    "    # For each user, the index of the target movie corresponding to the order of \"all_movies_in_order\".\n",
    "    full_user_to_target_index_full = [] \n",
    "\n",
    "\n",
    "    # Combining the watched movies per user in this order (train and then test users):\n",
    "    full_user_to_movie_id_to_rating  = train_users.user_to_movie_id_to_rating + test_users.user_to_movie_id_to_rating\n",
    "    # Combining the id of the target movie per user in this order (train and then test users):\n",
    "    full_user_to_target_movie_id = train_users.user_to_target_movie_id + test_users.user_to_target_movie_id\n",
    "\n",
    "\n",
    "    # The two function calls below are used to populate user_to_ratings_full_transform, user_to_ratings_full, and user_to_target_index_full (both train and full versions). \n",
    "    # User_to_ratings_full_transform is scaled with the movie_id_to_average_rating (both train and full versions).\n",
    "\n",
    "    pre_svd(movie_id_to_average_rating_train, train_users.movies_in_order, train_users.user_to_ratings_full_transform, train_users.user_to_ratings_full, \n",
    "            train_users.user_to_target_index_full, train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id)\n",
    "\n",
    "    pre_svd(movie_id_to_average_rating_full, all_movies_in_order, full_user_to_ratings_full_transform, full_user_to_ratings_full, full_user_to_target_index_full, \n",
    "                full_user_to_movie_id_to_rating, full_user_to_target_movie_id)\n",
    "\n",
    "\n",
    "    # In practice, there is a train and a test set, the train set is a selection of what the database has on record.\n",
    "    # The test data will usually be data that hasn't been seen before that can include any number of test users.\n",
    "    # When \"train_users.user_to_ratings_full_transform\" is used as the input of the svd function below,...\n",
    "    # \"svd_out_train\" is used to produce predictions used to train the model.\n",
    "    # When \"full_user_to_ratings_full_transform\" is used as the input of the svd function below,...\n",
    "    # \"svd_out_full\" is used to produce predictions used to test the models\n",
    "\n",
    "    # Note: The second parameter to this function is refered to as n and depending on other parameters to the model, like the rating bounds of train users,...\n",
    "    # the highest perfroming values of n can vary.\n",
    "\n",
    "    svd_out_train = svd_full(train_users.user_to_ratings_full_transform, N_VALUE_1, movie_id_to_average_rating_train)\n",
    "    svd_out_full = svd_full(full_user_to_ratings_full_transform, N_VALUE_2, movie_id_to_average_rating_full)\n",
    "\n",
    "\n",
    "    # The smaller svd provides predictions used to train the model.\n",
    "    train_users.feature_3 = [svd_out_train[i][train_users.user_to_target_index_full[i]] \n",
    "                            for i in range(len(train_users.user_to_ratings_full_transform))]\n",
    "    \n",
    "    # The larger svd provides predictions used to test the model.\n",
    "    test_users.feature_3 = [svd_out_full[i+len(train_users.user_to_ratings_full_transform)]\n",
    "                            [full_user_to_target_index_full[i+len(train_users.user_to_ratings_full_transform)]] \n",
    "                            for i in range(len(full_user_to_ratings_full_transform) - len(train_users.user_to_ratings_full_transform))]\n",
    "    \n",
    "\n",
    "    #LOOK: Try trivial solution. average train rating\n",
    "    #results: worse than the default netflix prize mse\n",
    "    #must be due to differences in data\n",
    "    \n",
    "    global average_rating_train\n",
    "    average_rating_train = []\n",
    "    \n",
    "    for item in train_users.user_to_target_movie_id:\n",
    "        if item in movie_id_to_average_rating_train:\n",
    "            average_rating_train.append(movie_id_to_average_rating_train[item])\n",
    "        else:\n",
    "            average_rating_train.append(overall_average_train)\n",
    "\n",
    "    global average_rating_test \n",
    "    average_rating_test = []\n",
    "    \n",
    "    for item in test_users.user_to_target_movie_id:\n",
    "        if item in movie_id_to_average_rating_train:\n",
    "            average_rating_test.append(movie_id_to_average_rating_train[item])\n",
    "        else:\n",
    "            average_rating_test.append(overall_average_train)\n",
    "\n",
    "    print(\"simple averages:\", mean_squared_error(test_users.user_to_target_rating, average_rating_test, squared=False))\n",
    "    print(\"svd:\", mean_squared_error(test_users.user_to_target_rating, test_users.feature_3, squared=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# populate train and test data (feature 3):\n",
    "load_feature_3()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "\n",
    "# This is used to show how the features approximate the target rating.\n",
    "print(\"Feature_1 to target comparison (train):\")\n",
    "print(train_users.feature_1[0:5])\n",
    "print(train_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_1 to target comparison (test):\")\n",
    "print(test_users.feature_1[0:5])\n",
    "print(test_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_2 to target comparison (train):\")\n",
    "print(train_users.feature_2[0:5])\n",
    "print(train_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_2 to target comparison (test):\")\n",
    "print(test_users.feature_2[0:5])\n",
    "print(test_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_3 to target comparison (train):\")\n",
    "print(train_users.feature_3[0:5])\n",
    "print(train_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_3 to target comparison (test):\")\n",
    "print(test_users.feature_3[0:5])\n",
    "print(test_users.user_to_target_rating[0:5])\n",
    "\n",
    "\n",
    "\n",
    "# simple averages: 1.0905920063365069\n",
    "# svd: 1.0777122790819773\n",
    "\n",
    "\n",
    "# test with more data... (10000 train users)\n",
    "# svd: 1.070615607659012\n",
    "# simple averages: 1.0797962050012693\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Creation\n",
    "\n",
    "Certain features and the target ratings constructed above are used to train and test models with consistent parameters for a number of runs.\n",
    "\n",
    "Note: each feature can function as a predictor to the target movies rating on their own. The purpose of the model (loosley speaking) is how much weight to give to each feature for the optimal prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5], [10], [15], [5, 5], [5, 10], [5, 15], [10, 5], [10, 10], [10, 15], [15, 5], [15, 10], [15, 15], [5, 5, 5], [5, 5, 10], [5, 5, 15], [5, 10, 5], [5, 10, 10], [5, 10, 15], [5, 15, 5], [5, 15, 10], [5, 15, 15], [10, 5, 5], [10, 5, 10], [10, 5, 15], [10, 10, 5], [10, 10, 10], [10, 10, 15], [10, 15, 5], [10, 15, 10], [10, 15, 15], [15, 5, 5], [15, 5, 10], [15, 5, 15], [15, 10, 5], [15, 10, 10], [15, 10, 15], [15, 15, 5], [15, 15, 10], [15, 15, 15]]\n",
      "Average r2_score without rounding: 1.0300035700094392\n",
      "Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 1.0364145859608938\n",
      "Minutes taken: 0.00247273842493693\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "# From testing, feature scaling was found not to improve performance in model performance or runtime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "# The alternative evaluation metric:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#New imports:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "# from skopt import BayesSearchCV\n",
    "\n",
    "#Netflix prize metric:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "start_time  = time.time()\n",
    "\n",
    "NOF_RUNS = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Generate different layer combinations, a hyperparametr in the mlp rergessor model\n",
    "nof_layers = 3\n",
    "step_size = 5\n",
    "min_nof_nodes = 5\n",
    "max_nof_nodes = 15\n",
    "\n",
    "\n",
    "layer_configs = []\n",
    "\n",
    "\n",
    "#LOOK: revert this back to 3 layers and max nof nodes  = 15\n",
    "for i in range(1,nof_layers+1):\n",
    "\n",
    "    #set the number of hidden layers\n",
    "    current_steps = [min_nof_nodes]*(i)\n",
    "\n",
    "    #vary the values of the hidden layers\n",
    "    for j in range(1, int(((max_nof_nodes-min_nof_nodes)/step_size)+1)**(i)+1):\n",
    "        layer_configs.append(current_steps.copy())\n",
    "\n",
    "        #update the correct layer by adding nodes equal to the parameter step_size\n",
    "        #and reseting the nodes to the right of it to min_nof_nodes\n",
    "        for k in range(i, -1, -1):\n",
    "            if(j%(int(((max_nof_nodes-min_nof_nodes)/step_size)+1)**(k)) == 0):\n",
    "                current_steps[i - k - 1] += step_size\n",
    "                for l in range(i - k, i):\n",
    "                    current_steps[l] = min_nof_nodes\n",
    "                break\n",
    "\n",
    "\n",
    "print(layer_configs)\n",
    "\n",
    "\n",
    "MLP_LAYERS = {'hidden_layer_sizes':layer_configs}\n",
    "SOLVERS = {'solver': ['lbfgs', 'sgd', 'adam']}\n",
    "\n",
    "SEED_INT = 1\n",
    "# Seed for consistent results across runtimes:\n",
    "random.seed(SEED_INT)\n",
    "\n",
    "def test_parameters(nof_runs, layers, train_input_features, test_input_features):\n",
    "    \"\"\"Test_parameters for a number of runs and return performance results:\"\"\"\n",
    "    train_inputs = [list(pair) for pair in train_input_features]\n",
    "    test_inputs = [list(pair) for pair in test_input_features]\n",
    "    return average_results(nof_runs, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "def average_results(nof_runs, layers, train_inputs, test_inputs):\n",
    "    \"\"\"Average the performance results for a number of models with identical inputs:\"\"\"\n",
    "    no_rounding = 0\n",
    "    rounding = 0\n",
    "    for _ in range(nof_runs):\n",
    "        pair = train_and_test(layers, train_inputs, test_inputs)\n",
    "        no_rounding+=pair[0]\n",
    "        rounding+=pair[1]\n",
    "    return no_rounding/nof_runs, rounding/nof_runs\n",
    "\n",
    "\n",
    "def train_and_test(layers, train_inputs, test_inputs):\n",
    "    \"\"\"Build, train, and test a model, then return the performance metric:\"\"\"\n",
    "\n",
    "    #This is where hyperparameter tuning may be helpful\n",
    "    #Instead of passing layers as a single vector\n",
    "    #pass it as a list of vectors to choose from\n",
    "\n",
    "    # MLP model:\n",
    "    #reg = MLPRegressor(max_iter = 1000, random_state = SEED_INT)\n",
    "\n",
    "    # Linear regression model:\n",
    "    clf = LinearRegression()\n",
    "\n",
    "\n",
    "    # Hyperparamter tuning:\n",
    "    #clf = GridSearchCV(reg, layers)\n",
    "    # clf = BayesSearchCV(reg, layers, random_state = SEED_INT)\n",
    "    #c lf = RandomizedSearchCV(reg, layers, random_state = SEED_INT)\n",
    "    # clf = GridSearchCV(reg, layers)\n",
    "\n",
    "\n",
    "    # Train model:\n",
    "    clf.fit(train_inputs, train_users.user_to_target_rating)\n",
    "\n",
    "    # Give the best parameters\n",
    "    # only works with hyperparamter tuning wrapper\n",
    "    # print(clf.best_params_)\n",
    "\n",
    "    # Print importance of the different input features to the model:\n",
    "    results = permutation_importance(clf, train_inputs, train_users.user_to_target_rating)\n",
    "    importances = results[\"importances_mean\"]\n",
    "    #LOOK: uncomment\n",
    "    #print(\"Feature Importance scores:\", \"First feature:\", importances[0],\"Second feature:\", importances[1])\n",
    "\n",
    "\n",
    "    # Make predictions for test inputs:\n",
    "    predictions = clf.predict(test_inputs)\n",
    "\n",
    "    # Test with and without roundings:\n",
    "    # Note: The actual ratings a user makes must be divisable by .5: \n",
    "    rounded_predictions = [round(item*2)/2 for item in predictions]\n",
    "\n",
    "\n",
    "    # Evaluation metric 0:\n",
    "    return(mean_squared_error(test_users.user_to_target_rating, predictions, squared=False), \n",
    "        mean_squared_error(test_users.user_to_target_rating, rounded_predictions, squared=False))\n",
    "\n",
    "    # Evaluation metric 1:\n",
    "    # return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "    #     r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    # Evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #    mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The current test is to average performance metric scores (currently r2_score) from 100 models trained on the same inputs.\n",
    "# The hidden layers are (10,10,5).\n",
    "# Note: layers only work when the mlp model is used.\n",
    "\n",
    "#LOOK: in the preliminary testing the number of runs is 100 to reduce the random varaition and make it ever more likely that the...\n",
    "#best perfoming network is actually the best \n",
    "\n",
    "avg_scores = test_parameters(NOF_RUNS, MLP_LAYERS, \n",
    "    # zip(train_users.feature_1, train_users.feature_3),\n",
    "    #   zip(test_users.feature_1, test_users.feature_3))\n",
    "    zip(average_rating_train, train_users.feature_1, train_users.feature_2, train_users.feature_3),\n",
    "      zip(average_rating_test, test_users.feature_1, test_users.feature_2, test_users.feature_3))\n",
    "\n",
    "\n",
    "print(\"Average r2_score without rounding:\",avg_scores[0])\n",
    "print(\"Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5):\",avg_scores[1])\n",
    "\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "\n",
    "\n",
    "\n",
    "#LOOK: Need to try the trivial algorithm!!! average rating across users \n",
    "#also need to report this!!! also need to compare how many \n",
    "#https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data/data\n",
    "#note: Also need to explain the difference in number of ratings provided by users in the netflix vs the dataset used here\n",
    "#should try to aim for the same genral range of provided ratings per user\n",
    "\n",
    "\n",
    "\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.17836146508107842\n",
    "# Minutes taken: 1.7588933110237122\n",
    "\n",
    "\n",
    "#LOOK: Minutes taken: 2.2997989495595297 with grid search...\n",
    "\n",
    "\n",
    "#LOOK: problem with scopt and baysian optimization and numpy\n",
    "#https://stackoverflow.com/questions/76321820/how-to-fix-the-numpy-int-attribute-error-when-using-skopt-bayessearchcv-in-sci\n",
    "\n",
    "#LOOK: for simplicity baysian search was not used because of a dependency problem\n",
    "\n",
    "#LOOK: include this in readme: before this current code using optimization,...\n",
    "#The older code tested 100 models per layer input to be sure to find the best performing parameter\n",
    "#testing a model multiple times is slow when using grid and random search because you arn't omitting values that are\n",
    "#not optimal unless you edit the list of hyperparameters (list of layer combination) which can be tedious\n",
    "#instead just applying a constant group of layers testing for 100 iterations and then adjusting the weigth in hopes of improvemnt\n",
    "#seemed to result int the best number of layers\n",
    "#the output and best perfoming layer combination is different from the grid search and individual testing\n",
    "#becasue every single model is tested 100 times in the previous code\n",
    "\n",
    "\n",
    "\n",
    "#test 1(30-50 train bounds): 0.2731692240380763 \n",
    "#test 2(20-40 train bounds): 0.2497466001941102\n",
    "#test 3(30-50 train bounds) linear regression: 0.27338731662383964\n",
    "\n",
    "\n",
    "#10000 train users, 1000 test users... (all features and linear regression)\n",
    "#Average rmse without rounding: 1.0067953614212595 \n",
    "#corresponding r2_score: 0.22093836404213862\n",
    "\n",
    "#5000 train users, 1000 test users... (all features and linear regression)\n",
    "#Average rmse without rounding: 1.0062429945462783\n",
    "#corresponding r2_score: 0.273988164991381"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
