{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Kaggle Key:Downloading the-movies-dataset.zip to ./the-movies-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228M/228M [00:23<00:00, 10.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell downloads the data needed for this jupyter notebook from kaggle and stores it in a new folder (the-movies-dataset) in the current directory.\n",
    "\n",
    "# Upon running this cell, the user will be asked for their username and key which can be found in a fresh api token from kaggle.\n",
    "\n",
    "# Instructions to get api token to authenticate the data request (Note: kaggle account required):\n",
    "# 1. Sign into kaggle.\n",
    "# 2. Go to the 'Account' tab of your user profile and select 'Create New Token'. \n",
    "# 3. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
    "\n",
    "# If the folder has been created and the files are already in that folder, than this cell does nothing and requires no credentials.\n",
    "import opendatasets as od\n",
    "\n",
    "# Data Source Information:\n",
    "# https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes taken: 0.5768450776735942\n",
      "        userId    id rating               title  \\\n",
      "6566765      1  1246    5.0        Rocky Balboa   \n",
      "6880303      1  2959    4.0      License to Wed   \n",
      "2083077      1  2762    4.5  Young and Innocent   \n",
      "1492304      1  1968    4.0       Fools Rush In   \n",
      "2638962      1   147    4.5       The 400 Blows   \n",
      "\n",
      "                                                                                                genres  \\\n",
      "6566765                                                                  [{'id': 18, 'name': 'Drama'}]   \n",
      "6880303                                                                 [{'id': 35, 'name': 'Comedy'}]   \n",
      "2083077                                     [{'id': 18, 'name': 'Drama'}, {'id': 80, 'name': 'Crime'}]   \n",
      "1492304  [{'id': 18, 'name': 'Drama'}, {'id': 35, 'name': 'Comedy'}, {'id': 10749, 'name': 'Romance'}]   \n",
      "2638962                                                                  [{'id': 18, 'name': 'Drama'}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                production_companies  \\\n",
      "6566765                                                                                                  [{'name': 'Columbia Pictures', 'id': 5}, {'name': 'Revolution Studios', 'id': 497}, {'name': 'Rogue Marble', 'id': 696}, {'name': 'Metro-Goldwyn-Mayer (MGM)', 'id': 8411}]   \n",
      "6880303  [{'name': 'Village Roadshow Pictures', 'id': 79}, {'name': 'Robert Simonds Productions', 'id': 3929}, {'name': 'Warner Bros.', 'id': 6194}, {'name': 'Phoenix Pictures', 'id': 11317}, {'name': 'Underground', 'id': 49326}, {'name': 'Proposal Productions', 'id': 49327}]   \n",
      "2083077                                                                                                                                                                                                                [{'name': 'Gaumont British Picture Corporation', 'id': 4978}]   \n",
      "1492304                                                                                                                                                                                                                                     [{'name': 'Columbia Pictures', 'id': 5}]   \n",
      "2638962                                                                                                                                 [{'name': 'Les Films du Carrosse', 'id': 53}, {'name': 'Sédif Productions', 'id': 10897}, {'name': 'The Criterion Collection', 'id': 10932}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       keywords  \\\n",
      "6566765  [{'id': 276, 'name': 'philadelphia'}, {'id': 396, 'name': 'transporter'}, {'id': 1721, 'name': 'fight'}, {'id': 2038, 'name': \"love of one's life\"}, {'id': 2416, 'name': 'publicity'}, {'id': 2792, 'name': 'boxer'}, {'id': 2968, 'name': 'grave'}, {'id': 3393, 'name': 'tombstone'}, {'id': 3586, 'name': 'tv station'}, {'id': 4487, 'name': 'boxing match'}, {'id': 4610, 'name': 'comeback'}, {'id': 4613, 'name': 'training'}, {'id': 5167, 'name': 'restaurant owner'}, {'id': 5378, 'name': 'world champion'}, {'id': 5379, 'name': 'challenger'}, {'id': 5380, 'name': 'virtual fight'}, {'id': 6066, 'name': 'defeat'}, {'id': 6067, 'name': 'victory'}, {'id': 10163, 'name': 'cancer'}, {'id': 155464, 'name': 'over-the-hill fighter'}]   \n",
      "6880303                                                                                                                                                                                                                                                                                                                                             [{'id': 1605, 'name': 'new love'}, {'id': 2856, 'name': 'ten commandments'}, {'id': 3582, 'name': 'bride'}, {'id': 3583, 'name': 'bridegroom'}, {'id': 6038, 'name': 'marriage'}, {'id': 6192, 'name': 'relation'}, {'id': 6281, 'name': 'partnership'}, {'id': 6704, 'name': 'civil registry office'}, {'id': 10093, 'name': 'priest'}, {'id': 13027, 'name': 'wedding'}, {'id': 14765, 'name': 'church'}]   \n",
      "2083077                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [{'id': 769, 'name': 'falsely accused'}, {'id': 1655, 'name': 'country house'}, {'id': 9826, 'name': 'murder'}, {'id': 9937, 'name': 'suspense'}]   \n",
      "1492304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [{'id': 828, 'name': 'waitress'}, {'id': 1463, 'name': 'culture clash'}, {'id': 9799, 'name': 'romantic comedy'}, {'id': 13149, 'name': 'pregnancy'}]   \n",
      "2638962                                                                                                                                                                                                                                                                                                                                                                      [{'id': 6930, 'name': 'fondling'}, {'id': 10183, 'name': 'independent film'}, {'id': 155518, 'name': 'nouvelle vague'}, {'id': 170268, 'name': 'skipping school'}, {'id': 170272, 'name': 'mise en scene'}, {'id': 170273, 'name': 'fingerprinting'}, {'id': 170279, 'name': '\\xa0mugshot'}, {'id': 170286, 'name': 'strict teacher'}, {'id': 170293, 'name': 'montmartre paris'}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        cast  \\\n",
      "6566765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [{'cast_id': 24, 'character': 'Rocky Balboa', 'credit_id': '52fe42e9c3a36847f802c61b', 'gender': 2, 'id': 16483, 'name': 'Sylvester Stallone', 'order': 0, 'profile_path': '/gnmwOa46C2TP35N7ARSzboTdx2u.jpg'}, {'cast_id': 25, 'character': 'Paulie', 'credit_id': '52fe42e9c3a36847f802c61f', 'gender': 2, 'id': 4521, 'name': 'Burt Young', 'order': 1, 'profile_path': '/rbSsSkQ72FoGcvwIHUxQWJ92I3W.jpg'}, {'cast_id': 26, 'character': 'Rocky Jr.', 'credit_id': '52fe42e9c3a36847f802c623', 'gender': 2, 'id': 16501, 'name': 'Milo Ventimiglia', 'order': 2, 'profile_path': '/maJeS6bA6ku21rSRceISQtwHL2h.jpg'}, {'cast_id': 27, 'character': 'Marie', 'credit_id': '52fe42e9c3a36847f802c627', 'gender': 1, 'id': 16502, 'name': 'Geraldine Hughes', 'order': 3, 'profile_path': '/bTXux3EJq25Fh2ixbet6MjdG3Fb.jpg'}, {'cast_id': 28, 'character': 'Steps', 'credit_id': '52fe42e9c3a36847f802c62b', 'gender': 2, 'id': 16503, 'name': 'James Francis Kelly III', 'order': 4, 'profile_path': '/iZyTQ2UlwNXrqLqPeNHbofFXubP.jpg'}, {'cast_id': 29, 'character': 'Duke', 'credit_id': '52fe42e9c3a36847f802c62f', 'gender': 2, 'id': 16504, 'name': 'Tony Burton', 'order': 5, 'profile_path': '/ue54hK217thXeQMzN4qUYXLImLd.jpg'}, {'cast_id': 30, 'character': 'L.C.', 'credit_id': '52fe42e9c3a36847f802c633', 'gender': 2, 'id': 16505, 'name': 'A. J. Benza', 'order': 6, 'profile_path': '/5hVinC6C1ZyD7c8EmZFTiEaF7vH.jpg'}, {'cast_id': 31, 'character': 'Adrian', 'credit_id': '52fe42e9c3a36847f802c637', 'gender': 1, 'id': 3094, 'name': 'Talia Shire', 'order': 7, 'profile_path': '/liNfrVB3eZFBOjcUGltISCucews.jpg'}, {'cast_id': 32, 'character': 'Martin', 'credit_id': '52fe42e9c3a36847f802c63b', 'gender': 2, 'id': 16506, 'name': 'Henry G. Sanders', 'order': 8, 'profile_path': '/2SU75g2CAIzGWbgfIlNvKZQhYTZ.jpg'}, {'cast_id': 33, 'character': \"Mason 'The Line' Dixon\", 'credit_id': '52fe42e9c3a36847f802c63f', 'gender': 2, 'id': 16507, 'name': 'Antonio Tarver', 'order': 9, 'profile_path': '/kJEljjHwBvrjoxqcSVntXlejgl1.jpg'}, {'cast_id': 34, 'character': 'Spider Rico', 'credit_id': '52fe42e9c3a36847f802c643', 'gender': 2, 'id': 16508, 'name': 'Pedro Lovell', 'order': 10, 'profile_path': None}, {'cast_id': 35, 'character': 'Isabel', 'credit_id': '52fe42e9c3a36847f802c647', 'gender': 1, 'id': 16509, 'name': 'Ana Gerena', 'order': 11, 'profile_path': None}, {'cast_id': 36, 'character': 'Angie', 'credit_id': '52fe42e9c3a36847f802c64b', 'gender': 1, 'id': 16510, 'name': 'Angela Boyd', 'order': 12, 'profile_path': None}, {'cast_id': 37, 'character': 'Bar Thug', 'credit_id': '52fe42e9c3a36847f802c64f', 'gender': 0, 'id': 16511, 'name': 'Louis Giansante', 'order': 13, 'profile_path': None}, {'cast_id': 38, 'character': \"Lucky's Bartender\", 'credit_id': '52fe42e9c3a36847f802c653', 'gender': 0, 'id': 16512, 'name': 'Maureen Schilling', 'order': 14, 'profile_path': None}, {'cast_id': 40, 'character': 'X-Cell', 'credit_id': '5761db05c3a3682f20000302', 'gender': 2, 'id': 98298, 'name': 'Lahmard J. Tate', 'order': 15, 'profile_path': '/4WcFReePSxyGQJWV5wXGNfY0Y7o.jpg'}]   \n",
      "6880303                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [{'cast_id': 18, 'character': 'Reverend Frank', 'credit_id': '52fe4376c3a36847f8056039', 'gender': 2, 'id': 2157, 'name': 'Robin Williams', 'order': 0, 'profile_path': '/sojtJyIV3lkUeThD7A2oHNm8183.jpg'}, {'cast_id': 19, 'character': 'Sadie Jones', 'credit_id': '52fe4376c3a36847f805603d', 'gender': 1, 'id': 16855, 'name': 'Mandy Moore', 'order': 1, 'profile_path': '/15sDtRpe301tZWrRYV31wjMuFpx.jpg'}, {'cast_id': 20, 'character': 'Ben Murphy', 'credit_id': '52fe4376c3a36847f8056041', 'gender': 2, 'id': 17697, 'name': 'John Krasinski', 'order': 2, 'profile_path': '/nOWwdZURikW22qo6OUSGFCTukgc.jpg'}, {'cast_id': 21, 'character': 'Carlisle', 'credit_id': '52fe4376c3a36847f8056045', 'gender': 2, 'id': 29020, 'name': 'Eric Christian Olsen', 'order': 3, 'profile_path': '/clbouet8o9IJlUd8WILD0lzHAtG.jpg'}, {'cast_id': 22, 'character': 'Lindsey Jones', 'credit_id': '52fe4376c3a36847f8056049', 'gender': 1, 'id': 15286, 'name': 'Christine Taylor', 'order': 4, 'profile_path': '/99OssnGmgGjduXFA7syxjNqt9tQ.jpg'}, {'cast_id': 23, 'character': 'Choir Boy', 'credit_id': '52fe4376c3a36847f805604d', 'gender': 2, 'id': 216, 'name': 'Josh Flitter', 'order': 5, 'profile_path': '/6RCA8tDWBxIVk9N3IqUjJEAzYGv.jpg'}, {'cast_id': 24, 'character': 'Joel', 'credit_id': '52fe4376c3a36847f8056051', 'gender': 2, 'id': 11827, 'name': 'DeRay Davis', 'order': 6, 'profile_path': '/w2JYPRLwXhNCpxpJc2v4UQYyMv8.jpg'}, {'cast_id': 25, 'character': 'Mr. Jones', 'credit_id': '52fe4376c3a36847f8056055', 'gender': 2, 'id': 21368, 'name': 'Peter Strauss', 'order': 7, 'profile_path': '/ufx1trct43k7UcT4DpoIMPZXi5A.jpg'}, {'cast_id': 26, 'character': 'Grandma Jones', 'credit_id': '52fe4376c3a36847f8056059', 'gender': 1, 'id': 6465, 'name': 'Grace Zabriskie', 'order': 8, 'profile_path': '/ibBabuSM1UyPYFFo0wBXhGbqElk.jpg'}, {'cast_id': 27, 'character': 'Mrs. Jones', 'credit_id': '52fe4376c3a36847f805605d', 'gender': 1, 'id': 29021, 'name': 'Roxanne Hart', 'order': 9, 'profile_path': '/yWGMW6HdhUGT2oIcQ4jmnkw7ZAM.jpg'}, {'cast_id': 28, 'character': 'Shelly', 'credit_id': '5586ee469251417f6f0059c8', 'gender': 1, 'id': 125167, 'name': 'Mindy Kaling', 'order': 10, 'profile_path': '/Agpd4tJyZ95hk74RifjnfnJpn9U.jpg'}, {'cast_id': 30, 'character': 'Expectant Father', 'credit_id': '56c3467cc3a36847c5001f66', 'gender': 2, 'id': 1368801, 'name': 'David Quinlan', 'order': 11, 'profile_path': '/2m75rrBhvOTtdUS9jlKW8GOHCBV.jpg'}, {'cast_id': 31, 'character': 'Judith', 'credit_id': '58e26093c3a36872f600dcf2', 'gender': 1, 'id': 113867, 'name': 'Angela Kinsey', 'order': 12, 'profile_path': '/omLdRLdwMLliVeVIualEnWVhm1a.jpg'}]   \n",
      "2083077                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [{'cast_id': 18, 'character': 'Erica Burgoyne', 'credit_id': '52fe436bc3a36847f8052cd5', 'gender': 1, 'id': 27939, 'name': 'Nova Pilbeam', 'order': 0, 'profile_path': '/l6oHJaRYrVxsvoTSmMS5wIXaei5.jpg'}, {'cast_id': 19, 'character': 'Robert Tisdall', 'credit_id': '52fe436bc3a36847f8052cd9', 'gender': 0, 'id': 27940, 'name': 'Derrick De Marney', 'order': 1, 'profile_path': '/7VRZ7K0EZ50haOlbVr7DHZ5550O.jpg'}, {'cast_id': 20, 'character': 'Col. Burgoyne', 'credit_id': '52fe436bc3a36847f8052cdd', 'gender': 2, 'id': 27929, 'name': 'Percy Marmont', 'order': 2, 'profile_path': '/p3DIyvlxx6B0SVIxcDaPUPlEV0U.jpg'}, {'cast_id': 21, 'character': 'Old Will', 'credit_id': '52fe436bc3a36847f8052ce1', 'gender': 2, 'id': 27941, 'name': 'Edward Rigby', 'order': 3, 'profile_path': '/B7GJ0jPtODqZVgVtZHPtvZl2tO.jpg'}, {'cast_id': 22, 'character': 'Ericas Tante Margaret', 'credit_id': '52fe436bc3a36847f8052ce5', 'gender': 1, 'id': 14304, 'name': 'Mary Clare', 'order': 4, 'profile_path': '/lAdEwCGiSUj9CCMPB4L9X4oujLe.jpg'}, {'cast_id': 23, 'character': 'Det. Insp. Kent', 'credit_id': '52fe436bc3a36847f8052ce9', 'gender': 2, 'id': 7383, 'name': 'John Longden', 'order': 5, 'profile_path': '/rsCoUEx2ThNIz12fBR6vPncCICk.jpg'}, {'cast_id': 24, 'character': 'Guy', 'credit_id': '52fe436bc3a36847f8052ced', 'gender': 2, 'id': 27942, 'name': 'George Curzon', 'order': 6, 'profile_path': None}, {'cast_id': 25, 'character': 'Ericas Onkel Basil', 'credit_id': '52fe436bc3a36847f8052cf1', 'gender': 2, 'id': 14303, 'name': 'Basil Radford', 'order': 7, 'profile_path': '/9STo7Tgdutplo78ZtyeINGWkXUk.jpg'}, {'cast_id': 26, 'character': 'Christine Clay', 'credit_id': '52fe436bc3a36847f8052cf5', 'gender': 1, 'id': 27943, 'name': 'Pamela Carme', 'order': 8, 'profile_path': None}, {'cast_id': 27, 'character': 'Detective Sergeant Miller', 'credit_id': '52fe436bc3a36847f8052cf9', 'gender': 2, 'id': 27944, 'name': 'George Merritt', 'order': 9, 'profile_path': None}, {'cast_id': 28, 'character': 'Henry Briggs', 'credit_id': '52fe436bc3a36847f8052cfd', 'gender': 2, 'id': 27945, 'name': 'J.H. Roberts', 'order': 10, 'profile_path': None}, {'cast_id': 29, 'character': \"Truckfahrer bei Tom's Hat\", 'credit_id': '52fe436bc3a36847f8052d01', 'gender': 2, 'id': 27946, 'name': 'Jerry Verno', 'order': 11, 'profile_path': None}, {'cast_id': 30, 'character': 'Police Sergeant Ruddock', 'credit_id': '52fe436bc3a36847f8052d05', 'gender': 2, 'id': 27947, 'name': 'H.F. Maltby', 'order': 12, 'profile_path': None}, {'cast_id': 31, 'character': 'Police Constable', 'credit_id': '52fe436bc3a36847f8052d09', 'gender': 2, 'id': 27948, 'name': 'John Miller', 'order': 13, 'profile_path': None}]   \n",
      "1492304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [{'cast_id': 2, 'character': 'Alex Whitman', 'credit_id': '52fe4327c3a36847f803e629', 'gender': 2, 'id': 14408, 'name': 'Matthew Perry', 'order': 0, 'profile_path': '/oSKEEDXDNnwWdQ68qfDVD6Q7Pxp.jpg'}, {'cast_id': 3, 'character': 'Isabel Fuentes', 'credit_id': '52fe4327c3a36847f803e62d', 'gender': 1, 'id': 3136, 'name': 'Salma Hayek', 'order': 1, 'profile_path': '/u5mg73xKVqm8oT93HoMmsgQHyoK.jpg'}, {'cast_id': 4, 'character': 'Jeff', 'credit_id': '52fe4327c3a36847f803e631', 'gender': 2, 'id': 4602, 'name': 'Jon Tenney', 'order': 2, 'profile_path': '/fiG1bW6DX1szsRDPIYjfIKPQ0kV.jpg'}, {'cast_id': 5, 'character': 'Lanie', 'credit_id': '52fe4327c3a36847f803e635', 'gender': 1, 'id': 6751, 'name': 'Siobhan Fallon', 'order': 3, 'profile_path': '/wVFa8GiY0xdOLFsvGygy9RMtcBc.jpg'}, {'cast_id': 16, 'character': 'Great Grandma', 'credit_id': '52fe4327c3a36847f803e675', 'gender': 1, 'id': 20360, 'name': 'Angelina Torres', 'order': 4, 'profile_path': None}, {'cast_id': 17, 'character': 'Richard Whitman', 'credit_id': '52fe4327c3a36847f803e679', 'gender': 2, 'id': 20361, 'name': 'John Bennett Perry', 'order': 5, 'profile_path': '/bzFhwuXsdZiOHRtBgz4XVELIFYO.jpg'}, {'cast_id': 18, 'character': 'Nan Whitman', 'credit_id': '52fe4327c3a36847f803e67d', 'gender': 1, 'id': 20362, 'name': 'Jill Clayburgh', 'order': 6, 'profile_path': '/twrfhIvbqHuJ7nXVpehvU6nyi6R.jpg'}, {'cast_id': 19, 'character': 'Cathy Stewart', 'credit_id': '52fe4327c3a36847f803e681', 'gender': 1, 'id': 20363, 'name': 'Suzanne Snyder', 'order': 7, 'profile_path': '/90FrTcjJudpeIYUjUzlO6XAmvnt.jpg'}, {'cast_id': 20, 'character': 'Amalia', 'credit_id': '52fe4327c3a36847f803e685', 'gender': 0, 'id': 13029, 'name': 'Anne Betancourt', 'order': 8, 'profile_path': '/6UU5P4DzjJTSBFztIu1nALT2tk0.jpg'}, {'cast_id': 21, 'character': 'Juan Fuentes', 'credit_id': '52fe4327c3a36847f803e689', 'gender': 2, 'id': 4511, 'name': 'Mark Adair-Rios', 'order': 9, 'profile_path': '/rX4d1e5jlF5P73qynjjUzJslB0c.jpg'}, {'cast_id': 22, 'character': 'Judd Marshall', 'credit_id': '52fe4327c3a36847f803e68d', 'gender': 2, 'id': 4171, 'name': 'Stanley DeSantis', 'order': 10, 'profile_path': '/4cHxkhTd7oklyNkdva9WJp0FLrX.jpg'}, {'cast_id': 23, 'character': 'Antonio Fuentes', 'credit_id': '52fe4327c3a36847f803e691', 'gender': 0, 'id': 4665, 'name': 'Josh Cruze', 'order': 11, 'profile_path': '/v3QrQzH0uGV9pd1dNR5Ue6a74qO.jpg'}, {'cast_id': 24, 'character': 'Petra', 'credit_id': '52fe4327c3a36847f803e695', 'gender': 0, 'id': 4666, 'name': 'Angela Lanza', 'order': 12, 'profile_path': '/zmf6TMWMVCdnuUfpgdnioaICk1L.jpg'}, {'cast_id': 25, 'character': 'Phil', 'credit_id': '52fe4327c3a36847f803e699', 'gender': 2, 'id': 4445, 'name': 'Chris Bauer', 'order': 13, 'profile_path': '/3KYVMaGkWTEDQ0T9lsu85pVbP4T.jpg'}, {'cast_id': 26, 'character': 'Chuy', 'credit_id': '577e438f925141440c000d63', 'gender': 0, 'id': 115874, 'name': 'Carlos Gómez', 'order': 14, 'profile_path': '/nBxwoMv1zrhNXyEjYXbcdmAdmF0.jpg'}]   \n",
      "2638962  [{'cast_id': 6, 'character': 'Antoine Doinel', 'credit_id': '52fe421ec3a36847f8005661', 'gender': 2, 'id': 1653, 'name': 'Jean-Pierre Léaud', 'order': 0, 'profile_path': '/dzkPODapVe4CSubEqI9ytTCqnZ7.jpg'}, {'cast_id': 7, 'character': 'Gilberte Doinel', 'credit_id': '52fe421ec3a36847f8005665', 'gender': 1, 'id': 1654, 'name': 'Claire Maurier', 'order': 1, 'profile_path': '/cP1n7zMsMKr77xJeR3CncomxEZ0.jpg'}, {'cast_id': 8, 'character': 'Julien Doinel', 'credit_id': '52fe421ec3a36847f8005669', 'gender': 0, 'id': 1655, 'name': 'Albert Rémy', 'order': 2, 'profile_path': '/6b8eyIXAV6oA5eX6ltc3hF7ZB3d.jpg'}, {'cast_id': 10, 'character': 'Mr. Bigey', 'credit_id': '52fe421ec3a36847f8005673', 'gender': 2, 'id': 1658, 'name': 'Georges Flamant', 'order': 3, 'profile_path': '/lQwmtPsFWME63x5M7IRF6g8bLrR.jpg'}, {'cast_id': 11, 'character': 'René', 'credit_id': '52fe421ec3a36847f8005677', 'gender': 0, 'id': 1659, 'name': 'Patrick Auffay', 'order': 4, 'profile_path': None}, {'cast_id': 12, 'character': 'Director of the school', 'credit_id': '52fe421ec3a36847f800567b', 'gender': 0, 'id': 1660, 'name': 'Robert Beauvais', 'order': 5, 'profile_path': None}, {'cast_id': 13, 'character': 'Mme Bigey', 'credit_id': '52fe421ec3a36847f800567f', 'gender': 0, 'id': 1661, 'name': 'Yvonne Claudie', 'order': 6, 'profile_path': None}, {'cast_id': 14, 'character': 'English Teacher', 'credit_id': '52fe421ec3a36847f8005683', 'gender': 0, 'id': 1662, 'name': 'Pierre Repp', 'order': 7, 'profile_path': '/1AUhiNGBAR0C6AU9iK1IXBs3QTz.jpg'}, {'cast_id': 17, 'character': 'French Teacher', 'credit_id': '52fe421ec3a36847f8005693', 'gender': 0, 'id': 1656, 'name': 'Guy Decomble', 'order': 8, 'profile_path': '/34iexAuqI1asyFounbSXSCFphen.jpg'}, {'cast_id': 20, 'character': 'Betrand Mauricet', 'credit_id': '52fe421ec3a36847f8005697', 'gender': 0, 'id': 1077237, 'name': 'Daniel Couturier', 'order': 9, 'profile_path': None}, {'cast_id': 21, 'character': 'Child', 'credit_id': '52fe421ec3a36847f800569b', 'gender': 0, 'id': 1077238, 'name': 'François Nocher', 'order': 10, 'profile_path': None}, {'cast_id': 22, 'character': 'Child', 'credit_id': '52fe421ec3a36847f800569f', 'gender': 2, 'id': 150939, 'name': 'Richard Kanayan', 'order': 11, 'profile_path': '/vCMDk3ifj2vJKZYCISXT3K6DYXF.jpg'}, {'cast_id': 23, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056a3', 'gender': 0, 'id': 1077239, 'name': 'Renaud Fontanarosa', 'order': 12, 'profile_path': None}, {'cast_id': 24, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056a7', 'gender': 0, 'id': 1077240, 'name': 'Michel Girard', 'order': 13, 'profile_path': None}, {'cast_id': 25, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056ab', 'gender': 0, 'id': 71997, 'name': 'Serge Moati', 'order': 14, 'profile_path': '/wccRQKHrX61sH4WlOtM1KBP4qaq.jpg'}, {'cast_id': 26, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056af', 'gender': 0, 'id': 1077241, 'name': 'Bernard Abbou', 'order': 15, 'profile_path': None}, {'cast_id': 27, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056b3', 'gender': 0, 'id': 1077242, 'name': 'Jean-François Bergouignan', 'order': 16, 'profile_path': None}, {'cast_id': 28, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056b7', 'gender': 0, 'id': 1077243, 'name': 'Michel Lesignor', 'order': 17, 'profile_path': None}, {'cast_id': 31, 'character': 'Man in Street', 'credit_id': '5457f0a1c3a3683993000156', 'gender': 2, 'id': 24299, 'name': 'Jean-Claude Brialy', 'order': 18, 'profile_path': '/g3kkYcAvq90tALMErxmdAIcIXsE.jpg'}, {'cast_id': 32, 'character': 'Woman with Dog', 'credit_id': '5457f0bec3a36839a0000144', 'gender': 1, 'id': 14812, 'name': 'Jeanne Moreau', 'order': 19, 'profile_path': '/uHJnVwCzehEoz0mIlwN7xkymql8.jpg'}, {'cast_id': 33, 'character': 'Man in Funfair', 'credit_id': '5457f0d3c3a368399300015b', 'gender': 2, 'id': 34613, 'name': 'Philippe de Broca', 'order': 20, 'profile_path': '/yrvmXE2SJBX659r2Y7eWwlmwfYd.jpg'}, {'cast_id': 34, 'character': 'Man in Funfair', 'credit_id': '5457f0e5c3a368399d00014c', 'gender': 0, 'id': 1650, 'name': 'François Truffaut', 'order': 21, 'profile_path': '/apCCV99N3FqB5NsEPqOzetlkprL.jpg'}]   \n",
      "\n",
      "                                                                               tagline  \\\n",
      "6566765                                                  It ain't over 'til it's over.   \n",
      "6880303                                   First came love... then came Reverend Frank.   \n",
      "2083077                                                          A Brilliant Melodrama   \n",
      "1492304  What if finding the love of your life meant changing the life that you loved?   \n",
      "2638962                                            Angel faces hell-bent for violence.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        overview  \n",
      "6566765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              When he loses a highly publicized virtual boxing match to ex-champ Rocky Balboa, reigning heavyweight titleholder, Mason Dixon retaliates by challenging Rocky to a nationally televised, 10-round exhibition bout. To the surprise of his son and friends, Rocky agrees to come out of retirement and face an opponent who's faster, stronger and thirty years his junior.  \n",
      "6880303                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Newly engaged, Ben and Sadie can't wait to start their life together and live happily ever after. However Sadie's family church's Reverend Frank won't bless their union until they pass his patented, \"foolproof\" marriage prep course consisting of outrageous classes, outlandish homework assignments and some outright invasion of privacy.  \n",
      "2083077  Derrick De Marney finds himself in a 39 Steps situation when he is wrongly accused of murder. While a fugitive from the law, De Marney is helped by heroine Nova Pilbeam, who three years earlier had played the adolescent kidnap victim in Hitchcock's The Man Who Knew Too Much. The obligatory \"fish out of water\" scene, in which the principals are briefly slowed down by a banal everyday event, occurs during a child's birthday party. The actual villain, whose identity is never in doubt (Hitchcock made thrillers, not mysteries) is played by George Curzon, who suffers from a twitching eye. Curzon's revelation during an elaborate nightclub sequence is a Hitchcockian tour de force, the sort of virtuoso sequence taken for granted in these days of flexible cameras and computer enhancement, but which in 1937 took a great deal of time, patience and talent to pull off. Released in the US as The Girl Was Young, Young and Innocent was based on a novel by Josephine Tey.  \n",
      "1492304                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Alex Whitman (Matthew Perry) is a designer from New York City who is sent to Las Vegas to supervise the construction of a nightclub that his firm has been hired to build. Alex is a straight-laced WASP-ish type who, while enjoying a night on the town, meets Isabel Fuentes (Salma Hayek), a free-spirited Mexican-American photographer. Alex and Isabel are overtaken by lust at first sight and end up sp  \n",
      "2638962                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     For young Parisian boy Antoine Doinel, life is one difficult situation after another. Surrounded by inconsiderate adults, including his neglectful parents, Antoine spends his days with his best friend, Rene, trying to plan for a better life. When one of their schemes goes awry, Antoine ends up in trouble with the law, leading to even more conflicts with unsympathetic authority figures.  \n"
     ]
    }
   ],
   "source": [
    "# This cell is for combining certain data from the necessary csv files into a single dataframe (complete_df).\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Rows are removed from each dataframe when they do not have sufficent data for a column or the data from a column does not exist.\n",
    "# This kind of row removal is done before multiple copies of the same movie data becomes present in multple rows, to save time and space.\n",
    "# Iteration through rows of a dataframe at this level is inefficient compared to list iteration.\n",
    "# This is why the dataframes are converted into lists before iteration and then back again to dataframes,... \n",
    "# so the merge function can be applied to combine the data into a single dataframe (complete_df).\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_df = pd.read_csv('./the-movies-dataset/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={'genres':\"string\",\"id\":\"string\",\"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\",\n",
    "                                    \"production_companies\" :\"string\"})[[\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"]]\n",
    "movies_df.dropna(inplace = True)\n",
    "movies_lst = [row for row in movies_df.values.tolist() if not (row[0][len(row[0])  - 2:] == \"[]\" or row[5][len(row[5]) - 2:] == \"[]\")]\n",
    "movies_df = pd.DataFrame(movies_lst, columns = (\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"), dtype = str)\n",
    "\n",
    "\n",
    "\n",
    "ratings_df = pd.read_csv('./the-movies-dataset/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"),\n",
    "                       dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})[[\"userId\", \"movieId\", \"rating\"]]\n",
    "ratings_df.rename(columns={\"movieId\": \"id\"}, inplace = True)\n",
    "ratings_df.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Question: What if the removal of duplicate movie ids per user was processed here instead of the cell below???\n",
    "# Answer: The duplicate removal function can be ran here,...\n",
    "# but the complete_list in the cell below can also be iterated over with relative complexity in order to remove duplicates.\n",
    "# The iteration in the next cell also populates the gap list...\n",
    "# which is critical to be ran directly before the function that determines bounds for users rated movies.\n",
    "# So, omitting the no duplicate function in this cell and making it run in the next cell avoids redundant iteration.\n",
    "\n",
    "\n",
    "# Question: What if the test and train ratings bounds was enforced here instead of the cell below???\n",
    "# Answer: The merge functions below needs to be executed before determining test and train users, because merge will remove rows and ratings from users...\n",
    "# before enforcing the users to be in a certain bounds for the number of their ratings. \n",
    "# The current timing of this function will ensure that the final users are within the set train or test bounds.\n",
    "\n",
    "\n",
    "keywords_df = pd.read_csv('./the-movies-dataset/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})[[\"id\", \"keywords\"]]\n",
    "keywords_df.dropna(inplace = True)\n",
    "keywords_lst = [row for row in keywords_df.values.tolist() if not (row[1][len(row[1])  - 2:] == \"[]\")]\n",
    "keywords_df = pd.DataFrame(keywords_lst, columns = (\"id\", \"keywords\"), dtype = str)\n",
    "\n",
    "\n",
    "credits_df = pd.read_csv(\"./the-movies-dataset/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})[[\"cast\", \"id\"]]\n",
    "credits_df.dropna(inplace = True)\n",
    "credits_lst = [row for row in credits_df.values.tolist() if (not row[0][len(row[0])  - 2:] == \"[]\")]\n",
    "credits_df = pd.DataFrame(credits_lst, columns = (\"cast\", \"id\"), dtype = str)\n",
    "\n",
    "\n",
    "# Default merge is inner: This only keeps movies that have the id existing in both dataframes.\n",
    "complete_df =  pd.merge(movies_df, ratings_df, on =\"id\")\n",
    "complete_df =  pd.merge(complete_df,keywords_df, on =\"id\")\n",
    "complete_df  = pd.merge(complete_df,credits_df, on =\"id\")\n",
    "\n",
    "\n",
    "complete_df.sort_values(by = 'userId', inplace = True)\n",
    "\n",
    "\n",
    "# Master dataframe: For each (user id, movie id) row combination there is the combined movie data from movies_df, ratings_df, keywords_df, and credits_df for the movie id in question.\n",
    "# The columns are reordered.\n",
    "complete_df  = complete_df.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "\n",
    "\n",
    "# For testing:\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "print(complete_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# Tested on personal machine:\n",
    "# Old run with dataframe iteration (old code): 1 minute and 5.7 seconds\n",
    "# New run with list conversion before iteration (current code): 37.1 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete number of users: 260788\n",
      "Number of highbound users before random selection: 27256\n",
      "Number of lowbound users before random selection: 68048\n",
      "Average number of ratings for the users chosen: 22.88625\n",
      "Minutes taken: 4.955366675059\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Seed for consistent results across runtimes:\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "\n",
    "def populate_names(item):\n",
    "    \"\"\"Extract names from the syntax of certain data entries:\"\"\"\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    index = 0\n",
    "    for item in jsons:\n",
    "        if(index == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        index += 1\n",
    "    return names\n",
    "\n",
    "\n",
    "def provide_data(row):\n",
    "    \"\"\"Extract data from row of complete_list:\"\"\"\n",
    "    movie_data = []\n",
    "    movie_data.append(int(row[0]))\n",
    "    movie_data.append(int(row[1]))\n",
    "    movie_data.append(float(row[2]))\n",
    "    movie_data.append(row[3])  \n",
    "\n",
    "    movie_data.append(populate_names(row[4]))\n",
    "    movie_data.append(populate_names(row[5]))\n",
    "    movie_data.append(populate_names(row[6]))\n",
    "    movie_data.append(populate_names(row[7]))\n",
    "\n",
    "    movie_data.append(str(row[8]))\n",
    "    movie_data.append(str(row[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "# The list of rows with users id, the users rating for the movie, and raw data for the movie:\n",
    "# Note: It is sorted by user_id.\n",
    "complete_list = complete_df.values.tolist()\n",
    "\n",
    "print(\"Complete number of users:\", len(list(complete_df[\"userId\"].unique()))) # 260788\n",
    "\n",
    "\n",
    "# The complete list of user rows without ratings of the same movie more than once for a given user:\n",
    "complete_list_no_dups = []\n",
    "\n",
    "# Distinquish the user the row belongs to:\n",
    "last_id = complete_list[0][0]\n",
    "\n",
    "# The set of movies that a user has rated:\n",
    "# It is used to omit later ratings of a movie that the user has already rated.\n",
    "movie_set = set()\n",
    "\n",
    "# The number of rows of movie data a single user takes up for each user:\n",
    "gaps = []\n",
    "\n",
    "# Appended to gaps when all of a users rows of movie data have been counted:\n",
    "gap_len = 0\n",
    "\n",
    "\n",
    "# Populates gaps and complete_list_no_dups by omitting movies that already have a rating in respect to each user:\n",
    "# Note: This code is faster than using dataframe methods.\n",
    "# Example: Filter data by user and then remove duplicate movie ids for each user.\n",
    "# This avoids slow dataframe iteration, but the filter method is also slow.\n",
    "for row in complete_list:\n",
    "    if last_id != row[0]:\n",
    "        movie_set= set()\n",
    "        complete_list_no_dups.append(row)\n",
    "        movie_set.add(row[1])\n",
    "        gaps.append(gap_len)\n",
    "        gap_len = 1\n",
    "    else:\n",
    "        if row[1] not in movie_set:\n",
    "            complete_list_no_dups.append(row)\n",
    "            gap_len+=1\n",
    "            movie_set.add(row[1])\n",
    "    last_id = row[0]\n",
    "\n",
    "# Add the last gap_len:\n",
    "gaps.append(gap_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Index in the complete_list_no_dups list:\n",
    "full_index = 0 \n",
    "# The start and end indices+1 of the users movies in complete_list_no_dups for each train user:\n",
    "high_bounds = [] \n",
    "# The start and end indices+1 of the users movies in complete_list_no_dups for each test user:\n",
    "low_bounds = [] \n",
    "\n",
    "\n",
    "# Populates bounds_train and bounds_test by testing each user if they are a valid train or test user:\n",
    "for user_index in range(len(gaps)):\n",
    "    if 30 <= gaps[user_index] and 50 >= gaps[user_index]:\n",
    "        high_bounds.append([full_index, full_index+gaps[user_index]])\n",
    "\n",
    "    elif 5 <= gaps[user_index] and 10 >= gaps[user_index]:\n",
    "        low_bounds.append([full_index, full_index+gaps[user_index]])\n",
    "\n",
    "    full_index+=gaps[user_index]    \n",
    "\n",
    "\n",
    "\n",
    "# Question: Why save bounds of the users movie indices instead of just storing movie information for each movie rated by each user???\n",
    "# Answer: The current code saves time and memory.\n",
    "# It is redundant to save all the movie information when only a subset of the train and test users will be selected below.\n",
    "\n",
    "print(\"Number of highbound users before random selection:\",len(high_bounds)) \n",
    "print(\"Number of lowbound users before random selection:\", len(low_bounds)) \n",
    "\n",
    "fill_in_users = 6000\n",
    "filler_user_bounds = random.sample(high_bounds, fill_in_users)\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(low_bounds)\n",
    "\n",
    "train_bound_max = 6000\n",
    "test_bound_max = 3000\n",
    "\n",
    "low_bounds_train_sample = low_bounds[test_bound_max:train_bound_max]\n",
    "low_bounds_test_sample = low_bounds[0:test_bound_max]\n",
    "\n",
    "\n",
    "# Transformed data of the selected train users and test users (in that order):\n",
    "sampled_data = []\n",
    "\n",
    "avg = 0\n",
    "cnt = 0\n",
    "\n",
    "for bound in low_bounds_train_sample:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        movie_data[0] = cnt\n",
    "        movie_data.append(\"train\")\n",
    "        sampled_data.append(movie_data)\n",
    "        avg += 1\n",
    "    cnt+=1\n",
    "\n",
    "\n",
    "for bound in filler_user_bounds:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        movie_data[0] = cnt\n",
    "        movie_data.append(\"filler\")\n",
    "        sampled_data.append(movie_data)\n",
    "        avg += 1\n",
    "    cnt+=1\n",
    "\n",
    "for bound in low_bounds_test_sample:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        movie_data[0] = cnt\n",
    "        movie_data.append(\"test\")\n",
    "        sampled_data.append(movie_data)\n",
    "        avg += 1\n",
    "    cnt+=1\n",
    "\n",
    "\n",
    "\n",
    "print(\"Average number of ratings for the users chosen:\", avg/cnt)\n",
    "\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "\n",
    "\n",
    "# Results, tested on personal machine:\n",
    "\n",
    "# With train users in range 50 <= gaps[user_index] and 70 >= gaps[user_index]\n",
    "# With test users in range 5 <= gaps[user_index] and 10 >= gaps[user_index]\n",
    "# Complete number of users: 260788\n",
    "# Number of train users before random selection: 14314\n",
    "# Number of test users before random selection: 68048\n",
    "# Average number of ratings for the users chosen: 50.236\n",
    "# Minutes taken: 4.779916667938233\n",
    "\n",
    "# With train users in range 30 <= gaps[user_index] and 50 >= gaps[user_index]\n",
    "# With test users in range 5 <= gaps[user_index] and 10 >= gaps[user_index]\n",
    "# Complete number of users: 260788\n",
    "# Number of train users before random selection: 27256\n",
    "# Number of test users before random selection: 68048\n",
    "# Average number of ratings for the users chosen: 33.36266666666667\n",
    "# Minutes taken: 3.207816179593404\n",
    "\n",
    "# With train users in range 11 <= gaps[user_index] and 31 >= gaps[user_index]\n",
    "# With test users in range 5 <= gaps[user_index] and 10 >= gaps[user_index]\n",
    "# Complete number of users: 260788\n",
    "# Number of train users before random selection: 70880\n",
    "# Number of test users before random selection: 68048\n",
    "# Average number of ratings for the users chosen: 16.7075\n",
    "# Minutes taken: 1.6583004673322042\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a constructed_data/constructed_data.csv file so that cells below can run without running this cell and above:\n",
    "import csv\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "final_directory = os.path.join(current_directory, 'constructed_data')\n",
    "if not os.path.exists(final_directory):\n",
    "   os.makedirs(final_directory)\n",
    "\n",
    "with open(\"constructed_data/constructed_data.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\", \"group\"])\n",
    "    writer.writerows(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the starting cell to run if the data is already saved to the constructed_data/constructed_data.csv. \n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructed_data/constructed_data.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Seed for consistent results across runtimes:\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "# These are lists of user data:\n",
    "# Each user in each list contains movie data for all the movies they rated.\n",
    "user_to_data_train = []\n",
    "user_to_data_filler = []\n",
    "user_to_data_test = []\n",
    "\n",
    "#The group type of the last movie\n",
    "last_type = \"train\"\n",
    "\n",
    "# Distinquish the user the row belongs to:\n",
    "user_id = data_list[0][0]\n",
    "\n",
    "# Ratings of movies for a user:\n",
    "ratings = []\n",
    "\n",
    "# For each user, distinuish whether they are a train or test user by the number of their corresponding movie rows:\n",
    "# Each movie entry for a user will be in consecutive rows in data_list becuase data_list is sorted by user.\n",
    "# Note: Technically in the third cell there is clearly distinquished test and train users before combining them in sampled_data...\n",
    "# so there is an arguably redundant processes here.\n",
    "# However, this cell executes instantly so there is no need to persist any user type labels.\n",
    "for row in data_list:\n",
    "    if last_type ==\"train\" and user_id != row[0]:\n",
    "        user_to_data_train.append(ratings)\n",
    "        ratings = [row]\n",
    "    elif last_type ==\"filler\" and user_id != row[0]:\n",
    "        user_to_data_filler.append(ratings)\n",
    "        ratings = [row]\n",
    "    elif last_type ==\"test\" and user_id != row[0]:\n",
    "        user_to_data_test.append(ratings)\n",
    "        ratings = [row]\n",
    "    else:\n",
    "        ratings.append(row)\n",
    "    user_id = row[0]\n",
    "    last_type = row[10]\n",
    "\n",
    "\n",
    "# Distinguish whether the last user is a test or train user:\n",
    "if(last_type == \"train\"):\n",
    "    user_to_data_train.append(ratings)\n",
    "elif(last_type == \"filler\"):\n",
    "    user_to_data_filler.append(ratings)\n",
    "else:\n",
    "    user_to_data_test.append(ratings)\n",
    "\n",
    "\n",
    "\n",
    "# This is where smaller or equal samples of train and test users are selcted from constructed_data.csv.\n",
    "# Note: Suppose you execute (cell 3) for a excessive amount of train and test users and they are saved in constructed_data.csv.\n",
    "# Then the cell above (cell 5) and this cell(cell 6) can be executed to select a smaller subset of those to use for the model...\n",
    "# without having to run the relatively slow (cell 3) over again.\n",
    "user_to_data_train = random.sample(user_to_data_train, 3000)\n",
    "user_to_data_filler = random.sample(user_to_data_filler, 6000)\n",
    "user_to_data_test = random.sample(user_to_data_test, 3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jackson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes taken: 0.6868159969647726\n",
      "Feature_1 to target comparison (train):\n",
      "[3.857142857142857, 3.375, 3.5, 3.9, 4.5625]\n",
      "[4.0, 1.0, 4.0, 4.0, 4.0]\n",
      "Feature_1 to target comparison (test):\n",
      "[3.142857142857143, 4.444444444444445, 2.4285714285714284, 4.666666666666667, 3.75]\n",
      "[3.0, 3.0, 3.0, 4.0, 5.0]\n",
      "Feature_2 to target comparison (train):\n",
      "[3.6572754869592856, 3.423819225214699, 3.0, 5.0, 4.801648271913668]\n",
      "[4.0, 1.0, 4.0, 4.0, 4.0]\n",
      "Feature_2 to target comparison (test):\n",
      "[3.0017647420497133, 5.000000000000001, 3.0, 4.373114811571005, 3.983298161747997]\n",
      "[3.0, 3.0, 3.0, 4.0, 5.0]\n",
      "Feature_3 to target comparison (train):\n",
      "[4.146995238866574, 3.486141970036194, 3.7390765217618602, 3.776869517518682, 4.10754810238034]\n",
      "[4.0, 1.0, 4.0, 4.0, 4.0]\n",
      "Feature_3 to target comparison (test):\n",
      "[4.028600531576326, 4.2982594791375925, 3.7476635063382386, 4.129001311105638, 3.9932482980535267]\n",
      "[3.0, 3.0, 3.0, 4.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import random\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Seed for consistent results across runtimes:\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "\n",
    "class user_type_vars():\n",
    "    \"\"\"Each of the variables in this class represent the data structures for a user type.\"\"\"\n",
    "    def __init__(self):\n",
    "        # For each user, a dictionary of movie_id to the movies rating for each movie the user watched:\n",
    "        self.user_to_movie_id_to_rating = [] \n",
    "\n",
    "        # For every movie watched by the user_type, a list of ratings:\n",
    "        self.movie_id_to_ratings = dict()\n",
    "\n",
    "        # All the movie ids in the order of where they appear first in the list of user ratings: \n",
    "        self.movies_in_order = OrderedSet()\n",
    "\n",
    "        # Model input features x for each user:\n",
    "        # train and test users only\n",
    "        self.feature_1 = []\n",
    "        self.feature_2 = []\n",
    "        self.feature_3 = []\n",
    "\n",
    "        # movie id of the users target movie\n",
    "        # train and test users only\n",
    "        self.user_to_target_movie_id = []\n",
    "\n",
    "        # For each user, this is the index of the users target movie in the order of movies_in_order.\n",
    "        # train and test users only\n",
    "        self.user_to_target_index_full = [] \n",
    "\n",
    "        # rating of the users traget movie\n",
    "        # train and test users only\n",
    "        self.user_to_target_rating  = [] \n",
    "\n",
    "\n",
    "# \"user_type_vars\" can represent a group of train users, a group of filler users, and a group of test users.\n",
    "train_users = user_type_vars()\n",
    "filler_users = user_type_vars()\n",
    "test_users = user_type_vars()\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def load_feature_1_and_2(overall_rating_sum, overall_rating_count, movies_in_order, user_to_data, movie_id_to_ratings,  user_to_movie_id_to_rating, user_to_target_movie_id, user_to_target_rating, feature_1, feature_2):\n",
    "    \"\"\"\n",
    "    This is ran once to be used to populate features 1 and 2 for the train_users...\n",
    "    and ran again to be used to populate features 1 and 2 for the test_users.\n",
    "    It also populates the train and test version of these variables: target_movies, movies_in_order, movie_id_to_ratings,...\n",
    "    user_to_movie_id_to_rating, user_to_target_rating.\n",
    "    These variables are used in the load_feature_3 function.\n",
    "    \"\"\" \n",
    "    for i in range(len(user_to_data)):\n",
    "        movie_id_to_words = dict()\n",
    "        movie_id_to_rating = dict()\n",
    "        index = 0\n",
    "        rand_index = random.randint(0, len(user_to_data[i])-1)\n",
    "\n",
    "        for movie_data in user_to_data[i]:\n",
    "            if index == rand_index:    \n",
    "                user_to_target_movie_id.append(movie_data[1])\n",
    "            else:\n",
    "                # The program should train and test while simulating the possibility of...\n",
    "                # some new movies having no existing rating data in the database.\n",
    "                # This is critical training and testing if the resulting model were to realistically be tried on completely new data...\n",
    "                # where target ratings are unknown (they need to be predicted).\n",
    "                \n",
    "                # This is why target ratings are omitted from movie_id_to_ratings.\n",
    "                # The same logic stands for overall_average_train which is formed by overall_rating_sum and overall_rating_count.\n",
    "                overall_rating_sum += float(movie_data[2])\n",
    "                overall_rating_count += 1\n",
    "\n",
    "                if movie_data[1] in movie_id_to_ratings.keys():\n",
    "                    movie_id_to_ratings[movie_data[1]].append(float(movie_data[2]))\n",
    "                else:\n",
    "                    movie_id_to_ratings[movie_data[1]] = [float(movie_data[2])]\n",
    "\n",
    "            movie_string = \"\"\n",
    "\n",
    "            # Use this code to apply all the text data and combine in to a single list of words (repeats allowed).\n",
    "            # for j in range (3,len(movie_data)-1):\n",
    "            #     if(j!= len(movie_data)-2):\n",
    "            #         movie_string+= movie_data[j]+\" \"\n",
    "            #     else:\n",
    "            #         movie_string+= movie_data[j]\n",
    "\n",
    "\n",
    "            # Use this truncated code to only include the genre column strings.\n",
    "            movie_string = movie_data[4]\n",
    "\n",
    "            # Lematization and conversion to lists of words:\n",
    "            cleaned_string = remove_stopwords(movie_string)\n",
    "            cleaned_list = [wnl.lemmatize(word) for word in cleaned_string.split(\" \")]\n",
    "            cleaned_list = [word[:-1] for word in cleaned_list if word.endswith(\".\")] + [word for word in cleaned_list if not word.endswith(\".\")]\n",
    "\n",
    "            movie_id_to_words[movie_data[1]] = cleaned_list\n",
    "            movie_id_to_rating[movie_data[1]] = float(movie_data[2])\n",
    "            movies_in_order.add(movie_data[1])\n",
    "            index+=1\n",
    "\n",
    "        user_to_movie_id_to_rating.append(movie_id_to_rating)\n",
    "\n",
    "        # Question: This function assumes that all movies have their corpus information somewhere in selected portion of the \"the-movies-dataset\",...\n",
    "        # inorder to make a word count vector.\n",
    "        # What happens if with the application of this program/model, a completely new movie to the data is found???\n",
    "        # Answer: Then feature_2 would not function as a model parameter and feature 3 would give poor performance, but feature 1 should still work fine.\n",
    "        # Note: For the runtime of this notebook, since the train and test data all come from \"the-movies-dataset\" there is no risk of this happening.\n",
    "\n",
    "        # The current users set of words from all the movies they rated:\n",
    "        users_words_in_order = OrderedSet()\n",
    "        for movie_id in movie_id_to_words.keys():\n",
    "            users_words_in_order.update(movie_id_to_words[movie_id])\n",
    "\n",
    "        # List of word counts in the order of users_words_in_order for each movie (excluding target movie):\n",
    "        word_counts = []\n",
    "\n",
    "        # Word counts in the order of users_words_in_order for the target movie:\n",
    "        target_word_counts = [] \n",
    "\n",
    "\n",
    "        # Populate words_counts and target_word_counts\n",
    "        # for movie_id in movie_id_to_words.keys():\n",
    "        #     if movie_id != target_movie_id:\n",
    "        #         temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "        #         temp_list = []\n",
    "        #         for word in users_words_in_order:\n",
    "        #             if word in temp_dict.keys():\n",
    "        #                 temp_list.append(temp_dict[word])\n",
    "        #             else:\n",
    "        #                 temp_list.append(0)  \n",
    "        #         word_counts.append(temp_list)  \n",
    "        #     else:\n",
    "        #         temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "        #         temp_list = []\n",
    "        #         for word in users_words_in_order:\n",
    "        #             if word in temp_dict.keys():\n",
    "        #                 temp_list.append(temp_dict[word])             \n",
    "        #             else:\n",
    "        #                 temp_list.append(0) \n",
    "        #         target_word_counts = temp_list\n",
    "\n",
    "\n",
    "        # Question: What if a list comprehension method was used instead of the above method???\n",
    "        # Answer: The performance benefits remain indifferent even when the corpus columns are maxed out.\n",
    "        # This code portion is not the bottleneck.\n",
    "\n",
    "        # Populate words_counts and target_word_counts\n",
    "        for movie_id in movie_id_to_words.keys():\n",
    "            temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "            if movie_id != user_to_target_movie_id[len(user_to_target_movie_id)-1]:\n",
    "                word_counts.append([(lambda x : temp_dict[x] if x in temp_dict.keys() else 0)(word) for word in users_words_in_order])  \n",
    "            else:\n",
    "                target_word_counts = [(lambda x : temp_dict[x] if x in temp_dict.keys() else 0)(word) for word in users_words_in_order]\n",
    "\n",
    "\n",
    "        # Construct the normalized tf-idf before applying cossine similairity/linear kernel: \n",
    "        # This places value on terms that are un-common in alot of documents,...\n",
    "        # while still placing value on how common they are in the document at hand.\n",
    "        # In this case documents are word counts for the corpuses of a single movie the user rated.\n",
    "        # This should lead to a more powerful quantifier for cossine similairity/linear kernel between documents.\n",
    "\n",
    "        complete_word_counts = word_counts.copy()\n",
    "        complete_word_counts.append(target_word_counts)\n",
    "        transformed_word_counts = TfidfTransformer().fit_transform(complete_word_counts).toarray()\n",
    "\n",
    "\n",
    "        # Populate ratings without the target rating:\n",
    "        ratings = []\n",
    "        for movie_id in movie_id_to_rating.keys():\n",
    "            if movie_id != user_to_target_movie_id[len(user_to_target_movie_id)-1]:\n",
    "                ratings.append(movie_id_to_rating[movie_id])\n",
    "            else:\n",
    "                # Add the target movie rating for the current user:\n",
    "                # Each user has only one target movie rating.\n",
    "                user_to_target_rating.append(movie_id_to_rating[movie_id])\n",
    "    \n",
    "\n",
    "        def predict():\n",
    "            \"Use the word counts and ratings to add predictions to feature_1 list and feature_2 list:\"\n",
    "            # Pred_1 is unweighted average of all of the users movies.\n",
    "            pred_1 = sum(ratings) / len(ratings)\n",
    "            # Pred_2 is weighted average of all the users movies (weights are based on (cossine similarity/linear kernel)),...\n",
    "            # unless denominator is zero (see below).\n",
    "            pred_2 = 0\n",
    "\n",
    "            cosine_sim = linear_kernel(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "            cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            numerator = 0\n",
    "            denominator = 0\n",
    "            pred_2 = pred_1\n",
    "            for i in range(len(ratings)):\n",
    "                numerator += cosine_sim[i]*ratings[i]\n",
    "                denominator += cosine_sim[i]\n",
    "    \n",
    "            # In case of potential division by zero:\n",
    "            if denominator != 0:\n",
    "                pred_2 = numerator/denominator\n",
    "    \n",
    "            return (pred_1, pred_2)\n",
    "        \n",
    "        predictions = predict()\n",
    "\n",
    "        feature_1.append(predictions[0])\n",
    "        feature_2.append(predictions[1])\n",
    "\n",
    "    return(overall_rating_sum, overall_rating_count)\n",
    "            \n",
    "        \n",
    "\n",
    "#overall_rating_sum, overall_rating_count, filler_users.movies_in_order, user_to_data_filler, filler_users.movie_id_to_ratings, filler_users.user_to_movie_id_to_rating\n",
    "\n",
    "def load_filler(overall_rating_sum, overall_rating_count, movies_in_order, user_to_data, movie_id_to_ratings, user_to_movie_id_to_rating):\n",
    "    for i in range(len(user_to_data)):\n",
    "        movie_id_to_rating = dict()\n",
    "        index = 0\n",
    "        rand_index = random.randint(0, len(user_to_data[i])-1)\n",
    "        for movie_data in user_to_data[i]:\n",
    "            if index != rand_index:    \n",
    "                overall_rating_sum += float(movie_data[2])\n",
    "                overall_rating_count += 1\n",
    "\n",
    "                if movie_data[1] in movie_id_to_ratings.keys():\n",
    "                    movie_id_to_ratings[movie_data[1]].append(float(movie_data[2]))\n",
    "                else:\n",
    "                    movie_id_to_ratings[movie_data[1]] = [float(movie_data[2])]\n",
    "\n",
    "           \n",
    "            movie_id_to_rating[movie_data[1]] = float(movie_data[2])\n",
    "            movies_in_order.add(movie_data[1])\n",
    "            index+=1\n",
    "\n",
    "        user_to_movie_id_to_rating.append(movie_id_to_rating)\n",
    "\n",
    "    return(overall_rating_sum, overall_rating_count)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# \"overall_rating_sum\" and \"overall_rating_count\" are used to calculate the overall train rating.\n",
    "# The overall_average_train (which is overall_rating_sum/overall_rating_count) is only set to the output of the \"train\" function call.\n",
    "# This is used to fill in ratings for movies that are only target movies for a certain set of users.\n",
    "# This is because in practice and full application of the program/model, the target movie ratings are unknown (they need to be predicted).\n",
    "\n",
    "overall_rating_sum = 0.0\n",
    "overall_rating_count = 0.0\n",
    "\n",
    "# Populate train data (feature 1 and feature 2):\n",
    "overall_rating_sum,overall_rating_count = load_feature_1_and_2(overall_rating_sum, overall_rating_count, train_users.movies_in_order, user_to_data_train, train_users.movie_id_to_ratings, train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id, train_users.user_to_target_rating, train_users.feature_1, train_users.feature_2)\n",
    "\n",
    "\n",
    "overall_rating_sum,overall_rating_count = load_filler(overall_rating_sum, overall_rating_count, filler_users.movies_in_order, user_to_data_filler, filler_users.movie_id_to_ratings, filler_users.user_to_movie_id_to_rating)\n",
    "\n",
    "overall_average = overall_rating_sum/overall_rating_count\n",
    "\n",
    "# Populate test data (feature 1 and feature 2):\n",
    "load_feature_1_and_2(0, 0, test_users.movies_in_order, user_to_data_test, test_users.movie_id_to_ratings, test_users.user_to_movie_id_to_rating, \n",
    "                test_users.user_to_target_movie_id, test_users.user_to_target_rating, test_users.feature_1, test_users.feature_2)\n",
    "\n",
    "\n",
    "def pre_svd(movie_id_to_average_rating, movies_in_order, user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, \n",
    "               user_to_movie_id_to_rating_filler,  user_to_movie_id_to_rating, user_to_target_movie_id):\n",
    "    \"\"\"\n",
    "    Populate the lists user_to_ratings_full and user_to_ratings_full_transform:\n",
    "    User_to_ratings_full_transform is used for svd because it includes entries from all movies in movies_in_order...\n",
    "    and transforms the data in user_to_ratings_full by subtracting the movie rating mean.\n",
    "    This means that the transformed value at the indices of unwatched movies and index coresponding to target movies are zero.\n",
    "    \"\"\"\n",
    "    for i in range(len(user_to_movie_id_to_rating_filler)):\n",
    "        ratings = []\n",
    "        transformed_ratings = []\n",
    "\n",
    "        for movie_id in movies_in_order:\n",
    "            if movie_id in user_to_movie_id_to_rating_filler[i].keys():\n",
    "                ratings.append(user_to_movie_id_to_rating_filler[i][movie_id])\n",
    "                transformed_ratings.append(user_to_movie_id_to_rating_filler[i][movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            else:\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                transformed_ratings.append(movie_id_to_average_rating[movie_id] - movie_id_to_average_rating[movie_id])\n",
    "\n",
    "        user_to_ratings_full.append(ratings)\n",
    "        user_to_ratings_full_transform.append(transformed_ratings)\n",
    "\n",
    "    for i in range(len(user_to_movie_id_to_rating)):\n",
    "        ratings = []\n",
    "        transformed_ratings = []\n",
    "        index = 0\n",
    "        for movie_id in movies_in_order:\n",
    "            if movie_id == user_to_target_movie_id[i]:\n",
    "                user_to_target_index_full.append(index)\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                transformed_ratings.append(movie_id_to_average_rating[movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            elif movie_id in user_to_movie_id_to_rating[i].keys():\n",
    "                ratings.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "                transformed_ratings.append(user_to_movie_id_to_rating[i][movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            else:\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                transformed_ratings.append(movie_id_to_average_rating[movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            index+=1\n",
    "            \n",
    "        user_to_ratings_full.append(ratings)\n",
    "        user_to_ratings_full_transform.append(transformed_ratings)\n",
    "\n",
    "\n",
    "\n",
    "def svd_full(user_to_ratings_full_transform, n, movie_id_to_average_rating):\n",
    "    \"\"\"\n",
    "    1. Get the svd of the user_to_ratings_full_transform \n",
    "    2. Truncate each factor to n components\n",
    "    3. Multiply the truncated components together (U X s) X V \n",
    "    4. Scale back the values to the orginal rating scale (1-5) and return result\n",
    "    \"\"\"\n",
    "    U, S, V = np.linalg.svd(user_to_ratings_full_transform, full_matrices=False)\n",
    "    \n",
    "    # Simplify factors to n components:\n",
    "    U=U[:,0:n]\n",
    "    S=np.diag(S)\n",
    "    S=S[0:n,0:n]\n",
    "    V=V[0:n,:]\n",
    "\n",
    "    # Reconstruct to a new array:\n",
    "    US = np.dot(U,S)\n",
    "    USV = np.dot(US,V)\n",
    "\n",
    "    # This tranforms the UsV row by row into the original rating scale (1-5).\n",
    "    USV = USV + np.tile(list(movie_id_to_average_rating.values()), (USV.shape[0],1))\n",
    "\n",
    "    # Be consistent with data structures:\n",
    "    return list(USV)\n",
    "\n",
    "\n",
    "\n",
    "def load_feature_3():\n",
    "    \"\"\"\n",
    "    Populate feature_3 with a method loosely outlined here:\n",
    "    1. Find the average ratings for movies \n",
    "    2. Pre_svd writes a rating for every movie for every user as well as a transformed version of those rating using the averages found above\n",
    "    3. Then use the output of the svd_full function by row for user and by column for the target movie rating prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Every movie ever seen by any user in either the test and train sets:\n",
    "    all_movies_in_order_filler_and_train = filler_users.movies_in_order | train_users.movies_in_order\n",
    "    all_movies_in_order_filler_and_test = filler_users.movies_in_order | test_users.movies_in_order\n",
    "\n",
    "    # When a movie has a number of target ratings and non-target ratings, then only the non-target ratings are used...\n",
    "    # to form the movies average rating.\n",
    "\n",
    "    # There is a difference between non-target ratings between \"movie_id_to_average_rating_train\" and \"movie_id_to_average_rating_full\".\n",
    "    # \"movie_id_to_average_rating_train\" considers the train set and \"movie_id_to_average_rating_full\" considers the train and test set.\n",
    "\n",
    "    # When a movie has only target ratings in either the train of full dataset,...\n",
    "    # instead of using the mean of the actual target ratings for \"movie_id_to_average_rating_train\" or \"movie_id_to_average_rating_full\",...\n",
    "    # the movies average rating takes on the value of overall_average_train.\n",
    "    # This is used to simlulate the potential application of this model when there are movies to be rated for a new user that have no ratings in the existing data.\n",
    "\n",
    "    # The code below deliniates two different averages for valid movies, a train average and a train+test or full average.\n",
    "    # The train average is used to normalize the ratings of the movies for train users in the first pre_svd call.\n",
    "    # The train+test averages are used to normalize the ratings of the movies for train+test users in the second pre_svd call.\n",
    "\n",
    "    movie_id_to_average_rating_filler_and_train = dict()\n",
    "    for movie in all_movies_in_order_filler_and_train:\n",
    "        temp = 0\n",
    "        if(movie in filler_users.movie_id_to_ratings and movie in train_users.movie_id_to_ratings):\n",
    "            for rating in filler_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_filler_and_train[movie] = temp/(len(filler_users.movie_id_to_ratings[movie])+len(train_users.movie_id_to_ratings[movie]))\n",
    "\n",
    "        elif(movie in filler_users.movie_id_to_ratings):\n",
    "            for rating in filler_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_filler_and_train[movie] = temp/(len(filler_users.movie_id_to_ratings[movie]))\n",
    "\n",
    "        elif(movie in train_users.movie_id_to_ratings):        \n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_filler_and_train[movie] = temp/(len(train_users.movie_id_to_ratings[movie]))\n",
    "        else:\n",
    "            movie_id_to_average_rating_filler_and_train[movie] = overall_average\n",
    "\n",
    "\n",
    "    movie_id_to_average_rating_filler_and_test = dict()\n",
    "    for movie in all_movies_in_order_filler_and_test:\n",
    "        temp = 0\n",
    "        if(movie in filler_users.movie_id_to_ratings and movie in test_users.movie_id_to_ratings):\n",
    "            for rating in filler_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_filler_and_test[movie] = temp/(len(filler_users.movie_id_to_ratings[movie])+len(test_users.movie_id_to_ratings[movie]))\n",
    "\n",
    "        elif(movie in filler_users.movie_id_to_ratings):\n",
    "            for rating in filler_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_filler_and_test[movie] = temp/(len(filler_users.movie_id_to_ratings[movie]))\n",
    "\n",
    "        elif(movie in test_users.movie_id_to_ratings):        \n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_filler_and_test[movie] = temp/(len(test_users.movie_id_to_ratings[movie]))\n",
    "        else:\n",
    "            movie_id_to_average_rating_filler_and_test[movie] = overall_average\n",
    "\n",
    "\n",
    "\n",
    "    filler_train_user_to_ratings_full = []\n",
    "    filler_test_user_to_ratings_full = []  \n",
    "\n",
    "    filler_train_user_to_ratings_full_transform = []\n",
    "    filler_test_user_to_ratings_full_transform = []  \n",
    "\n",
    "\n",
    "    #LOOK: In svd need a way to determine when the users stop being filler users and start being train and test users\n",
    "\n",
    "    #movie_id_to_average_rating, movies_in_order, user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, user_to_movie_id_to_rating_filler,  user_to_movie_id_to_rating, user_to_target_movie_id\n",
    "    pre_svd(movie_id_to_average_rating_filler_and_train, all_movies_in_order_filler_and_train, filler_train_user_to_ratings_full_transform, filler_train_user_to_ratings_full, \n",
    "            train_users.user_to_target_index_full, filler_users.user_to_movie_id_to_rating,  train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id)\n",
    "\n",
    "    pre_svd(movie_id_to_average_rating_filler_and_test, all_movies_in_order_filler_and_test, filler_test_user_to_ratings_full_transform, filler_test_user_to_ratings_full, \n",
    "            test_users.user_to_target_index_full, filler_users.user_to_movie_id_to_rating,  test_users.user_to_movie_id_to_rating, test_users.user_to_target_movie_id)\n",
    "\n",
    "\n",
    "    # In practice, there is a train and a test set, the train set is a selection of what the database has on record.\n",
    "    # The test data will usually be data that hasn't been seen before that can include any number of test users.\n",
    "    # When \"train_users.user_to_ratings_full_transform\" is used as the input of the svd function below,...\n",
    "    # \"svd_out_train\" is used to produce predictions used to train the model.\n",
    "    # When \"full_user_to_ratings_full_transform\" is used as the input of the svd function below,...\n",
    "    # \"svd_out_full\" is used to produce predictions used to test the models\n",
    "\n",
    "    # Note: The second parameter to this function is refered to as n and depending on other parameters to the model, like the rating bounds of train users,...\n",
    "    # the highest perfroming values of n can vary.\n",
    "\n",
    "    svd_out_train = svd_full(filler_train_user_to_ratings_full_transform, 100, movie_id_to_average_rating_filler_and_train)\n",
    "    svd_out_test = svd_full(filler_test_user_to_ratings_full_transform, 100, movie_id_to_average_rating_filler_and_test)\n",
    "\n",
    "\n",
    "    # The smaller svd provides predictions used to train the model.\n",
    "    train_users.feature_3 = [svd_out_train[i+len(filler_users.user_to_movie_id_to_rating)]\n",
    "                             [train_users.user_to_target_index_full[i]] \n",
    "                            for i in range(len(train_users.user_to_target_index_full))]\n",
    "    \n",
    "    # The larger svd provides predictions used to test the model.\n",
    "    test_users.feature_3 = [svd_out_test[i+len(filler_users.user_to_movie_id_to_rating)]\n",
    "                            [test_users.user_to_target_index_full[i]] \n",
    "                            for i in range(len(test_users.user_to_target_index_full))]\n",
    "\n",
    "\n",
    "# populate train and test data (feature 3):\n",
    "load_feature_3()\n",
    "\n",
    "\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "\n",
    "# This is used to show how the features approximate the target rating.\n",
    "print(\"Feature_1 to target comparison (train):\")\n",
    "print(train_users.feature_1[0:5])\n",
    "print(train_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_1 to target comparison (test):\")\n",
    "print(test_users.feature_1[0:5])\n",
    "print(test_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_2 to target comparison (train):\")\n",
    "print(train_users.feature_2[0:5])\n",
    "print(train_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_2 to target comparison (test):\")\n",
    "print(test_users.feature_2[0:5])\n",
    "print(test_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_3 to target comparison (train):\")\n",
    "print(train_users.feature_3[0:5])\n",
    "print(train_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(\"Feature_3 to target comparison (test):\")\n",
    "print(test_users.feature_3[0:5])\n",
    "print(test_users.user_to_target_rating[0:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance scores: First feature: 0.19613035777976978 Second feature: 0.17941474161489337\n",
      "Feature Importance scores: First feature: 0.19623517913930844 Second feature: 0.17439255734489775\n",
      "Feature Importance scores: First feature: 0.1990057983840691 Second feature: 0.1710636843170918\n",
      "Feature Importance scores: First feature: 0.18745787426199906 Second feature: 0.1762687711055277\n",
      "Feature Importance scores: First feature: 0.19558314666489565 Second feature: 0.17636757265018896\n",
      "Feature Importance scores: First feature: 0.19359219635984665 Second feature: 0.18463137623011366\n",
      "Feature Importance scores: First feature: 0.18560046986699064 Second feature: 0.18007598656498425\n",
      "Feature Importance scores: First feature: 0.20017861945794174 Second feature: 0.18011376912135293\n",
      "Feature Importance scores: First feature: 0.1936504129661147 Second feature: 0.18058825251887245\n",
      "Feature Importance scores: First feature: 0.1959772082739676 Second feature: 0.17205537570159773\n",
      "Feature Importance scores: First feature: 0.20299802626558994 Second feature: 0.1699951624208256\n",
      "Feature Importance scores: First feature: 0.20038216536020564 Second feature: 0.17499585771558232\n",
      "Feature Importance scores: First feature: 0.1983888933857127 Second feature: 0.1762992060678455\n",
      "Feature Importance scores: First feature: 0.20745708873157093 Second feature: 0.17844782395902387\n",
      "Feature Importance scores: First feature: 0.19516367379268038 Second feature: 0.1780583909974524\n",
      "Feature Importance scores: First feature: 0.20277675763406586 Second feature: 0.17974663671553975\n",
      "Feature Importance scores: First feature: 0.20129130286082214 Second feature: 0.17862930465149052\n",
      "Feature Importance scores: First feature: 0.1895208480263281 Second feature: 0.1736252918038434\n",
      "Feature Importance scores: First feature: 0.19916143890763036 Second feature: 0.1753218592509837\n",
      "Feature Importance scores: First feature: 0.19621597913568506 Second feature: 0.17613769138846735\n",
      "Feature Importance scores: First feature: 0.1942338257474013 Second feature: 0.17899185592823053\n",
      "Feature Importance scores: First feature: 0.19472717260477118 Second feature: 0.17977562815621073\n",
      "Feature Importance scores: First feature: 0.1937215276258987 Second feature: 0.17830809870374373\n",
      "Feature Importance scores: First feature: 0.2061919342901332 Second feature: 0.18248113088589832\n",
      "Feature Importance scores: First feature: 0.19008596517571513 Second feature: 0.17733266754433985\n",
      "Feature Importance scores: First feature: 0.19282959007249656 Second feature: 0.1693856070335189\n",
      "Feature Importance scores: First feature: 0.19094542624766267 Second feature: 0.1749203918278752\n",
      "Feature Importance scores: First feature: 0.18775927594088665 Second feature: 0.17163215660166722\n",
      "Feature Importance scores: First feature: 0.20233458975375457 Second feature: 0.18050686819769052\n",
      "Feature Importance scores: First feature: 0.19354925121798972 Second feature: 0.1753445136675847\n",
      "Feature Importance scores: First feature: 0.20277391705193998 Second feature: 0.17862888676912503\n",
      "Feature Importance scores: First feature: 0.1910724533133104 Second feature: 0.17858515865797825\n",
      "Feature Importance scores: First feature: 0.19882537856186408 Second feature: 0.1811657205518031\n",
      "Feature Importance scores: First feature: 0.19456621032113783 Second feature: 0.17266359952741595\n",
      "Feature Importance scores: First feature: 0.19172144419192075 Second feature: 0.1790752618164537\n",
      "Feature Importance scores: First feature: 0.19366380382759885 Second feature: 0.17768790495445405\n",
      "Feature Importance scores: First feature: 0.19894331400065968 Second feature: 0.17160528873624545\n",
      "Feature Importance scores: First feature: 0.1906568947311133 Second feature: 0.1717315394276314\n",
      "Feature Importance scores: First feature: 0.20064152682486655 Second feature: 0.1817736373222158\n",
      "Feature Importance scores: First feature: 0.20001973484002383 Second feature: 0.18040491042181012\n",
      "Feature Importance scores: First feature: 0.19746045438593343 Second feature: 0.16938126405402204\n",
      "Feature Importance scores: First feature: 0.18917784275506755 Second feature: 0.1749813533650516\n",
      "Feature Importance scores: First feature: 0.20027293230216747 Second feature: 0.1787836918325401\n",
      "Feature Importance scores: First feature: 0.19194167295272274 Second feature: 0.1771439215873689\n",
      "Feature Importance scores: First feature: 0.19863945639775454 Second feature: 0.1798175645450925\n",
      "Feature Importance scores: First feature: 0.18907717174823765 Second feature: 0.1713537965243689\n",
      "Feature Importance scores: First feature: 0.19176424479658835 Second feature: 0.17362287344050537\n",
      "Feature Importance scores: First feature: 0.1932283194811238 Second feature: 0.17881557382813199\n",
      "Feature Importance scores: First feature: 0.19737558150490803 Second feature: 0.17537413365666837\n",
      "Feature Importance scores: First feature: 0.19623334615001872 Second feature: 0.18349131325256948\n",
      "Feature Importance scores: First feature: 0.19447737538910354 Second feature: 0.18023777365191185\n",
      "Feature Importance scores: First feature: 0.19612133523322697 Second feature: 0.17561580912688263\n",
      "Feature Importance scores: First feature: 0.18897172075260896 Second feature: 0.1728441339771309\n",
      "Feature Importance scores: First feature: 0.19549721472377624 Second feature: 0.17138696619983323\n",
      "Feature Importance scores: First feature: 0.1935969306958453 Second feature: 0.18732590697531454\n",
      "Feature Importance scores: First feature: 0.18783866974676405 Second feature: 0.17294401038020651\n",
      "Feature Importance scores: First feature: 0.19469588076756034 Second feature: 0.17762320768988926\n",
      "Feature Importance scores: First feature: 0.18365642746087543 Second feature: 0.174675411104692\n",
      "Feature Importance scores: First feature: 0.2023565127509473 Second feature: 0.174141193086041\n",
      "Feature Importance scores: First feature: 0.19522579540553378 Second feature: 0.18045767448336952\n",
      "Feature Importance scores: First feature: 0.19421860719882975 Second feature: 0.18079308944890324\n",
      "Feature Importance scores: First feature: 0.19963623291226384 Second feature: 0.1767200156384645\n",
      "Feature Importance scores: First feature: 0.19368750699457468 Second feature: 0.17710757380972988\n",
      "Feature Importance scores: First feature: 0.1908403553234023 Second feature: 0.17499267638896582\n",
      "Feature Importance scores: First feature: 0.20052322366594497 Second feature: 0.18040498188600135\n",
      "Feature Importance scores: First feature: 0.18799679726257407 Second feature: 0.1754067535242127\n",
      "Feature Importance scores: First feature: 0.2048016152843573 Second feature: 0.17784525257845224\n",
      "Feature Importance scores: First feature: 0.19690408407706564 Second feature: 0.1798928846206572\n",
      "Feature Importance scores: First feature: 0.19744417892525054 Second feature: 0.1784345654618062\n",
      "Feature Importance scores: First feature: 0.1899698476945598 Second feature: 0.1765195429874578\n",
      "Feature Importance scores: First feature: 0.2003692989081351 Second feature: 0.17924416637365662\n",
      "Feature Importance scores: First feature: 0.19090890948804307 Second feature: 0.18036705499875139\n",
      "Feature Importance scores: First feature: 0.19884139407000304 Second feature: 0.17968271865852475\n",
      "Feature Importance scores: First feature: 0.1972852086400256 Second feature: 0.17252267935164933\n",
      "Feature Importance scores: First feature: 0.19539512490561678 Second feature: 0.18120261981564242\n",
      "Feature Importance scores: First feature: 0.188787097233416 Second feature: 0.1771193141533311\n",
      "Feature Importance scores: First feature: 0.19964553983148753 Second feature: 0.1776342693875668\n",
      "Feature Importance scores: First feature: 0.19423860840519316 Second feature: 0.1783709629665428\n",
      "Feature Importance scores: First feature: 0.19883200733407658 Second feature: 0.18004992077014265\n",
      "Feature Importance scores: First feature: 0.200399357934767 Second feature: 0.17488060587390375\n",
      "Feature Importance scores: First feature: 0.19631589362169283 Second feature: 0.17476195844328993\n",
      "Feature Importance scores: First feature: 0.2065105483946384 Second feature: 0.1847922469010579\n",
      "Feature Importance scores: First feature: 0.19623285640770227 Second feature: 0.17150699614108694\n",
      "Feature Importance scores: First feature: 0.19366118289696382 Second feature: 0.17948974811781898\n",
      "Feature Importance scores: First feature: 0.19103160331700958 Second feature: 0.17196706257864222\n",
      "Feature Importance scores: First feature: 0.20308919020566774 Second feature: 0.17479862530992257\n",
      "Feature Importance scores: First feature: 0.19779803951670324 Second feature: 0.17759551501672904\n",
      "Feature Importance scores: First feature: 0.1904656164510386 Second feature: 0.17337819177181585\n",
      "Feature Importance scores: First feature: 0.20121389565206488 Second feature: 0.18271023105005899\n",
      "Feature Importance scores: First feature: 0.192051106833816 Second feature: 0.16890733588880408\n",
      "Feature Importance scores: First feature: 0.1922146067637458 Second feature: 0.17079020472193018\n",
      "Feature Importance scores: First feature: 0.1972694956456305 Second feature: 0.17547062002730557\n",
      "Feature Importance scores: First feature: 0.19365288082139567 Second feature: 0.17495671050563036\n",
      "Feature Importance scores: First feature: 0.19768124808478393 Second feature: 0.17807781478003584\n",
      "Feature Importance scores: First feature: 0.19990784030812955 Second feature: 0.17326333175028177\n",
      "Feature Importance scores: First feature: 0.19503118586462675 Second feature: 0.17487939154053037\n",
      "Feature Importance scores: First feature: 0.20152880406805043 Second feature: 0.17392221192947843\n",
      "Feature Importance scores: First feature: 0.19816784462992862 Second feature: 0.18017102754895292\n",
      "Feature Importance scores: First feature: 0.18285107156160993 Second feature: 0.17573230353555824\n",
      "Feature Importance scores: First feature: 0.2002297914058265 Second feature: 0.17485989325023404\n",
      "Average r2_score without rounding: 0.22824082576769877\n",
      "Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.21123777156172363\n",
      "Minutes taken: 0.025550484657287598\n"
     ]
    }
   ],
   "source": [
    "# Build models based off multiple features: \n",
    "# The features themselves are reasonably accuracte predictors of the traget rating for a (movie, user) combination.\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "# From testing, feature scaling was found not to improve performance in model accuracy or runtime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "# The alternative evaluation metric:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "start_time  = time.time()\n",
    "\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "def test_parameters(nof_runs, layers, train_input_features, test_input_features):\n",
    "    \"\"\"Test_parameters for a number of runs and return performance results:\"\"\"\n",
    "    train_inputs = [list(pair) for pair in train_input_features]\n",
    "    test_inputs = [list(pair) for pair in test_input_features]\n",
    "    return average_results(nof_runs, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "def average_results(nof_runs, layers, train_inputs, test_inputs):\n",
    "    \"\"\"Average the performance results for a number of models with identical inputs:\"\"\"\n",
    "    no_rounding = 0\n",
    "    rounding = 0\n",
    "    for _ in range(nof_runs):\n",
    "        pair = train_and_test(layers, train_inputs, test_inputs)\n",
    "        no_rounding+=pair[0]\n",
    "        rounding+=pair[1]\n",
    "    return no_rounding/nof_runs, rounding/nof_runs\n",
    "\n",
    "\n",
    "def train_and_test(layers, train_inputs, test_inputs):\n",
    "    \"\"\"Build, train, and test a model, then return accuracy scores:\"\"\"\n",
    "\n",
    "    # MLP model:\n",
    "    # reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\", max_iter = 1000, random_state = seed_int)\n",
    "\n",
    "    # Linear regression model:\n",
    "    reg = LinearRegression()\n",
    "\n",
    "    # Train model:\n",
    "    reg.fit(train_inputs, train_users.user_to_target_rating)\n",
    "\n",
    "    # Print importance of the different input features to the model:\n",
    "    results = permutation_importance(reg, train_inputs, train_users.user_to_target_rating)\n",
    "    importances = results[\"importances_mean\"]\n",
    "    print(\"Feature Importance scores:\", \"First feature:\", importances[0],\"Second feature:\", importances[1])\n",
    "\n",
    "\n",
    "    # Make predictions for test inputs:\n",
    "    predictions = reg.predict(test_inputs)\n",
    "\n",
    "    # Test with and without roundings:\n",
    "    # Note: The actual ratings a user makes must be divisable by .5: \n",
    "    rounded_predictions = [round(item*2)/2 for item in predictions]\n",
    "\n",
    "    # Evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    # Evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #    mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "\n",
    "# The current test is to average accuracy scores (currently r2_score) from 100 models trained on the same inputs.\n",
    "# The hidden layers are (10,10,5).\n",
    "# Note: layers only work when the mlp model is used.\n",
    "\n",
    "avg_scores = test_parameters(100, (10,10,5), \n",
    "    zip(train_users.feature_1, train_users.feature_3),\n",
    "      zip(test_users.feature_1, test_users.feature_3))\n",
    "\n",
    "\n",
    "print(\"Average r2_score without rounding:\",avg_scores[0])\n",
    "print(\"Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5):\",avg_scores[1])\n",
    "\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "\n",
    "#LOOK: there is slight variation in results between running on ubuntu and windows but it is very accurat to a certaun decimal\n",
    "\n",
    "\n",
    "\n",
    "#All 50 70, lineare regression, feature 1 and feature 3\n",
    "#Note: may need to try re-test since varaibles were changed during runtime\n",
    "\n",
    "# (2000, 7000, 1000) n = 10, 10\n",
    "# Average r2_score without rounding: 0.23844596811914204\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.22079020222171086\n",
    "\n",
    "# (2000, 7000, 1000) n = 5, 5\n",
    "# Average r2_score without rounding: 0.23385044162563054\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.21526009605616025\n",
    "\n",
    "# (2000, 7000, 1000) n = 20,20\n",
    "# Average r2_score without rounding: 0.23900341093359118\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.22346606004375172\n",
    "\n",
    "# (2000, 7000, 1000) n = 50,50\n",
    "# Average r2_score without rounding: 0.24181639491410348\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.23113685246693558\n",
    "\n",
    "# (2000, 7000, 1000) n = 100,100\n",
    "# Average r2_score without rounding: 0.2406656837381301\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.23131524298840492\n",
    "\n",
    "# (2000, 7000, 1000) n = 50, 75\n",
    "# Average r2_score without rounding: 0.23998759527682625\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.2245364031725681\n",
    "\n",
    "# (2000, 7000, 1000) n = 75, 50\n",
    "# Average r2_score without rounding: 0.24206648265211061\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.2330991482030988\n",
    "\n",
    "# (2000, 7000, 1000) n = 75, 75\n",
    "# Average r2_score without rounding: 0.24024903320249208\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.22578513682285387\n",
    "\n",
    "# (200, 7000, 100) n = 75, 75:\n",
    "# Average r2_score without rounding: 0.2282563762053039\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.2325581395348841\n",
    "\n",
    "# (200, 7000, 100) n = 100, 100:\n",
    "# Average r2_score without rounding: 0.22792673429229293\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.2271914132379251\n",
    "\n",
    "# (200, 7000, 100) n = 50,50:\n",
    "# Average r2_score without rounding: 0.23090533781837752\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.22182468694096613\n",
    "\n",
    "# (5000, 5000, 1000) n = 50,50:\n",
    "# Average r2_score without rounding: 0.25437009692571544\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.24149609887569934\n",
    "\n",
    "# (5000, 5000, 1000) n = 100, 100:\n",
    "# Average r2_score without rounding: 0.2582537822276442\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.24368093968546722\n",
    "\n",
    "# (5000, 5000, 1000) n = 150, 150:\n",
    "# Average r2_score without rounding: 0.2591925290672041\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.24058574853829628\n",
    "\n",
    "\n",
    "# filer users 100+, linear regression, feature 1 and feature 3:\n",
    "\n",
    "# (5000, 5000, 1000) n = 100, 100:\n",
    "# Average r2_score without rounding: 0.2565874236965786\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.24465759221383723\n",
    "\n",
    "\n",
    "#LOOK: This doesn't seem to be an improvement...\n",
    "\n",
    "\n",
    "# filter users 30-50, linear regression, feature 1 and feature 3:\n",
    "\n",
    "# (5000, 5000, 1000) n = 150, 150:\n",
    "\n",
    "# Average r2_score without rounding: 0.23203987404576865\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.2137118576030375\n",
    "\n",
    "# (5000, 5000, 1000) n = 20, 20:\n",
    "# Average r2_score without rounding: 0.2277894327499794\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.21227242850940453\n",
    "\n",
    "# 3000, 6000, 3000 n = 50,50:\n",
    "# Average r2_score without rounding: 0.2288480459483372\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.21135503347035947\n",
    "\n",
    "# 3000, 6000, 3000 n = 100,100:\n",
    "# Average r2_score without rounding: 0.22824082576769877\n",
    "# Average r2_score with rounded prediction to nearest .5 (note: the actual ratings a user makes must be divisable by .5): 0.21123777156172363\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
