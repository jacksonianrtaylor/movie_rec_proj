{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    length = len(array[8])\n",
    "    if(array[8][length-4:]==\"<NA>\"):\n",
    "        return False\n",
    "    length = len(array[9])\n",
    "    if(array[9][length-4:]==\"<NA>\"):\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#convert the dataframe into an array and build a dictionary\n",
    "user_to_data = dict()\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "\n",
    "#get all unique user ids\n",
    "list_of_user_ids = []\n",
    "last_id  = -1\n",
    "for item in complete_array:\n",
    "    if(item[0]!= last_id):\n",
    "        list_of_user_ids.append(item[0])\n",
    "        last_id = item[0]\n",
    "\n",
    "\n",
    "index  = 0\n",
    "#this has been tested with 5000, 10000, 20000, 100000\n",
    "nof_users = 20000\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, nof_users):\n",
    "    user_to_data[list_of_user_ids[i]] = []\n",
    "    for j in range(index, len(complete_array)):\n",
    "        if complete_array[j][0] == list_of_user_ids[i]:\n",
    "            #condition is checked for complete_array[j]\n",
    "            if(condition(complete_array[j])):\n",
    "                #this is where data is tranformed\n",
    "                transformed = provide_data(complete_array[j])\n",
    "                user_to_data[list_of_user_ids[i]].append(transformed)         \n",
    "        else:\n",
    "            #ignore if the number of ratings for a user is too small\n",
    "            if (len(user_to_data[list_of_user_ids[i]])<10):\n",
    "                del user_to_data[list_of_user_ids[i]]\n",
    "            index = j+1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for key in user_to_data.keys():\n",
    "        writer.writerows(user_to_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie id to list of its ratings by all users\n",
    "movie_to_ratings = dict()\n",
    "\n",
    "#user id to the ratings of movies by the user\n",
    "user_to_ratings = dict()\n",
    "\n",
    "#The list created by the constructed data csv is in order by user id\n",
    "#This code populates movie_to_ratings and user_to_ratings\n",
    "user_id = -1\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_ratings[row[0]] = [row]\n",
    "    else:\n",
    "        user_to_ratings[row[0]].append(row)\n",
    "\n",
    "    if(row[1] in movie_to_ratings.keys()):\n",
    "        movie_to_ratings[row[1]].append(row[2])\n",
    "    else:\n",
    "        movie_to_ratings[row[1]] = [row[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "#dictionary of user id to a list of strings of combined textual features for each movie rated by the user\n",
    "#the strings do not include ratings or movie id\n",
    "user_to_corpus_list = dict()\n",
    "\n",
    "for key in user_to_ratings.keys():\n",
    "    movie_strings = []\n",
    "    for movie_data in user_to_ratings[key]:\n",
    "        movie_string = \"\"\n",
    "        #avoid the first three data points (user id, movieid, and rating)\n",
    "        #use only the text data\n",
    "        for index in range (3,len(movie_data)):\n",
    "            if(index!= len(movie_data)-1):\n",
    "                movie_string+= movie_data[index]+\" \"\n",
    "            else:\n",
    "                movie_string+= movie_data[index]\n",
    "        cleaned = remove_stopwords(movie_string)\n",
    "        movie_strings.append(cleaned)\n",
    "    user_to_corpus_list[key] = movie_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_32376\\640835818.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#get average rating for a single movie amoung all users who rated it\n",
    "def get_avg_movie_rating(movie_id):\n",
    "    ret =0 \n",
    "    cnt = 0\n",
    "    for item in movie_to_ratings[movie_id]:\n",
    "        ret+= float(item)\n",
    "        cnt+=1\n",
    "    return float(ret/cnt)\n",
    "\n",
    "\n",
    "#get all the movie ratings from a single user\n",
    "def get_user_ratings(user_id):\n",
    "    ret = []\n",
    "    for item in user_to_ratings[user_id]:\n",
    "        ret.append(float(item[2]))\n",
    "    return ret\n",
    "\n",
    "\n",
    "#user to model independent var X\n",
    "user_to_features = dict()\n",
    "#user to model dependent var y\n",
    "user_to_rand_rating = dict()\n",
    "\n",
    "#populate user_to_features and user_to_rand_rating\n",
    "for key in user_to_corpus_list.keys():\n",
    "\n",
    "    count_matrix = CountVectorizer().fit_transform(user_to_corpus_list[key]).toarray().tolist()\n",
    "    rand_index = random.randint(0, len(count_matrix)-1)\n",
    "    rand_test_item = count_matrix[rand_index]\n",
    "    del count_matrix[rand_index]\n",
    "\n",
    "    #find similarity by the count of each word between the random selected movie and the other movies rated by the user\n",
    "    cosine_sim = cosine_similarity(X = count_matrix ,Y = [rand_test_item])\n",
    "\n",
    "    ratings = copy.deepcopy(get_user_ratings(key))\n",
    "    similairities = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "    random_rating = ratings[rand_index]\n",
    "    user_to_rand_rating[key] = random_rating\n",
    "    del ratings[rand_index]\n",
    "\n",
    "\n",
    "    movie_rating_avg = get_avg_movie_rating(user_to_ratings[key][rand_index][1])\n",
    "\n",
    "    user_rating_avg =  float(np.sum(ratings)/(len(ratings)))\n",
    "    user_rating_skew = skew(ratings)\n",
    "    if(math.isnan(user_rating_skew)):\n",
    "        user_rating_skew = 0\n",
    "    user_rating_kurt = kurtosis(ratings)\n",
    "    if(math.isnan(user_rating_kurt)):\n",
    "        user_rating_kurt = 0\n",
    "    user_rating_var = statistics.variance(ratings)\n",
    "\n",
    "\n",
    "    sim_average = float(np.sum(similairities)/(len(similairities)))\n",
    "    sim_skew = skew(similairities) \n",
    "    if(math.isnan(sim_skew)):\n",
    "        sim_skew = 0\n",
    "    sim_kurt = kurtosis(similairities)\n",
    "    if(math.isnan(sim_kurt)):\n",
    "        sim_kurt = 0\n",
    "    sim_var = statistics.variance(similairities)\n",
    "\n",
    "\n",
    "    # there are many curve defining features used here that may be impotent and can be cut or kept in the next cell...\n",
    "    # there may stil be other distribution measures that improve the model...\n",
    "\n",
    "    for sim, rating in zip(similairities, ratings):\n",
    "        if key not in user_to_features:\n",
    "            user_to_features[key] = [[sim, rating, user_rating_avg, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var]]\n",
    "        else:\n",
    "            user_to_features[key].append([sim, rating, user_rating_avg, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Pred: 3.4026720776634836 Actual: 3.0\n",
      "Pred: 3.150661318448408 Actual: 1.0\n",
      "Pred: 3.8946701011881406 Actual: 3.0\n",
      "Pred: 4.154769922859377 Actual: 4.0\n",
      "Pred: 2.8520412412406126 Actual: 3.0\n",
      "Pred: 4.43332327491636 Actual: 4.0\n",
      "Pred: 3.502655414085598 Actual: 1.5\n",
      "Pred: 4.433323274916359 Actual: 4.5\n",
      "Pred: 3.0459161832028383 Actual: 4.0\n",
      "Pred: 3.4677051111562798 Actual: 4.0\n",
      "Pred: 3.8550056954129155 Actual: 3.0\n",
      "Pred: 2.8806280724253543 Actual: 1.0\n",
      "Pred: 4.213657615052885 Actual: 5.0\n",
      "Pred: 3.8296143617269194 Actual: 3.0\n",
      "Pred: 3.4629427961412462 Actual: 3.0\n",
      "Pred: 3.298612033083878 Actual: 3.0\n",
      "Pred: 3.622958943476154 Actual: 4.0\n",
      "Pred: 3.986252627220968 Actual: 5.0\n",
      "Pred: 3.7258379477098846 Actual: 4.0\n",
      "Pred: 3.57963928753527 Actual: 3.0\n",
      "Pred: 3.0952884179055995 Actual: 3.0\n",
      "Pred: 3.7962018574758782 Actual: 3.0\n",
      "Pred: 4.000891477214731 Actual: 4.5\n",
      "Pred: 2.637904571021359 Actual: 2.5\n",
      "Pred: 4.355011288190003 Actual: 4.5\n",
      "Pred: 4.226621274652279 Actual: 1.0\n",
      "Pred: 4.262469507398852 Actual: 4.0\n",
      "Pred: 4.272453800714876 Actual: 4.0\n",
      "Pred: 3.669942254160454 Actual: 5.0\n",
      "Pred: 4.115372533788493 Actual: 4.5\n",
      "Pred: 3.140556055605004 Actual: 1.0\n",
      "Pred: 3.7090130703612174 Actual: 4.5\n",
      "Pred: 3.986403780087581 Actual: 5.0\n",
      "Pred: 2.9353485627486586 Actual: 3.0\n",
      "Pred: 4.433323274916358 Actual: 5.0\n",
      "Pred: 3.5297829084603234 Actual: 3.0\n",
      "Pred: 2.4293154193490043 Actual: 0.5\n",
      "Pred: 2.9902278726770533 Actual: 4.0\n",
      "Pred: 3.2984620791877317 Actual: 5.0\n",
      "Pred: 3.4477284455164035 Actual: 3.0\n",
      "Pred: 3.8226095858012217 Actual: 4.0\n",
      "Pred: 4.367028291850715 Actual: 4.0\n",
      "Pred: 3.027790745197386 Actual: 1.0\n",
      "Pred: 2.6474539887229547 Actual: 3.5\n",
      "Pred: 3.4387418473878153 Actual: 1.5\n",
      "Pred: 3.874274471130606 Actual: 4.5\n",
      "Pred: 3.7543909252940106 Actual: 2.0\n",
      "Pred: 2.7892757098481162 Actual: 2.0\n",
      "Pred: 4.26628837532642 Actual: 4.0\n",
      "Pred: 3.5103810696214146 Actual: 4.0\n",
      "Pred: 3.6572959735260886 Actual: 3.0\n",
      "Pred: 2.508090344955257 Actual: 3.0\n",
      "Pred: 4.433323274916359 Actual: 4.0\n",
      "Pred: 3.6784049715930784 Actual: 3.0\n",
      "Pred: 4.43332327491636 Actual: 4.5\n",
      "Pred: 4.4333232749163605 Actual: 5.0\n",
      "Pred: 4.433323274916358 Actual: 5.0\n",
      "Pred: 3.5126700564419977 Actual: 4.0\n",
      "Pred: 4.129660498226989 Actual: 4.0\n",
      "Pred: 3.859348624424929 Actual: 4.5\n",
      "Pred: 3.321667923687597 Actual: 4.0\n",
      "Pred: 3.493545109561288 Actual: 3.0\n",
      "Pred: 2.6655784394584074 Actual: 3.0\n",
      "Pred: 3.542918674991754 Actual: 1.0\n",
      "Pred: 3.5960346199590805 Actual: 1.0\n",
      "Pred: 3.4819038234326465 Actual: 2.0\n",
      "Pred: 3.8242618634218037 Actual: 5.0\n",
      "Pred: 4.025023549935459 Actual: 3.0\n",
      "Pred: 3.701580026811938 Actual: 4.0\n",
      "Pred: 3.7620977005948135 Actual: 4.0\n",
      "Pred: 3.952494162914974 Actual: 4.5\n",
      "Pred: 4.165502544915241 Actual: 4.5\n",
      "Pred: 3.6972857541985196 Actual: 5.0\n",
      "Pred: 2.5341801471006513 Actual: 2.0\n",
      "Pred: 4.402162291241515 Actual: 4.0\n",
      "Pred: 3.2462086275014204 Actual: 4.0\n",
      "Pred: 4.4333232749163605 Actual: 4.5\n",
      "Pred: 3.588771319266311 Actual: 3.0\n",
      "Pred: 3.081446729725437 Actual: 3.0\n",
      "Pred: 4.2155341637625146 Actual: 5.0\n",
      "Pred: 3.136640774215633 Actual: 2.0\n",
      "Pred: 4.433323274916359 Actual: 4.0\n",
      "Pred: 3.376200886527717 Actual: 4.0\n",
      "Pred: 4.291550969804933 Actual: 4.5\n",
      "Pred: 3.674235382751248 Actual: 2.0\n",
      "Pred: 4.225726942752981 Actual: 3.0\n",
      "Pred: 3.6940087735987936 Actual: 3.0\n",
      "Pred: 2.2842035635615208 Actual: 2.5\n",
      "Pred: 3.3918070721962343 Actual: 4.0\n",
      "Pred: 4.433323274916359 Actual: 4.0\n",
      "Pred: 4.281185095094506 Actual: 3.0\n",
      "Pred: 4.433323274916359 Actual: 4.0\n",
      "Pred: 2.300653607732965 Actual: 4.0\n",
      "Pred: 3.970853313965595 Actual: 3.0\n",
      "Pred: 4.393046672615295 Actual: 5.0\n",
      "Pred: 4.273949497973384 Actual: 4.5\n",
      "Pred: 3.74334145817022 Actual: 3.0\n",
      "Pred: 3.952248467280725 Actual: 4.5\n",
      "Pred: 4.368990618332956 Actual: 5.0\n",
      "Pred: 4.4189815352841535 Actual: 4.0\n",
      "Pred: 2.623889967402446 Actual: 3.0\n",
      "Pred: 3.4706286684126386 Actual: 3.0\n",
      "Pred: 3.967247209897398 Actual: 5.0\n",
      "Pred: 3.441317968685245 Actual: 4.0\n",
      "Pred: 4.196336826637402 Actual: 5.0\n",
      "Pred: 4.257455713798656 Actual: 5.0\n",
      "Pred: 3.293060526761562 Actual: 4.0\n",
      "Pred: 2.505541889945941 Actual: 1.0\n",
      "Pred: 4.005616840572033 Actual: 5.0\n",
      "Pred: 4.4333232749163605 Actual: 5.0\n",
      "Pred: 3.3268495929137214 Actual: 4.0\n",
      "Pred: 4.278469791461593 Actual: 4.0\n",
      "Pred: 3.438229820319682 Actual: 5.0\n",
      "Pred: 3.978767811064531 Actual: 4.0\n",
      "Pred: 3.295525405339578 Actual: 4.0\n",
      "Pred: 4.180649051808923 Actual: 4.0\n",
      "Pred: 4.4333232749163605 Actual: 4.0\n",
      "Pred: 3.829773887557027 Actual: 5.0\n",
      "Pred: 3.534563325284561 Actual: 3.0\n",
      "Pred: 3.612402831454632 Actual: 3.5\n",
      "Pred: 3.0199231041189525 Actual: 1.0\n",
      "Pred: 2.7845379643108727 Actual: 4.5\n",
      "Pred: 3.623198737945023 Actual: 3.0\n",
      "Pred: 4.3594640468093235 Actual: 2.0\n",
      "Pred: 4.06712797870012 Actual: 4.0\n",
      "Pred: 4.153043396957986 Actual: 4.0\n",
      "Pred: 4.433323274916357 Actual: 5.0\n",
      "Pred: 3.5443535882433674 Actual: 4.0\n",
      "Pred: 3.9861823419375324 Actual: 2.0\n",
      "Pred: 2.870865381708428 Actual: 3.0\n",
      "Pred: 4.4333232749163605 Actual: 4.0\n",
      "Pred: 3.4316771620781763 Actual: 3.0\n",
      "Pred: 2.579586005248857 Actual: 3.0\n",
      "Pred: 3.852647886024339 Actual: 5.0\n",
      "Pred: 4.049532165753899 Actual: 4.5\n",
      "Pred: 4.433323274916352 Actual: 5.0\n",
      "Pred: 4.174697478961974 Actual: 3.0\n",
      "Pred: 2.911617541827161 Actual: 3.0\n",
      "Pred: 3.2609394739129427 Actual: 3.5\n",
      "Pred: 3.809034245019965 Actual: 3.5\n",
      "Pred: 2.695491186672534 Actual: 3.5\n",
      "Pred: 3.309983298161828 Actual: 3.0\n",
      "Pred: 4.165736776958552 Actual: 4.5\n",
      "Pred: 3.4790146980208996 Actual: 3.0\n",
      "Pred: 2.679608256427845 Actual: 1.0\n",
      "Pred: 4.009204285384047 Actual: 5.0\n",
      "Pred: 3.446735440266569 Actual: 4.0\n",
      "Pred: 3.821181287221736 Actual: 2.0\n",
      "Pred: 3.9308280345983073 Actual: 4.5\n",
      "Pred: 4.433323274916359 Actual: 5.0\n",
      "Pred: 4.269774242514095 Actual: 4.5\n",
      "Pred: 4.1808089814889975 Actual: 5.0\n",
      "Pred: 3.2235150290131895 Actual: 3.0\n",
      "Pred: 3.5764980864596954 Actual: 4.0\n",
      "Pred: 3.2457884728936928 Actual: 4.0\n",
      "Pred: 3.989588080566816 Actual: 4.0\n",
      "Pred: 3.3915131647863275 Actual: 3.0\n",
      "Pred: 2.9350185612873703 Actual: 3.0\n",
      "Pred: 3.750927286591265 Actual: 3.0\n",
      "Pred: 3.0501036993592794 Actual: 3.0\n",
      "Pred: 3.4394207333984737 Actual: 4.0\n",
      "Pred: 3.8517332256954644 Actual: 3.0\n",
      "Pred: 3.785448534349157 Actual: 5.0\n",
      "Pred: 2.9547834217803026 Actual: 3.0\n",
      "Pred: 3.645695141892647 Actual: 4.5\n",
      "Pred: 4.431913486288564 Actual: 4.5\n",
      "Pred: 3.125759987934732 Actual: 3.0\n",
      "Pred: 3.936886396025243 Actual: 4.5\n",
      "Pred: 3.858023700810851 Actual: 4.0\n",
      "Pred: 3.5209668373305183 Actual: 4.0\n",
      "Pred: 3.7036528374444204 Actual: 2.5\n",
      "Pred: 3.5407985994303797 Actual: 5.0\n",
      "Pred: 3.9086761344508374 Actual: 3.0\n",
      "Pred: 2.9140633393782505 Actual: 1.0\n",
      "Pred: 4.43332327491636 Actual: 4.5\n",
      "Pred: 3.4608819511324964 Actual: 4.0\n",
      "Pred: 3.124089027282366 Actual: 3.0\n",
      "Pred: 2.954820742638687 Actual: 2.0\n",
      "Pred: 4.433323274916359 Actual: 5.0\n",
      "Pred: 3.176549174783226 Actual: 2.0\n",
      "Pred: 3.5389992421337886 Actual: 5.0\n",
      "Pred: 3.384515263444413 Actual: 2.5\n",
      "Pred: 3.120468798063739 Actual: 3.0\n",
      "Pred: 4.009117424667228 Actual: 4.0\n",
      "Pred: 3.5507346559232986 Actual: 3.5\n",
      "Pred: 1.3504443267255117 Actual: 0.5\n",
      "Pred: 3.4391679747719075 Actual: 2.5\n",
      "Pred: 3.7936389539634545 Actual: 2.0\n",
      "Pred: 4.148640110975676 Actual: 5.0\n",
      "Pred: 2.8131248265633766 Actual: 1.0\n",
      "Pred: 4.4333232749163605 Actual: 5.0\n",
      "Pred: 3.4484581907485405 Actual: 4.0\n",
      "Pred: 3.9427422033805963 Actual: 3.5\n",
      "Pred: 3.652958405228837 Actual: 4.0\n",
      "Pred: 4.4333232749163605 Actual: 4.5\n",
      "Pred: 3.626452328577633 Actual: 3.0\n",
      "Pred: 3.545296374739727 Actual: 4.0\n",
      "Pred: 3.321006826439667 Actual: 3.0\n",
      "Pred: 3.059169888688665 Actual: 3.0\n",
      "Pred: 4.43332327491636 Actual: 4.5\n",
      "Pred: 3.7143325653075925 Actual: 4.0\n",
      "Pred: 3.4332286317788134 Actual: 4.0\n",
      "Pred: 3.8170070267731555 Actual: 4.5\n",
      "Pred: 3.8207773705236585 Actual: 5.0\n",
      "Pred: 3.2881632824635143 Actual: 5.0\n",
      "Pred: 4.067135934994022 Actual: 5.0\n",
      "Pred: 3.882972420147373 Actual: 3.5\n",
      "Pred: 3.772912094715719 Actual: 3.5\n",
      "Pred: 4.433323274916359 Actual: 4.0\n",
      "Pred: 3.651643527098792 Actual: 3.5\n",
      "Pred: 3.712971186708847 Actual: 0.5\n",
      "Pred: 2.437704122561642 Actual: 3.0\n",
      "Pred: 4.309225725131994 Actual: 4.0\n",
      "Pred: 3.836824491708239 Actual: 5.0\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 4.094006339415738 Actual: 3.0\n",
      "Pred: 4.127829586318639 Actual: 2.0\n",
      "Pred: 3.2020726266888127 Actual: 3.0\n",
      "Pred: 3.5799823447117927 Actual: 3.0\n",
      "Pred: 3.450935134000864 Actual: 4.0\n",
      "Pred: 2.5848383210238373 Actual: 0.5\n",
      "Pred: 3.0799954482713594 Actual: 4.0\n",
      "Pred: 4.273957468898536 Actual: 4.0\n",
      "Pred: 4.433323274916358 Actual: 5.0\n",
      "Pred: 4.433323274916359 Actual: 4.5\n",
      "Pred: 4.165737144969466 Actual: 5.0\n",
      "Pred: 2.7171812091111613 Actual: 4.0\n",
      "Pred: 3.8819954569845554 Actual: 5.0\n",
      "Pred: 3.197354051951951 Actual: 4.5\n",
      "Pred: 4.307843459599808 Actual: 2.0\n",
      "Pred: 3.3254741735144626 Actual: 3.0\n",
      "Pred: 3.620107376039637 Actual: 3.0\n",
      "Pred: 3.7470462544968512 Actual: 3.0\n",
      "Pred: 4.433323274916359 Actual: 5.0\n",
      "Pred: 3.48726686118268 Actual: 3.5\n",
      "Pred: 4.127857786288301 Actual: 5.0\n",
      "Pred: 4.43332327491636 Actual: 4.0\n",
      "Pred: 3.7716201781877943 Actual: 4.0\n",
      "Pred: 4.151042115172371 Actual: 3.0\n",
      "Pred: 4.12831481205615 Actual: 4.0\n",
      "Pred: 4.147418081539105 Actual: 4.0\n",
      "Pred: 4.433323274916353 Actual: 4.5\n",
      "Pred: 3.954146173177616 Actual: 4.0\n",
      "Pred: 3.7998599805055924 Actual: 4.5\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 4.321781009154366 Actual: 4.0\n",
      "Pred: 3.6291767338233165 Actual: 3.0\n",
      "Pred: 3.68525595452253 Actual: 4.0\n",
      "Pred: 3.9051312794056607 Actual: 3.0\n",
      "Pred: 4.266019569324562 Actual: 4.0\n",
      "Pred: 3.592542340663548 Actual: 3.5\n",
      "Pred: 4.065445285006214 Actual: 4.0\n",
      "Pred: 3.8720320549124656 Actual: 3.5\n",
      "Pred: 4.349657805855346 Actual: 5.0\n",
      "Pred: 3.2963611640894523 Actual: 2.5\n",
      "Pred: 2.652877790088875 Actual: 3.0\n",
      "Pred: 3.6787637473705295 Actual: 4.5\n",
      "Pred: 3.6164009780284108 Actual: 4.0\n",
      "Pred: 3.854346986673301 Actual: 2.0\n",
      "Pred: 3.512340490664907 Actual: 4.0\n",
      "Pred: 3.757671972110441 Actual: 2.0\n",
      "Pred: 3.5510210741480424 Actual: 2.5\n",
      "Pred: 4.4333232749163605 Actual: 4.5\n",
      "Pred: 3.9527151103453764 Actual: 3.0\n",
      "Pred: 4.132315612244768 Actual: 4.0\n",
      "Pred: 3.575398080408398 Actual: 4.0\n",
      "Pred: 3.458880373350626 Actual: 3.5\n",
      "Pred: 3.435315377956951 Actual: 5.0\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 4.43332327491636 Actual: 4.0\n",
      "Pred: 3.269929004142183 Actual: 3.5\n",
      "Pred: 4.33956115230828 Actual: 5.0\n",
      "Pred: 4.321959858304295 Actual: 5.0\n",
      "Pred: 4.43332327491636 Actual: 3.0\n",
      "Pred: 3.4645753510600317 Actual: 4.0\n",
      "Pred: 3.9625961858936933 Actual: 5.0\n",
      "Pred: 3.522951643370799 Actual: 4.0\n",
      "Pred: 4.433323274916355 Actual: 5.0\n",
      "Pred: 3.253294614075185 Actual: 3.0\n",
      "Pred: 4.3453841543804055 Actual: 4.5\n",
      "Pred: 4.154659413992739 Actual: 3.5\n",
      "Pred: 3.5332774399268723 Actual: 4.0\n",
      "Pred: 3.715159036547413 Actual: 5.0\n",
      "Pred: 4.433323274916358 Actual: 5.0\n",
      "Pred: 3.504347432322177 Actual: 4.0\n",
      "Pred: 3.3109733844527938 Actual: 4.0\n",
      "Pred: 4.43332327491636 Actual: 3.5\n",
      "Pred: 4.065832732256198 Actual: 3.5\n",
      "Pred: 3.382104582878995 Actual: 2.0\n",
      "Pred: 3.292546969466698 Actual: 3.0\n",
      "Pred: 3.9497860496614354 Actual: 4.5\n",
      "Pred: 2.9982484496670363 Actual: 1.5\n",
      "Pred: 3.2519905816324286 Actual: 4.0\n",
      "Pred: 2.8430500206501734 Actual: 4.0\n",
      "Pred: 3.903973678939027 Actual: 4.0\n",
      "Pred: 3.5320590239732312 Actual: 2.0\n",
      "Pred: 3.769996076212294 Actual: 3.0\n",
      "Pred: 3.7519570983882984 Actual: 4.0\n",
      "Pred: 3.7067011614520937 Actual: 4.0\n",
      "Pred: 3.7683072982726227 Actual: 4.0\n",
      "Pred: 3.538792886847318 Actual: 3.0\n",
      "Pred: 3.562399002231827 Actual: 3.0\n",
      "Pred: 4.433323274916359 Actual: 5.0\n",
      "Pred: 3.8360215847825443 Actual: 3.0\n",
      "Pred: 3.5544902196341024 Actual: 4.0\n",
      "Pred: 3.497892168618573 Actual: 4.0\n",
      "Pred: 3.3175486440899644 Actual: 5.0\n",
      "Pred: 3.3877267095251122 Actual: 4.0\n",
      "Pred: 4.162994319504023 Actual: 5.0\n",
      "Pred: 3.969316410650205 Actual: 4.0\n",
      "Pred: 3.806623818216747 Actual: 4.0\n",
      "Pred: 3.731255681818755 Actual: 2.0\n",
      "Pred: 4.123381298865931 Actual: 5.0\n",
      "Pred: 3.4021845382999962 Actual: 4.0\n",
      "Pred: 3.8740502668047583 Actual: 5.0\n",
      "Pred: 3.8473475261453256 Actual: 3.0\n",
      "Pred: 3.3556604754330617 Actual: 2.5\n",
      "Pred: 2.814891687477152 Actual: 3.0\n",
      "Pred: 3.690735056437783 Actual: 3.0\n",
      "Pred: 2.2673757095334643 Actual: 2.0\n",
      "Pred: 3.8645774048426125 Actual: 4.5\n",
      "Pred: 3.201379891094006 Actual: 4.0\n",
      "Pred: 2.815904739310349 Actual: 4.0\n",
      "Pred: 3.620962408849472 Actual: 4.0\n",
      "Pred: 3.184030509847607 Actual: 4.0\n",
      "Pred: 3.013099408716302 Actual: 3.0\n",
      "Pred: 3.611611872892853 Actual: 3.5\n",
      "Pred: 3.9117240938810327 Actual: 5.0\n",
      "Pred: 3.8986061503783525 Actual: 4.0\n",
      "Pred: 4.234695357234747 Actual: 5.0\n",
      "Pred: 3.007414905330311 Actual: 5.0\n",
      "Pred: 3.77373646406696 Actual: 3.0\n",
      "Pred: 3.6518045635417655 Actual: 5.0\n",
      "Pred: 2.7002855764513467 Actual: 2.5\n",
      "Pred: 3.3787605106311727 Actual: 4.0\n",
      "Pred: 4.187363503076891 Actual: 4.0\n",
      "Pred: 3.429084826122197 Actual: 4.0\n",
      "Pred: 3.6787511256909102 Actual: 5.0\n",
      "Pred: 3.986768930102468 Actual: 4.0\n",
      "Pred: 3.174565154627683 Actual: 4.0\n",
      "Pred: 3.064623762766136 Actual: 3.0\n",
      "Pred: 3.508757554424903 Actual: 3.5\n",
      "Pred: 3.7740614041298293 Actual: 4.0\n",
      "Pred: 4.433323274916351 Actual: 5.0\n",
      "Pred: 3.3047820928165823 Actual: 5.0\n",
      "Pred: 4.43332327491636 Actual: 4.5\n",
      "Pred: 4.433323274916358 Actual: 5.0\n",
      "Pred: 3.8381872108428365 Actual: 4.0\n",
      "Pred: 3.5497161641242445 Actual: 4.0\n",
      "Pred: 2.941848649310116 Actual: 3.0\n",
      "Pred: 2.9250299136357643 Actual: 1.0\n",
      "Pred: 4.403835559950733 Actual: 3.5\n",
      "Pred: 4.433323274916355 Actual: 4.0\n",
      "Pred: 3.4892354377581385 Actual: 3.0\n",
      "Pred: 3.1983496607135247 Actual: 3.5\n",
      "Pred: 3.6434607110355586 Actual: 4.0\n",
      "Pred: 3.1116841141953038 Actual: 3.0\n",
      "Pred: 3.9081730865091657 Actual: 3.0\n",
      "Pred: 3.6588977602495474 Actual: 3.5\n",
      "Pred: 4.377673428647288 Actual: 4.0\n",
      "Pred: 3.595437543254183 Actual: 2.0\n",
      "Pred: 2.8933504852546856 Actual: 4.5\n",
      "Pred: 4.413569283010355 Actual: 5.0\n",
      "Pred: 3.8365913072677813 Actual: 5.0\n",
      "Pred: 4.313505636149715 Actual: 4.0\n",
      "Pred: 3.65227946439265 Actual: 3.5\n",
      "Pred: 1.7719853016226104 Actual: 1.0\n",
      "Pred: 3.5122803575446357 Actual: 3.0\n",
      "Pred: 3.9849145670912014 Actual: 5.0\n",
      "Pred: 3.81182777742741 Actual: 4.0\n",
      "Pred: 4.180182140171136 Actual: 4.5\n",
      "Pred: 3.9620073194151195 Actual: 4.0\n",
      "Pred: 3.7273460774938285 Actual: 3.5\n",
      "Pred: 4.433323274916358 Actual: 5.0\n",
      "Pred: 4.366496992722142 Actual: 4.0\n",
      "Pred: 4.113566000715703 Actual: 3.5\n",
      "Pred: 3.515383157388443 Actual: 3.0\n",
      "Pred: 3.3405932795629854 Actual: 5.0\n",
      "Pred: 3.6940475253066465 Actual: 1.5\n",
      "Pred: 3.1645150510884825 Actual: 3.5\n",
      "Pred: 3.129608409186412 Actual: 1.0\n",
      "Pred: 2.1048868279664465 Actual: 2.0\n",
      "Pred: 2.6253458334052424 Actual: 3.0\n",
      "Pred: 3.9217850732772273 Actual: 4.0\n",
      "Pred: 3.5430665083974784 Actual: 1.0\n",
      "Pred: 3.796967475657074 Actual: 4.0\n",
      "Pred: 3.891073208109803 Actual: 3.0\n",
      "Pred: 3.081498292320331 Actual: 4.0\n",
      "Pred: 3.836020938953716 Actual: 4.0\n",
      "Pred: 1.8019580016618697 Actual: 3.0\n",
      "Pred: 3.2584051593847922 Actual: 5.0\n",
      "Pred: 3.8685030337101396 Actual: 3.0\n",
      "Pred: 3.4701762348653484 Actual: 3.0\n",
      "Pred: 3.9998187646327894 Actual: 4.0\n",
      "Pred: 3.359345188163178 Actual: 5.0\n",
      "Pred: 3.799704069717348 Actual: 4.0\n",
      "Pred: 4.162185824003785 Actual: 3.5\n",
      "Pred: 3.846462439515995 Actual: 3.0\n",
      "Pred: 4.298106943788796 Actual: 4.0\n",
      "Pred: 2.8984961327460383 Actual: 3.5\n",
      "Pred: 3.713163325585507 Actual: 5.0\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 4.182460866787024 Actual: 3.0\n",
      "Pred: 3.9236645561929864 Actual: 3.0\n",
      "Pred: 2.953289002343847 Actual: 2.0\n",
      "Pred: 3.4437519627062607 Actual: 3.0\n",
      "Pred: 3.6298548152606473 Actual: 3.0\n",
      "Pred: 4.433323274916358 Actual: 4.0\n",
      "Pred: 3.9486826883764263 Actual: 5.0\n",
      "Pred: 3.988428934700121 Actual: 4.0\n",
      "Pred: 4.037175486628954 Actual: 4.5\n",
      "Pred: 3.758714935469138 Actual: 4.0\n",
      "Pred: 3.7207008611485466 Actual: 5.0\n",
      "Pred: 3.0565285852009345 Actual: 2.0\n",
      "Pred: 3.4519200487786397 Actual: 3.0\n",
      "Pred: 3.7713948719789396 Actual: 4.0\n",
      "Pred: 2.097021869901635 Actual: 3.0\n",
      "Pred: 4.352642266377728 Actual: 4.5\n",
      "Pred: 3.8027283322982783 Actual: 3.0\n",
      "Pred: 3.249197876615497 Actual: 3.5\n",
      "Pred: 3.9913842480176913 Actual: 4.0\n",
      "Pred: 4.267662057461107 Actual: 3.5\n",
      "Pred: 4.427620948796748 Actual: 4.0\n",
      "Pred: 3.6674311495250813 Actual: 4.0\n",
      "Pred: 3.721674755381174 Actual: 4.0\n",
      "Pred: 3.366152271604783 Actual: 3.0\n",
      "Pred: 3.7528993926538834 Actual: 3.5\n",
      "Pred: 3.926581594056813 Actual: 4.5\n",
      "Pred: 3.371664428394309 Actual: 4.0\n",
      "Pred: 4.016279335314359 Actual: 3.5\n",
      "Pred: 4.433323274916359 Actual: 5.0\n",
      "Pred: 3.7620779148447734 Actual: 3.0\n",
      "Pred: 3.336297792543674 Actual: 4.0\n",
      "Pred: 3.5040687370349555 Actual: 3.0\n",
      "Pred: 3.4662313873851027 Actual: 3.0\n",
      "Pred: 3.487247308990083 Actual: 4.0\n",
      "Pred: 4.031689998117874 Actual: 4.5\n",
      "Pred: 4.367976347402037 Actual: 3.5\n",
      "Pred: 4.279922501451502 Actual: 5.0\n",
      "Pred: 3.6664854933672872 Actual: 3.5\n",
      "Pred: 3.7851036917401473 Actual: 4.0\n",
      "Pred: 4.115302677329885 Actual: 5.0\n",
      "Pred: 4.005394855131105 Actual: 3.0\n",
      "Pred: 3.3071714747479986 Actual: 3.0\n",
      "Pred: 3.5891037287964536 Actual: 4.0\n",
      "Pred: 3.361171290737764 Actual: 3.0\n",
      "Pred: 4.006444642395885 Actual: 3.5\n",
      "Pred: 4.433323274916355 Actual: 4.0\n",
      "Pred: 2.5347130979976193 Actual: 4.0\n",
      "Pred: 2.7222822269433338 Actual: 1.0\n",
      "Pred: 3.764983048371917 Actual: 4.0\n",
      "Pred: 3.6695373305717096 Actual: 5.0\n",
      "Pred: 2.7461033717112833 Actual: 3.0\n",
      "Pred: 3.5771606747649396 Actual: 4.0\n",
      "Pred: 3.4764991684821154 Actual: 4.0\n",
      "Pred: 3.9514248930394253 Actual: 5.0\n",
      "Pred: 4.433323274916359 Actual: 3.0\n",
      "Pred: 3.654828599695391 Actual: 4.0\n",
      "Pred: 4.1804914369603585 Actual: 3.0\n",
      "Pred: 4.143393647229202 Actual: 4.0\n",
      "Pred: 3.7300034611588178 Actual: 2.5\n",
      "Pred: 2.8854293938534523 Actual: 3.0\n",
      "Pred: 3.2421565274444353 Actual: 4.0\n",
      "Pred: 2.7110402386359485 Actual: 3.0\n",
      "Pred: 3.5788868621058687 Actual: 3.5\n",
      "Pred: 3.706414017027656 Actual: 4.0\n",
      "Pred: 3.990129663683953 Actual: 3.0\n",
      "Pred: 3.0888143803478463 Actual: 3.0\n",
      "Pred: 3.844750284360568 Actual: 5.0\n",
      "Pred: 4.102807551591081 Actual: 4.0\n",
      "Pred: 2.562547894466867 Actual: 1.0\n",
      "Pred: 3.970098010773082 Actual: 3.0\n",
      "Pred: 3.6162833817386457 Actual: 4.5\n",
      "Pred: 3.5857383279383876 Actual: 3.0\n",
      "Pred: 3.8547691234305 Actual: 4.0\n",
      "Pred: 3.6453165873937174 Actual: 4.0\n",
      "Pred: 3.5226113990350347 Actual: 3.5\n",
      "Pred: 3.5744159922597736 Actual: 3.0\n",
      "Pred: 4.433323274916359 Actual: 5.0\n",
      "Pred: 3.8848229478919505 Actual: 4.5\n",
      "Pred: 3.515677765637721 Actual: 3.0\n",
      "Pred: 3.4228462118830927 Actual: 3.5\n",
      "Pred: 2.387109306547838 Actual: 3.0\n",
      "Pred: 3.6680670703770972 Actual: 3.0\n",
      "Pred: 3.4882148152510064 Actual: 1.0\n",
      "Pred: 3.7634948119202187 Actual: 5.0\n",
      "Pred: 3.4693794566871583 Actual: 4.5\n",
      "Pred: 2.991258957967049 Actual: 4.5\n",
      "Pred: 2.7254595507630675 Actual: 3.0\n",
      "Pred: 3.6098233041197396 Actual: 3.0\n",
      "Pred: 4.433306077501814 Actual: 5.0\n",
      "Pred: 2.6066710735077945 Actual: 2.0\n",
      "Pred: 4.433323274916353 Actual: 5.0\n",
      "Pred: 3.4159693930742567 Actual: 3.0\n",
      "Pred: 3.7364188122640667 Actual: 5.0\n",
      "Pred: 3.8211514207340884 Actual: 3.0\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 3.322828814936531 Actual: 3.0\n",
      "Pred: 4.014393444496659 Actual: 5.0\n",
      "Pred: 3.667054181131649 Actual: 3.0\n",
      "Pred: 3.810094972134038 Actual: 4.0\n",
      "Pred: 4.3866532699339 Actual: 4.0\n",
      "Pred: 4.4333232749163605 Actual: 5.0\n",
      "Pred: 3.6628976899479477 Actual: 3.0\n",
      "Pred: 3.0751883593624485 Actual: 2.0\n",
      "Pred: 3.084022129344338 Actual: 3.0\n",
      "Pred: 3.4036209030342897 Actual: 2.0\n",
      "Pred: 4.074673389942969 Actual: 2.0\n",
      "Pred: 4.433323274916353 Actual: 4.5\n",
      "Pred: 4.4333232749163525 Actual: 4.0\n",
      "Pred: 4.312681140517771 Actual: 4.5\n",
      "Pred: 3.77799540610084 Actual: 4.0\n",
      "Pred: 4.014933160301802 Actual: 4.0\n",
      "Pred: 3.8358368115601422 Actual: 5.0\n",
      "Pred: 4.0076909937685885 Actual: 3.5\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 3.927475120654193 Actual: 2.0\n",
      "Pred: 4.1173253627114486 Actual: 5.0\n",
      "Pred: 3.9722421560687597 Actual: 2.0\n",
      "Pred: 3.806684017736368 Actual: 3.0\n",
      "Pred: 4.015247404465858 Actual: 4.0\n",
      "Pred: 4.433323274916358 Actual: 4.0\n",
      "Pred: 3.2341392314857775 Actual: 2.0\n",
      "Pred: 2.270605953858238 Actual: 3.0\n",
      "Pred: 3.9279982945010428 Actual: 5.0\n",
      "Pred: 3.631853055387996 Actual: 4.0\n",
      "Pred: 3.8490615500388468 Actual: 4.0\n",
      "Pred: 3.894819078375351 Actual: 4.5\n",
      "Pred: 3.875826142544906 Actual: 4.0\n",
      "Pred: 2.9389144146812547 Actual: 2.0\n",
      "Pred: 4.028482352491077 Actual: 4.0\n",
      "Pred: 3.8674526472150093 Actual: 5.0\n",
      "Pred: 4.067217693432546 Actual: 4.0\n",
      "Pred: 3.4433867724901077 Actual: 3.0\n",
      "Pred: 3.289219513047625 Actual: 3.0\n",
      "Pred: 3.5362908745956045 Actual: 3.0\n",
      "Pred: 2.604530514131685 Actual: 2.0\n",
      "Pred: 3.563590230009371 Actual: 4.0\n",
      "Pred: 3.6609775120766654 Actual: 3.5\n",
      "Pred: 3.7753405306736925 Actual: 5.0\n",
      "Pred: 4.006853435002907 Actual: 3.0\n",
      "Pred: 3.245934631832761 Actual: 2.0\n",
      "Pred: 4.036821288030414 Actual: 3.0\n",
      "Pred: 3.28551021212065 Actual: 3.0\n",
      "Pred: 3.854916537462019 Actual: 4.0\n",
      "Pred: 3.4323861433206244 Actual: 3.0\n",
      "Pred: 3.624234491332288 Actual: 2.0\n",
      "Pred: 3.331259627143334 Actual: 5.0\n",
      "Pred: 3.2813255063693036 Actual: 3.0\n",
      "Pred: 3.1777860548184176 Actual: 3.0\n",
      "Pred: 3.4148561973835028 Actual: 4.0\n",
      "Pred: 3.398314307460183 Actual: 3.0\n",
      "Pred: 3.107415665451142 Actual: 3.0\n",
      "Pred: 3.9580893329275075 Actual: 1.5\n",
      "Pred: 2.2923209437178484 Actual: 3.5\n",
      "Pred: 2.3697966882977743 Actual: 1.0\n",
      "Pred: 3.6984869564191056 Actual: 2.0\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 4.151172683676213 Actual: 4.0\n",
      "Pred: 3.7848432291596543 Actual: 3.0\n",
      "Pred: 2.9307802712286994 Actual: 2.0\n",
      "Pred: 3.8896275690379096 Actual: 2.5\n",
      "Pred: 4.374825951617455 Actual: 5.0\n",
      "Pred: 3.0278934253196406 Actual: 2.5\n",
      "Pred: 3.476075099929983 Actual: 3.5\n",
      "Pred: 4.32655537444384 Actual: 5.0\n",
      "Pred: 4.199377885408798 Actual: 5.0\n",
      "Pred: 4.069374619707004 Actual: 3.0\n",
      "Pred: 4.263349824967562 Actual: 4.5\n",
      "Pred: 3.937219846199537 Actual: 4.0\n",
      "Pred: 4.184443996303994 Actual: 3.0\n",
      "Pred: 1.8929276431947129 Actual: 1.0\n",
      "Pred: 3.528754605243815 Actual: 0.5\n",
      "Pred: 3.554386803828787 Actual: 3.5\n",
      "Pred: 3.162420938419492 Actual: 2.0\n",
      "Pred: 3.9894736723847783 Actual: 5.0\n",
      "Pred: 3.7819170888391582 Actual: 2.0\n",
      "Pred: 3.1483759953893102 Actual: 3.5\n",
      "Pred: 3.8873462222250086 Actual: 5.0\n",
      "Pred: 3.6843847829392122 Actual: 1.0\n",
      "Pred: 4.43332327491636 Actual: 4.0\n",
      "Pred: 3.0367534169019943 Actual: 4.0\n",
      "Pred: 3.3163867142850374 Actual: 1.0\n",
      "Pred: 4.359658847421916 Actual: 4.0\n",
      "Pred: 4.432702822004922 Actual: 4.0\n",
      "Pred: 3.0891752531229213 Actual: 2.5\n",
      "Pred: 3.3297947012441877 Actual: 4.0\n",
      "Pred: 3.5755069720479646 Actual: 4.5\n",
      "Pred: 2.506489441393827 Actual: 2.0\n",
      "Pred: 4.433323274916359 Actual: 3.5\n",
      "Pred: 3.5885114424929467 Actual: 5.0\n",
      "Pred: 3.8467645432412567 Actual: 3.0\n",
      "Pred: 2.979801772757349 Actual: 3.0\n",
      "Pred: 3.250570583898694 Actual: 3.0\n",
      "Pred: 4.171761801452245 Actual: 4.5\n",
      "Pred: 3.5827764403900724 Actual: 3.0\n",
      "Pred: 4.433323274916354 Actual: 5.0\n",
      "Pred: 3.1996954573999075 Actual: 3.0\n",
      "Pred: 3.774170937805861 Actual: 3.0\n",
      "Pred: 2.877798098200355 Actual: 3.0\n",
      "Pred: 3.3464297301017574 Actual: 3.0\n",
      "Pred: 4.064702117100007 Actual: 5.0\n",
      "Pred: 3.7814536743741187 Actual: 3.0\n",
      "Pred: 3.2426374573559538 Actual: 3.0\n",
      "Pred: 2.4717395121990267 Actual: 3.0\n",
      "Pred: 3.5305618829853675 Actual: 3.0\n",
      "Pred: 4.43332327491636 Actual: 4.5\n",
      "Pred: 3.448247406385122 Actual: 3.0\n",
      "Pred: 4.096105887591101 Actual: 3.0\n",
      "Pred: 2.952739989131722 Actual: 4.0\n",
      "Pred: 2.919015810501663 Actual: 3.0\n",
      "Pred: 3.9306142825783565 Actual: 4.5\n",
      "Pred: 3.7041244782844793 Actual: 4.0\n",
      "Pred: 3.0866543166370675 Actual: 3.0\n",
      "Pred: 3.1361947379680846 Actual: 3.0\n",
      "Pred: 3.2553237672460793 Actual: 3.5\n",
      "Pred: 2.811907274238561 Actual: 2.0\n",
      "Pred: 4.175219536357904 Actual: 5.0\n",
      "Pred: 3.9468453467631393 Actual: 4.0\n",
      "Pred: 4.201278368450932 Actual: 3.0\n",
      "Pred: 3.9032962510834746 Actual: 3.0\n",
      "Pred: 2.6762045717967116 Actual: 5.0\n",
      "Pred: 4.433323274916359 Actual: 4.0\n",
      "Pred: 3.177315950753835 Actual: 3.0\n",
      "Pred: 2.738723024746155 Actual: 1.0\n",
      "Pred: 3.6731322756834475 Actual: 3.0\n",
      "Pred: 4.433323274916359 Actual: 4.5\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 4.43332327491636 Actual: 3.5\n",
      "Pred: 4.430632854568896 Actual: 4.5\n",
      "Pred: 3.2299143761807145 Actual: 1.0\n",
      "Pred: 2.3480463870430666 Actual: 1.0\n",
      "Pred: 4.2787138607244675 Actual: 4.5\n",
      "Pred: 1.9707095106861112 Actual: 1.5\n",
      "Pred: 3.595600619654092 Actual: 5.0\n",
      "Pred: 3.8703351898387086 Actual: 3.5\n",
      "Pred: 4.085890071427731 Actual: 3.0\n",
      "Pred: 2.2475037913598084 Actual: 4.0\n",
      "Pred: 3.537807594673096 Actual: 2.0\n",
      "Pred: 3.3662937557851245 Actual: 2.5\n",
      "Pred: 2.949658437712114 Actual: 3.0\n",
      "Pred: 3.882094678919762 Actual: 5.0\n",
      "Pred: 3.331657236904827 Actual: 3.0\n",
      "Pred: 4.023307599327766 Actual: 4.0\n",
      "Pred: 3.848855139505638 Actual: 5.0\n",
      "Pred: 3.500533143153683 Actual: 5.0\n",
      "Pred: 2.647935489642448 Actual: 5.0\n",
      "Pred: 3.776985752321218 Actual: 4.5\n",
      "Pred: 3.550299274154592 Actual: 4.0\n",
      "Pred: 3.460619848753568 Actual: 1.5\n",
      "Pred: 2.9961482463040814 Actual: 3.0\n",
      "Pred: 3.7737448321095792 Actual: 3.5\n",
      "Pred: 3.715742288044848 Actual: 3.0\n",
      "Pred: 4.031196827191779 Actual: 4.0\n",
      "Pred: 3.9929675895599637 Actual: 4.0\n",
      "Pred: 4.433323274916358 Actual: 4.0\n",
      "Pred: 3.4867808395720736 Actual: 4.0\n",
      "Pred: 3.604452918372575 Actual: 5.0\n",
      "Pred: 4.024642459792998 Actual: 4.0\n",
      "Pred: 3.4147932642485257 Actual: 5.0\n",
      "Pred: 3.0020542948247004 Actual: 3.0\n",
      "Pred: 3.6767663097080083 Actual: 3.5\n",
      "Pred: 4.015795399927809 Actual: 3.0\n",
      "Pred: 4.0613934168586 Actual: 3.0\n",
      "Pred: 4.369217697739718 Actual: 5.0\n",
      "Pred: 4.013673685835565 Actual: 4.0\n",
      "Pred: 4.379980613272512 Actual: 5.0\n",
      "Pred: 3.5232103586737558 Actual: 3.0\n",
      "Pred: 4.433323274916359 Actual: 5.0\n",
      "Pred: 3.855732200644317 Actual: 5.0\n",
      "Pred: 3.514891486279878 Actual: 4.0\n",
      "Pred: 4.433323274916359 Actual: 4.0\n",
      "Pred: 4.2920807089730175 Actual: 4.0\n",
      "Pred: 4.357643455128638 Actual: 5.0\n",
      "Pred: 2.876202041651304 Actual: 4.0\n",
      "Pred: 3.328582905139143 Actual: 4.0\n",
      "Pred: 4.019922785993145 Actual: 4.5\n",
      "Pred: 3.856404862995542 Actual: 1.0\n",
      "Pred: 2.486792425713898 Actual: 2.0\n",
      "Pred: 2.517001430582242 Actual: 2.5\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 3.76056591273018 Actual: 4.0\n",
      "Pred: 3.940629564609826 Actual: 5.0\n",
      "Pred: 3.1133712795553476 Actual: 2.0\n",
      "Pred: 2.9881798091988836 Actual: 3.0\n",
      "Pred: 3.4620156975739103 Actual: 2.0\n",
      "Pred: 3.5401281836233487 Actual: 3.0\n",
      "Pred: 3.9333954827014868 Actual: 4.0\n",
      "Pred: 4.264444131794202 Actual: 4.0\n",
      "Pred: 3.014805201016913 Actual: 4.0\n",
      "Pred: 4.284468595539141 Actual: 4.0\n",
      "Pred: 2.4749267303191322 Actual: 1.5\n",
      "Pred: 3.7652658420318064 Actual: 3.0\n",
      "Pred: 3.89967505867355 Actual: 3.0\n",
      "Pred: 3.920343279000841 Actual: 3.0\n",
      "Pred: 3.7660089206372773 Actual: 3.0\n",
      "Pred: 3.150601944218095 Actual: 3.0\n",
      "Pred: 4.433323274916354 Actual: 4.5\n",
      "Pred: 4.433323274916352 Actual: 5.0\n",
      "Pred: 3.5037515446237832 Actual: 3.0\n",
      "Pred: 3.9693513289279467 Actual: 5.0\n",
      "Pred: 3.5454888011866252 Actual: 3.0\n",
      "Pred: 3.758746181296735 Actual: 4.0\n",
      "Pred: 2.8273145854365738 Actual: 2.0\n",
      "Pred: 4.411360878007369 Actual: 4.0\n",
      "Pred: 3.696352879088184 Actual: 3.0\n",
      "Pred: 3.359344014031702 Actual: 4.0\n",
      "Pred: 4.425615801552384 Actual: 4.5\n",
      "Pred: 3.8501250747839664 Actual: 5.0\n",
      "Pred: 1.3504443267255117 Actual: 0.5\n",
      "Pred: 3.417029461289385 Actual: 3.0\n",
      "Pred: 3.728785184750691 Actual: 4.5\n",
      "Pred: 3.52500864208579 Actual: 2.0\n",
      "Pred: 3.5270751210878157 Actual: 5.0\n",
      "Pred: 3.514253114239572 Actual: 3.0\n",
      "Pred: 2.864040176635561 Actual: 4.0\n",
      "Pred: 3.632009309587324 Actual: 4.0\n",
      "Pred: 1.9109820097915613 Actual: 1.5\n",
      "Pred: 3.4403906482097253 Actual: 4.5\n",
      "Pred: 3.5263594781908507 Actual: 3.5\n",
      "Pred: 2.0894467414616096 Actual: 1.0\n",
      "Pred: 2.586301960874966 Actual: 3.0\n",
      "Pred: 4.0701649117975265 Actual: 4.0\n",
      "Pred: 3.324424816815431 Actual: 4.0\n",
      "Pred: 3.5971509033754105 Actual: 4.0\n",
      "Pred: 3.9210838073267067 Actual: 5.0\n",
      "Pred: 4.333521414756319 Actual: 4.0\n",
      "Pred: 3.4879729092815266 Actual: 5.0\n",
      "Pred: 4.4323704958722026 Actual: 5.0\n",
      "Pred: 3.5771096370700044 Actual: 3.5\n",
      "Pred: 3.400683909706997 Actual: 3.0\n",
      "Pred: 4.401848236838345 Actual: 5.0\n",
      "Pred: 3.724907957767738 Actual: 3.0\n",
      "Pred: 3.828719395437186 Actual: 4.0\n",
      "Pred: 3.013967988146719 Actual: 3.0\n",
      "Pred: 3.877223972042041 Actual: 3.0\n",
      "Pred: 4.353104726033897 Actual: 5.0\n",
      "Pred: 3.832611725574239 Actual: 4.5\n",
      "Pred: 3.8009146916854726 Actual: 3.0\n",
      "Pred: 4.4333232749163605 Actual: 4.0\n",
      "Pred: 4.285311630838847 Actual: 5.0\n",
      "Pred: 3.9404938156245652 Actual: 4.5\n",
      "Pred: 4.43332327491636 Actual: 5.0\n",
      "Pred: 3.612746381522702 Actual: 3.0\n",
      "Pred: 3.5714195230009924 Actual: 3.0\n",
      "Pred: 3.803527244644957 Actual: 3.0\n",
      "Pred: 3.5046160708484306 Actual: 3.5\n",
      "Pred: 4.1024436540600995 Actual: 4.0\n",
      "Pred: 4.180035551408546 Actual: 4.5\n",
      "Pred: 3.7586431941785823 Actual: 4.0\n",
      "Pred: 3.7933841704230904 Actual: 4.5\n",
      "Pred: 3.1664580761619607 Actual: 3.0\n",
      "Pred: 3.015416648398811 Actual: 4.0\n",
      "Pred: 3.785909249912364 Actual: 4.0\n",
      "Pred: 4.433323274916359 Actual: 4.0\n",
      "Pred: 3.4662220341832053 Actual: 3.0\n",
      "Pred: 3.832922634362629 Actual: 4.0\n",
      "Pred: 4.433323274916355 Actual: 5.0\n",
      "Pred: 4.433323274916354 Actual: 4.0\n",
      "Pred: 3.89514575473033 Actual: 4.0\n",
      "Pred: 4.43332327491636 Actual: 3.0\n",
      "Pred: 3.3014046655257823 Actual: 3.0\n",
      "Pred: 3.1958867754853397 Actual: 3.0\n",
      "Pred: 3.658745793320607 Actual: 4.0\n",
      "Pred: 3.476522035382189 Actual: 2.5\n",
      "Pred: 3.965053377102111 Actual: 3.0\n",
      "Pred: 4.125659142026875 Actual: 4.0\n",
      "Pred: 4.43332327491636 Actual: 2.5\n",
      "Pred: 3.6718593201044167 Actual: 4.5\n",
      "Pred: 2.6162998237824664 Actual: 1.5\n",
      "Pred: 3.8317418490321384 Actual: 4.0\n",
      "Pred: 4.3288629046954 Actual: 4.0\n",
      "Pred: 4.229789563953465 Actual: 4.0\n",
      "Pred: 2.9038480772477784 Actual: 3.0\n",
      "Pred: 2.9672188032847133 Actual: 3.0\n",
      "Pred: 4.118724898927605 Actual: 5.0\n",
      "Pred: 3.954526469350886 Actual: 5.0\n",
      "Pred: 4.43332327491636 Actual: 4.0\n",
      "Pred: 3.715395195351517 Actual: 5.0\n",
      "Pred: 2.940199779402635 Actual: 4.5\n",
      "Pred: 3.240103312942325 Actual: 3.0\n",
      "Pred: 2.9784772515565816 Actual: 3.0\n",
      "Pred: 3.1116410814115323 Actual: 4.0\n",
      "Pred: 3.445917699464339 Actual: 3.0\n",
      "Pred: 3.569622785758027 Actual: 4.0\n",
      "Pred: 3.96371034794814 Actual: 3.5\n",
      "Pred: 4.253740532343414 Actual: 5.0\n",
      "Pred: 3.9319736367614424 Actual: 5.0\n",
      "Pred: 3.4952817132173006 Actual: 3.0\n",
      "Pred: 3.768226894984802 Actual: 1.5\n",
      "Pred: 3.9556073379575376 Actual: 4.0\n",
      "Pred: 3.977092521540388 Actual: 2.0\n",
      "Pred: 3.6236053469793483 Actual: 5.0\n",
      "Pred: 3.6596933955353013 Actual: 4.5\n",
      "Pred: 3.4819581143379534 Actual: 3.0\n",
      "Pred: 3.8675097572104735 Actual: 4.0\n",
      "Pred: 3.6708879552962745 Actual: 4.0\n",
      "Pred: 3.3265023898680055 Actual: 3.0\n",
      "Pred: 3.5552547740701033 Actual: 3.0\n",
      "Pred: 3.708100714030929 Actual: 5.0\n",
      "Pred: 3.7994183491971625 Actual: 5.0\n",
      "Pred: 4.4333232749163605 Actual: 3.5\n",
      "Pred: 4.089044353171082 Actual: 5.0\n",
      "Pred: 4.136927960387693 Actual: 5.0\n",
      "Pred: 3.8013223672917587 Actual: 4.0\n",
      "Pred: 3.8745345572129284 Actual: 3.0\n",
      "Pred: 4.0676886539829855 Actual: 2.0\n",
      "Pred: 3.610578242266557 Actual: 5.0\n",
      "Pred: 3.2970826133491915 Actual: 3.5\n",
      "Pred: 4.067857424678845 Actual: 3.0\n",
      "Pred: 3.376555222703189 Actual: 3.5\n",
      "Pred: 3.674975257814117 Actual: 3.5\n",
      "Pred: 4.397838392094569 Actual: 4.0\n",
      "Pred: 3.643075052784106 Actual: 4.0\n",
      "Pred: 4.381421552969071 Actual: 2.0\n",
      "Pred: 2.3962667810700697 Actual: 3.0\n",
      "Pred: 3.70367938599191 Actual: 3.0\n",
      "Pred: 4.152650916983275 Actual: 5.0\n",
      "Pred: 3.507018801046436 Actual: 5.0\n",
      "Pred: 4.293122526061059 Actual: 4.0\n",
      "Pred: 4.23381533385051 Actual: 3.0\n",
      "Pred: 3.05289281299257 Actual: 4.0\n",
      "Pred: 3.1958823939445837 Actual: 3.0\n",
      "Pred: 3.6517889499918494 Actual: 3.0\n",
      "Pred: 3.5844370262042236 Actual: 4.5\n",
      "Pred: 2.6238133131777754 Actual: 2.5\n",
      "Pred: 2.7769001587789446 Actual: 4.0\n",
      "Pred: 2.8886131598914715 Actual: 3.0\n",
      "Pred: 4.302247966936069 Actual: 4.0\n",
      "Pred: 4.42960298168342 Actual: 4.0\n",
      "Pred: 4.433323274916358 Actual: 5.0\n",
      "Pred: 3.734697445402478 Actual: 5.0\n",
      "Pred: 3.5017306103416637 Actual: 3.0\n",
      "Pred: 4.433323274916358 Actual: 5.0\n",
      "Pred: 3.609313307761782 Actual: 3.0\n",
      "Pred: 3.8954913873256825 Actual: 3.0\n",
      "Pred: 3.3135666904690178 Actual: 5.0\n",
      "Pred: 3.09195534853028 Actual: 2.0\n",
      "Pred: 2.9783334602697793 Actual: 4.0\n",
      "Pred: 2.5725221447085183 Actual: 1.5\n",
      "Pred: 4.43332327491636 Actual: 3.5\n",
      "Pred: 3.9314789590491785 Actual: 4.0\n",
      "Pred: 2.951394550074611 Actual: 3.0\n",
      "Pred: 3.8267419155392965 Actual: 3.0\n",
      "Pred: 3.7161213495738905 Actual: 3.0\n",
      "Pred: 3.9524965751647896 Actual: 4.0\n",
      "Pred: 2.854652496763819 Actual: 3.5\n",
      "Pred: 3.3644550717969723 Actual: 3.0\n",
      "Pred: 2.8227717528965144 Actual: 2.0\n",
      "Pred: 3.360579415985514 Actual: 4.0\n",
      "Pred: 3.820795159174786 Actual: 3.5\n",
      "Pred: 3.831243836199969 Actual: 3.0\n",
      "Pred: 4.396167954520535 Actual: 4.0\n",
      "Pred: 4.205413334138948 Actual: 3.5\n",
      "Pred: 3.415851899577508 Actual: 3.0\n",
      "Pred: 3.8924389531122685 Actual: 5.0\n",
      "Pred: 3.6463332810287197 Actual: 3.0\n",
      "Pred: 3.673708942248122 Actual: 3.5\n",
      "Pred: 3.841774789387979 Actual: 5.0\n",
      "Pred: 2.070007383952088 Actual: 0.5\n",
      "Pred: 4.090770332547901 Actual: 4.0\n",
      "Pred: 4.433323274916357 Actual: 5.0\n",
      "Pred: 3.376699575252666 Actual: 4.0\n",
      "Pred: 3.217287696799769 Actual: 3.5\n",
      "Pred: 4.056473848518992 Actual: 3.0\n",
      "Pred: 3.4495053040407115 Actual: 4.0\n",
      "Pred: 2.984896361283475 Actual: 3.0\n",
      "Pred: 3.5951388023612636 Actual: 3.0\n",
      "Pred: 3.0867853390477094 Actual: 4.0\n",
      "Pred: 4.433323274916359 Actual: 5.0\n",
      "Pred: 3.612470974421485 Actual: 3.0\n",
      "Pred: 3.3407693551102486 Actual: 5.0\n",
      "Pred: 3.727933305357191 Actual: 3.0\n",
      "Pred: 3.343243261580078 Actual: 1.0\n",
      "Pred: 2.7476320481947485 Actual: 5.0\n",
      "Pred: 2.8428962925370587 Actual: 3.0\n",
      "Pred: 3.4885228630041416 Actual: 2.5\n",
      "Pred: 4.406855901261227 Actual: 4.0\n",
      "Pred: 3.2326588108556233 Actual: 3.0\n",
      "Pred: 3.9569865991668642 Actual: 5.0\n",
      "Pred: 4.410948300712192 Actual: 4.0\n",
      "Pred: 3.5292764939524752 Actual: 3.0\n",
      "Pred: 3.191639718692127 Actual: 3.0\n",
      "Pred: 4.43332327491636 Actual: 4.0\n",
      "Pred: 3.5423455053349855 Actual: 4.0\n",
      "Pred: 3.8216101067013093 Actual: 4.0\n",
      "Pred: 3.4368426934773013 Actual: 3.5\n",
      "Pred: 3.729319164331359 Actual: 4.5\n",
      "Pred: 2.3125979303401967 Actual: 2.5\n",
      "Pred: 4.0428627201471485 Actual: 5.0\n",
      "Pred: 3.859597600161429 Actual: 4.0\n",
      "Pred: 3.1476405722331955 Actual: 3.0\n",
      "Pred: 3.8537003444437246 Actual: 4.0\n",
      "Pred: 4.37304725327236 Actual: 5.0\n",
      "Pred: 3.7729062138258693 Actual: 3.0\n",
      "Pred: 3.7905575290720748 Actual: 3.0\n",
      "Pred: 3.275618051485691 Actual: 2.5\n",
      "Pred: 4.079867591500507 Actual: 4.0\n",
      "Pred: 3.2338262640108586 Actual: 4.0\n",
      "Pred: 4.433323274916355 Actual: 4.5\n",
      "Pred: 3.3535526996532803 Actual: 3.0\n",
      "Pred: 3.7875575085036384 Actual: 1.0\n",
      "Pred: 4.050693932744826 Actual: 5.0\n",
      "Pred: 3.4628157412420126 Actual: 3.5\n",
      "Pred: 4.203115728745528 Actual: 4.5\n",
      "Pred: 4.154498942110347 Actual: 3.0\n",
      "Pred: 3.67762142945314 Actual: 3.0\n",
      "Pred: 4.4333232749163605 Actual: 3.0\n",
      "Pred: 2.6982164768090415 Actual: 3.0\n",
      "Pred: 3.403867480249177 Actual: 4.0\n",
      "Pred: 4.4333232749163605 Actual: 4.0\n",
      "Pred: 3.3911547131457636 Actual: 5.0\n",
      "Pred: 4.433323274916357 Actual: 5.0\n",
      "Pred: 4.398882619615558 Actual: 4.5\n",
      "Pred: 4.094367238751157 Actual: 4.5\n",
      "Pred: 3.8849236301121737 Actual: 4.5\n",
      "Pred: 3.6980866821457536 Actual: 3.0\n",
      "Pred: 4.239948162246833 Actual: 0.5\n",
      "Pred: 3.8442073559513035 Actual: 5.0\n",
      "Pred: 4.021708085282185 Actual: 4.5\n",
      "Pred: 3.6775067016448393 Actual: 4.0\n",
      "Pred: 3.57418024965564 Actual: 1.0\n",
      "Pred: 3.6844693726101574 Actual: 3.0\n",
      "Pred: 3.5796251068963607 Actual: 3.0\n",
      "Pred: 3.578570757510189 Actual: 3.0\n",
      "Pred: 3.4923622932874956 Actual: 2.5\n",
      "Pred: 4.130800757614463 Actual: 5.0\n",
      "Pred: 3.9345422019309586 Actual: 4.0\n",
      "Pred: 3.8227562776579993 Actual: 4.0\n",
      "Pred: 3.009460734774152 Actual: 1.5\n",
      "Pred: 3.403027056196935 Actual: 3.0\n",
      "Pred: 3.9506019213339116 Actual: 5.0\n",
      "Pred: 3.1456158915915284 Actual: 3.0\n",
      "Pred: 3.8328203943823382 Actual: 3.5\n",
      "Pred: 2.7008055527012576 Actual: 3.0\n",
      "Pred: 2.9436244743798703 Actual: 1.0\n",
      "Pred: 3.876992658015884 Actual: 4.0\n",
      "Pred: 3.899707622834365 Actual: 4.0\n",
      "Pred: 3.5802686724746065 Actual: 4.0\n",
      "Pred: 2.9462868009979433 Actual: 2.0\n",
      "Pred: 2.3519386573625853 Actual: 2.0\n",
      "Pred: 2.5556498166691894 Actual: 3.0\n",
      "Pred: 3.7881221458824275 Actual: 4.0\n",
      "Pred: 3.1302164690266125 Actual: 3.0\n",
      "Pred: 3.0296685246216764 Actual: 1.0\n",
      "Pred: 2.8702475262473306 Actual: 4.0\n",
      "Pred: 3.140591330429221 Actual: 3.0\n",
      "overall score: 0.2667063296881431\n",
      "Minutes: 0.22508241335550944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#test the time taken to train and predict\n",
    "start = time.time()\n",
    "\n",
    "#this is where you may select certain features to be used to build the model\n",
    "new_user_to_features = dict()\n",
    "\n",
    "for key in user_to_features.keys():\n",
    "    new_list = []\n",
    "    for item in user_to_features[key]:\n",
    "        # can try reducing the features like below:\n",
    "        # item[0:4]+ item[6:7]+ item[10:11]\n",
    "        # item[0:4]\n",
    "        new_list.append(item[0:4])\n",
    "    new_user_to_features[key] = new_list\n",
    "\n",
    "#seed\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#instead of using test train split...\n",
    "user_to_X_train = dict()\n",
    "user_to_y_train = dict()\n",
    "user_to_X_test = dict()\n",
    "user_to_y_test = dict()\n",
    "\n",
    "#There is a problem with using the same users in training and testing and this code ensures that it doesn't happen\n",
    "#The model should beable to be used effectively for new users and not just memorized for existing users\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "for key in new_user_to_features.keys():\n",
    "    if(random.randint(0,10) == 0):\n",
    "        user_to_X_test[key] = new_user_to_features[key]\n",
    "        user_to_y_test[key] = user_to_rand_rating[key]\n",
    "        c1+=1\n",
    "\n",
    "    else:\n",
    "        user_to_X_train[key] = new_user_to_features[key]\n",
    "        user_to_y_train[key] = user_to_rand_rating[key]\n",
    "        c2+=1\n",
    "\n",
    "#used to train model\n",
    "X_train = [] \n",
    "y_train = []\n",
    "\n",
    "#populate X_train and y_train\n",
    "for key in user_to_X_train.keys():\n",
    "    for item in user_to_X_train[key]:\n",
    "        X_train.append(item)\n",
    "        y_train.append(user_to_y_train[key])\n",
    "\n",
    "\n",
    "# scale training features...\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "\n",
    "\n",
    "#train model\n",
    "layers = (2,2,2)\n",
    "# act = \"tanh\"\n",
    "# solve = \"adam\"\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# act = \"relu\"\n",
    "# solve = \"sgd\"\n",
    "# act = \"tanh\"\n",
    "# solve = \"sgd\"\n",
    "act = \"relu\"\n",
    "solve = \"adam\"\n",
    "\n",
    "\n",
    "regr = MLPRegressor(hidden_layer_sizes=layers,activation =act, solver =solve,  max_iter=10000, random_state =seed_int)\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "print(regr.n_iter_)\n",
    "\n",
    "#dictionary of users to test features that have been scaled\n",
    "new_user_to_X_test = dict()\n",
    "\n",
    "# used to scale test features then the new scaled features are returned ...\n",
    "# as the values of the approriate user key in new_user_to_X_test \n",
    "X_test = []\n",
    "\n",
    "#populate X_test, key, and counts that are later used to build new_user_to_X_test, a verison of...\n",
    "#user_to_X_test with scaled features \n",
    "#need to decompose then recompose\n",
    "keys = []\n",
    "counts = []\n",
    "for key in user_to_X_test.keys():\n",
    "    cnt = 0\n",
    "    for item in user_to_X_test[key]:\n",
    "        X_test.append(item)\n",
    "        cnt+=1\n",
    "    counts.append(cnt)\n",
    "    keys.append(key)\n",
    "\n",
    "#scale test features...\n",
    "scalar = StandardScaler()\n",
    "X_test = scalar.fit_transform(X_test)\n",
    "\n",
    "#populate new_user_to_X_test with scaled test features\n",
    "cnt = 0\n",
    "for num, key in zip(counts, keys):\n",
    "    new_user_to_X_test[key] = []\n",
    "    for i in range(num):\n",
    "        new_user_to_X_test[key].append(X_test[cnt])\n",
    "        cnt+=1\n",
    "\n",
    "\n",
    "# user id to the average predicted rating for the randomly chosen movie\n",
    "user_to_avg_rating = dict()\n",
    "\n",
    "# populate user_to_avg_rating by averaging the predictions from all the feature inputs of the...\n",
    "# movies a user has watched that are not the randomly chosen movie itself\n",
    "for key in new_user_to_X_test.keys():\n",
    "    sum =0\n",
    "    cnt =0 \n",
    "    predicted = regr.predict(new_user_to_X_test[key])\n",
    "    for item in predicted:\n",
    "        sum+=item\n",
    "        cnt+=1\n",
    "    user_to_avg_rating[key] = float(sum/cnt)\n",
    "\n",
    "\n",
    "#outputs\n",
    "actuals_list = []\n",
    "preds_list = []\n",
    "for key in user_to_avg_rating.keys():\n",
    "    print(\"Pred: \"+str(user_to_avg_rating[key]) , \"Actual: \"+str(user_to_y_test[key]))\n",
    "    actuals_list.append(user_to_y_test[key])\n",
    "    preds_list.append(user_to_avg_rating[key])\n",
    "print(\"overall score:\", r2_score(actuals_list, preds_list))\n",
    "\n",
    "#test the time taken to train and predict\n",
    "end = time.time()\n",
    "print(\"Minutes:\", float((end - start)/60))\n",
    "\n",
    "\n",
    "#feature importance scores:\n",
    "#https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance\n",
    "\n",
    "#perhaps there is a way to visualize this of the model outputs below in a systematic way???\n",
    "\n",
    "# Tests:\n",
    "\n",
    "# full features:\n",
    "# with linear regression:\n",
    "# overall score: 0.2657455495660592\n",
    "\n",
    "# with mlp...:\n",
    "\n",
    "# first fours features:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2667063296881431\n",
    "\n",
    "#all features: \n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.26606490897808244\n",
    "\n",
    "#all features: \n",
    "# layers: (2,2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.260461734737799\n",
    "\n",
    "#all features: \n",
    "# layers: (4,4,4)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.22932745175064528\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2482634902547255\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (2,2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2616102684471122\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (3,3,3)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.25487207187202243\n",
    "\n",
    "#first two featurs:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: -0.00430015574935827\n",
    "\n",
    "#first two featurs:\n",
    "# layers: (2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.04468421358737418\n",
    "\n",
    "#3rd and 4th features:\n",
    "# layers: (2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2546480453878024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
