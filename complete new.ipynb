{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    length = len(array[8])\n",
    "    if(array[8][length-4:]==\"<NA>\"):\n",
    "        return False\n",
    "    length = len(array[9])\n",
    "    if(array[9][length-4:]==\"<NA>\"):\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#convert the dataframe into an array and build a dictionary\n",
    "user_to_data = dict()\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "\n",
    "#get all unique user ids\n",
    "list_of_user_ids = []\n",
    "last_id  = -1\n",
    "for item in complete_array:\n",
    "    if(item[0]!= last_id):\n",
    "        list_of_user_ids.append(item[0])\n",
    "        last_id = item[0]\n",
    "\n",
    "\n",
    "index  = 0\n",
    "#this has been tested with 5000, 10000, 20000, 100000\n",
    "nof_users = 20000\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, nof_users):\n",
    "    user_to_data[list_of_user_ids[i]] = []\n",
    "    for j in range(index, len(complete_array)):\n",
    "        if complete_array[j][0] == list_of_user_ids[i]:\n",
    "            #condition is checked for complete_array[j]\n",
    "            if(condition(complete_array[j])):\n",
    "                #this is where data is tranformed\n",
    "                transformed = provide_data(complete_array[j])\n",
    "                user_to_data[list_of_user_ids[i]].append(transformed)         \n",
    "        else:\n",
    "            #ignore if the number of ratings for a user is too small\n",
    "            if (len(user_to_data[list_of_user_ids[i]])<10):\n",
    "                del user_to_data[list_of_user_ids[i]]\n",
    "            index = j+1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for key in user_to_data.keys():\n",
    "        writer.writerows(user_to_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie id to list of its ratings by all users\n",
    "movie_to_ratings = dict()\n",
    "\n",
    "#user id to the ratings of movies by the user\n",
    "user_to_ratings = dict()\n",
    "\n",
    "#The list created by the constructed data csv is in order by user id\n",
    "#This code populates movie_to_ratings and user_to_ratings\n",
    "user_id = -1\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_ratings[row[0]] = [row]\n",
    "    else:\n",
    "        user_to_ratings[row[0]].append(row)\n",
    "\n",
    "    if(row[1] in movie_to_ratings.keys()):\n",
    "        movie_to_ratings[row[1]].append(row[2])\n",
    "    else:\n",
    "        movie_to_ratings[row[1]] = [row[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "#dictionary of user id to a list of strings of combined textual features for each movie rated by the user\n",
    "#the strings do not include ratings or movie id\n",
    "user_to_corpus_list = dict()\n",
    "\n",
    "for key in user_to_ratings.keys():\n",
    "    movie_strings = []\n",
    "    for movie_data in user_to_ratings[key]:\n",
    "        movie_string = \"\"\n",
    "        #avoid the first three data points (user id, movieid, and rating)\n",
    "        #use only the text data\n",
    "        for index in range (3,len(movie_data)):\n",
    "            if(index!= len(movie_data)-1):\n",
    "                movie_string+= movie_data[index]+\" \"\n",
    "            else:\n",
    "                movie_string+= movie_data[index]\n",
    "        cleaned = remove_stopwords(movie_string)\n",
    "        movie_strings.append(cleaned)\n",
    "    user_to_corpus_list[key] = movie_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:61: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_18660\\3442270765.py:64: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#get average rating for a single movie amoung all users who rated it\n",
    "def get_avg_movie_rating(movie_id):\n",
    "    ret =0 \n",
    "    cnt = 0\n",
    "    for item in movie_to_ratings[movie_id]:\n",
    "        ret+= float(item)\n",
    "        cnt+=1\n",
    "    return float(ret/cnt)\n",
    "\n",
    "\n",
    "#get all the movie ratings from a single user\n",
    "def get_user_ratings(user_id):\n",
    "    ret = []\n",
    "    for item in user_to_ratings[user_id]:\n",
    "        ret.append(float(item[2]))\n",
    "    return ret\n",
    "\n",
    "\n",
    "#user to model independent var X\n",
    "user_to_features = dict()\n",
    "#user to model dependent var y\n",
    "user_to_rand_rating = dict()\n",
    "\n",
    "#populate user_to_features and user_to_rand_rating\n",
    "for key in user_to_corpus_list.keys():\n",
    "\n",
    "    count_matrix = CountVectorizer().fit_transform(user_to_corpus_list[key]).toarray().tolist()\n",
    "    rand_index = random.randint(0, len(count_matrix)-1)\n",
    "    rand_test_item = count_matrix[rand_index]\n",
    "    del count_matrix[rand_index]\n",
    "\n",
    "    #find similarity by the count of each word between the random selected movie and the other movies rated by the user\n",
    "    cosine_sim = cosine_similarity(X = count_matrix ,Y = [rand_test_item])\n",
    "\n",
    "    ratings = copy.deepcopy(get_user_ratings(key))\n",
    "    similairities = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "    random_rating = ratings[rand_index]\n",
    "    user_to_rand_rating[key] = random_rating\n",
    "    del ratings[rand_index]\n",
    "\n",
    "\n",
    "    movie_rating_avg = get_avg_movie_rating(user_to_ratings[key][rand_index][1])\n",
    "\n",
    "    user_rating_avg =  float(np.sum(ratings)/(len(ratings)))\n",
    "    user_rating_skew = skew(ratings)\n",
    "    if(math.isnan(user_rating_skew)):\n",
    "        user_rating_skew = 0\n",
    "    user_rating_kurt = kurtosis(ratings)\n",
    "    if(math.isnan(user_rating_kurt)):\n",
    "        user_rating_kurt = 0\n",
    "    user_rating_var = statistics.variance(ratings)\n",
    "\n",
    "\n",
    "    sim_average = float(np.sum(similairities)/(len(similairities)))\n",
    "    sim_skew = skew(similairities) \n",
    "    if(math.isnan(sim_skew)):\n",
    "        sim_skew = 0\n",
    "    sim_kurt = kurtosis(similairities)\n",
    "    if(math.isnan(sim_kurt)):\n",
    "        sim_kurt = 0\n",
    "    sim_var = statistics.variance(similairities)\n",
    "\n",
    "\n",
    "    # there are many curve defining features used here that may be impotent and can be cut or kept in the next cell...\n",
    "    # there may stil be other distribution measures that improve the model...\n",
    "\n",
    "    # might try inputing some function of sim and rating rather than incluing them on their own\n",
    "    for sim, rating in zip(similairities, ratings):\n",
    "        if key not in user_to_features:\n",
    "            # user_to_features[key] = [[(rating - user_rating_avg)*sim, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var]]\n",
    "            user_to_features[key] = [[(rating - user_rating_avg)*sim]]\n",
    "        else:\n",
    "            # user_to_features[key].append([(rating - user_rating_avg)*sim, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var])\n",
    "            user_to_features[key].append([(rating - user_rating_avg)*sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.15128403e-05]\n",
      "13\n",
      "Pred: 3.5435533581863066 Actual: 3.0\n",
      "Pred: 3.5440199455876016 Actual: 1.0\n",
      "Pred: 3.543632079951758 Actual: 3.0\n",
      "Pred: 3.5438802160027456 Actual: 4.0\n",
      "Pred: 3.5438608286778175 Actual: 3.0\n",
      "Pred: 3.543865420565713 Actual: 4.0\n",
      "Pred: 3.5438717760914713 Actual: 1.5\n",
      "Pred: 3.543767890400353 Actual: 4.5\n",
      "Pred: 3.543865896065421 Actual: 4.0\n",
      "Pred: 3.543945273399721 Actual: 4.0\n",
      "Pred: 3.543540843921028 Actual: 3.0\n",
      "Pred: 3.5432479942607022 Actual: 1.0\n",
      "Pred: 3.544154633895148 Actual: 5.0\n",
      "Pred: 3.5435335465005138 Actual: 3.0\n",
      "Pred: 3.5440377419773172 Actual: 3.0\n",
      "Pred: 3.5435180340710515 Actual: 3.0\n",
      "Pred: 3.54419026327665 Actual: 4.0\n",
      "Pred: 3.5435657076802114 Actual: 5.0\n",
      "Pred: 3.544023764144912 Actual: 4.0\n",
      "Pred: 3.5436857059973046 Actual: 3.0\n",
      "Pred: 3.5438042190907906 Actual: 3.0\n",
      "Pred: 3.5436697159802835 Actual: 3.0\n",
      "Pred: 3.543294557329048 Actual: 4.5\n",
      "Pred: 3.5437551742999878 Actual: 2.5\n",
      "Pred: 3.544076585500153 Actual: 4.5\n",
      "Pred: 3.5437462003800153 Actual: 1.0\n",
      "Pred: 3.54371897101104 Actual: 4.0\n",
      "Pred: 3.543878936410382 Actual: 4.0\n",
      "Pred: 3.5440196742479912 Actual: 5.0\n",
      "Pred: 3.543655722796518 Actual: 4.5\n",
      "Pred: 3.5438555711565813 Actual: 1.0\n",
      "Pred: 3.5437204551881827 Actual: 4.5\n",
      "Pred: 3.543846080936932 Actual: 5.0\n",
      "Pred: 3.5434552365051726 Actual: 3.0\n",
      "Pred: 3.5438722021203524 Actual: 5.0\n",
      "Pred: 3.543420936020371 Actual: 3.0\n",
      "Pred: 3.5438094431763325 Actual: 0.5\n",
      "Pred: 3.5443352411694846 Actual: 4.0\n",
      "Pred: 3.5447161153331725 Actual: 5.0\n",
      "Pred: 3.543993983204895 Actual: 3.0\n",
      "Pred: 3.543947474606185 Actual: 4.0\n",
      "Pred: 3.543717895133089 Actual: 4.0\n",
      "Pred: 3.5437501969827108 Actual: 1.0\n",
      "Pred: 3.5448776974223706 Actual: 3.5\n",
      "Pred: 3.5433948133408535 Actual: 1.5\n",
      "Pred: 3.543957446944929 Actual: 4.5\n",
      "Pred: 3.543697943086555 Actual: 2.0\n",
      "Pred: 3.5441387099754795 Actual: 2.0\n",
      "Pred: 3.5435326819818385 Actual: 4.0\n",
      "Pred: 3.543427554003272 Actual: 4.0\n",
      "Pred: 3.544526898925104 Actual: 3.0\n",
      "Pred: 3.5437662832183254 Actual: 3.0\n",
      "Pred: 3.5441810410895522 Actual: 4.0\n",
      "Pred: 3.5435366736012037 Actual: 3.0\n",
      "Pred: 3.5436848871285798 Actual: 4.5\n",
      "Pred: 3.5439939387183244 Actual: 5.0\n",
      "Pred: 3.544202713415846 Actual: 5.0\n",
      "Pred: 3.5436061118377884 Actual: 4.0\n",
      "Pred: 3.543810626643308 Actual: 4.0\n",
      "Pred: 3.543771429402076 Actual: 4.5\n",
      "Pred: 3.543735835360344 Actual: 4.0\n",
      "Pred: 3.543872206602939 Actual: 3.0\n",
      "Pred: 3.5436178585119498 Actual: 3.0\n",
      "Pred: 3.5437077931615417 Actual: 1.0\n",
      "Pred: 3.5438600567713694 Actual: 1.0\n",
      "Pred: 3.543682894293141 Actual: 2.0\n",
      "Pred: 3.544104944085659 Actual: 5.0\n",
      "Pred: 3.5437656868893592 Actual: 3.0\n",
      "Pred: 3.544110684646613 Actual: 4.0\n",
      "Pred: 3.5437862364543413 Actual: 4.0\n",
      "Pred: 3.5440088442159454 Actual: 4.5\n",
      "Pred: 3.5436359243384903 Actual: 4.5\n",
      "Pred: 3.5437185242398366 Actual: 5.0\n",
      "Pred: 3.543930967009338 Actual: 2.0\n",
      "Pred: 3.5437294729340563 Actual: 4.0\n",
      "Pred: 3.54351819721918 Actual: 4.0\n",
      "Pred: 3.543769725001285 Actual: 4.5\n",
      "Pred: 3.5434341907586364 Actual: 3.0\n",
      "Pred: 3.5438088974059716 Actual: 3.0\n",
      "Pred: 3.5439130242057053 Actual: 5.0\n",
      "Pred: 3.5436094847550463 Actual: 2.0\n",
      "Pred: 3.543940933252388 Actual: 4.0\n",
      "Pred: 3.5438503711834732 Actual: 4.0\n",
      "Pred: 3.543830352021512 Actual: 4.5\n",
      "Pred: 3.544016734689672 Actual: 2.0\n",
      "Pred: 3.543814922905402 Actual: 3.0\n",
      "Pred: 3.5438287680574194 Actual: 3.0\n",
      "Pred: 3.5438574874423874 Actual: 2.5\n",
      "Pred: 3.5438897277154733 Actual: 4.0\n",
      "Pred: 3.5432548127107952 Actual: 4.0\n",
      "Pred: 3.5438493670725633 Actual: 3.0\n",
      "Pred: 3.5436261942500984 Actual: 4.0\n",
      "Pred: 3.5438788459661414 Actual: 4.0\n",
      "Pred: 3.5441863506293356 Actual: 3.0\n",
      "Pred: 3.543422077842534 Actual: 5.0\n",
      "Pred: 3.5438068285198843 Actual: 4.5\n",
      "Pred: 3.5435692895547044 Actual: 3.0\n",
      "Pred: 3.543252886786616 Actual: 4.5\n",
      "Pred: 3.5437594192653328 Actual: 5.0\n",
      "Pred: 3.543932073663553 Actual: 4.0\n",
      "Pred: 3.5436717610142274 Actual: 3.0\n",
      "Pred: 3.543839334289633 Actual: 3.0\n",
      "Pred: 3.54381374736866 Actual: 5.0\n",
      "Pred: 3.5439470000198554 Actual: 4.0\n",
      "Pred: 3.543669801966176 Actual: 5.0\n",
      "Pred: 3.5439259708986284 Actual: 5.0\n",
      "Pred: 3.5437166385841334 Actual: 4.0\n",
      "Pred: 3.5438149600537083 Actual: 1.0\n",
      "Pred: 3.544253276527113 Actual: 5.0\n",
      "Pred: 3.5440393899121356 Actual: 5.0\n",
      "Pred: 3.543762228605822 Actual: 4.0\n",
      "Pred: 3.5438452371260136 Actual: 4.0\n",
      "Pred: 3.5440243601752566 Actual: 5.0\n",
      "Pred: 3.543742252611807 Actual: 4.0\n",
      "Pred: 3.5433169088491843 Actual: 4.0\n",
      "Pred: 3.5438687145728314 Actual: 4.0\n",
      "Pred: 3.5437488045522008 Actual: 4.0\n",
      "Pred: 3.54346410078879 Actual: 5.0\n",
      "Pred: 3.54354983700078 Actual: 3.0\n",
      "Pred: 3.543734517001571 Actual: 3.5\n",
      "Pred: 3.5436492849573527 Actual: 1.0\n",
      "Pred: 3.543817390148149 Actual: 4.5\n",
      "Pred: 3.543782825454183 Actual: 3.0\n",
      "Pred: 3.543855035416786 Actual: 2.0\n",
      "Pred: 3.5439456789025487 Actual: 4.0\n",
      "Pred: 3.544228699617002 Actual: 4.0\n",
      "Pred: 3.543921102283568 Actual: 5.0\n",
      "Pred: 3.543994772306912 Actual: 4.0\n",
      "Pred: 3.5436876051208808 Actual: 2.0\n",
      "Pred: 3.5440000427879057 Actual: 3.0\n",
      "Pred: 3.5437024290634582 Actual: 4.0\n",
      "Pred: 3.543929986493679 Actual: 3.0\n",
      "Pred: 3.5442858836915407 Actual: 3.0\n",
      "Pred: 3.5436557826159185 Actual: 5.0\n",
      "Pred: 3.542583952771203 Actual: 4.5\n",
      "Pred: 3.5438553616317723 Actual: 5.0\n",
      "Pred: 3.5441950548028482 Actual: 3.0\n",
      "Pred: 3.5436835984621675 Actual: 3.0\n",
      "Pred: 3.5438256779983117 Actual: 3.5\n",
      "Pred: 3.5439264015313805 Actual: 3.5\n",
      "Pred: 3.5436272414469063 Actual: 3.5\n",
      "Pred: 3.543651355492261 Actual: 3.0\n",
      "Pred: 3.5436519968732285 Actual: 4.5\n",
      "Pred: 3.5437761933200065 Actual: 3.0\n",
      "Pred: 3.543551327215106 Actual: 1.0\n",
      "Pred: 3.5431977976085784 Actual: 5.0\n",
      "Pred: 3.543912270600299 Actual: 4.0\n",
      "Pred: 3.543867148042849 Actual: 2.0\n",
      "Pred: 3.5439881751426157 Actual: 4.5\n",
      "Pred: 3.5436186727762746 Actual: 5.0\n",
      "Pred: 3.5436394991029334 Actual: 4.5\n",
      "Pred: 3.5438083549724224 Actual: 5.0\n",
      "Pred: 3.5433706469059953 Actual: 3.0\n",
      "Pred: 3.5433424635044846 Actual: 4.0\n",
      "Pred: 3.5437453221084865 Actual: 4.0\n",
      "Pred: 3.543926701772169 Actual: 4.0\n",
      "Pred: 3.543765678590941 Actual: 3.0\n",
      "Pred: 3.544210227735069 Actual: 3.0\n",
      "Pred: 3.54400586865653 Actual: 3.0\n",
      "Pred: 3.5441370047187957 Actual: 3.0\n",
      "Pred: 3.5439403779126075 Actual: 4.0\n",
      "Pred: 3.543369457477095 Actual: 3.0\n",
      "Pred: 3.5440450455801304 Actual: 5.0\n",
      "Pred: 3.543730637545126 Actual: 3.0\n",
      "Pred: 3.544053935019989 Actual: 4.5\n",
      "Pred: 3.5436518320634582 Actual: 4.5\n",
      "Pred: 3.5434976644436276 Actual: 3.0\n",
      "Pred: 3.543853287571645 Actual: 4.5\n",
      "Pred: 3.5435805213687788 Actual: 4.0\n",
      "Pred: 3.5436968792263355 Actual: 4.0\n",
      "Pred: 3.5437894942583017 Actual: 2.5\n",
      "Pred: 3.5439403530450218 Actual: 5.0\n",
      "Pred: 3.544061442721599 Actual: 3.0\n",
      "Pred: 3.54360633847473 Actual: 1.0\n",
      "Pred: 3.5438151915104092 Actual: 4.5\n",
      "Pred: 3.5438253608950476 Actual: 4.0\n",
      "Pred: 3.5441028782071644 Actual: 3.0\n",
      "Pred: 3.543663152764926 Actual: 2.0\n",
      "Pred: 3.543842029712133 Actual: 5.0\n",
      "Pred: 3.5438263212241505 Actual: 2.0\n",
      "Pred: 3.543869289994185 Actual: 5.0\n",
      "Pred: 3.543805459960513 Actual: 2.5\n",
      "Pred: 3.544058536361202 Actual: 3.0\n",
      "Pred: 3.5438334168929218 Actual: 4.0\n",
      "Pred: 3.5442893213415823 Actual: 3.5\n",
      "Pred: 3.5440550498601384 Actual: 0.5\n",
      "Pred: 3.5437286595804203 Actual: 2.5\n",
      "Pred: 3.543995227464521 Actual: 2.0\n",
      "Pred: 3.543839232619276 Actual: 5.0\n",
      "Pred: 3.543714995385149 Actual: 1.0\n",
      "Pred: 3.5440722777322793 Actual: 5.0\n",
      "Pred: 3.5439893033592376 Actual: 4.0\n",
      "Pred: 3.5439640049654404 Actual: 3.5\n",
      "Pred: 3.5438003196788603 Actual: 4.0\n",
      "Pred: 3.5436886271460386 Actual: 4.5\n",
      "Pred: 3.543746642692686 Actual: 3.0\n",
      "Pred: 3.5437486310270803 Actual: 4.0\n",
      "Pred: 3.5438527752810347 Actual: 3.0\n",
      "Pred: 3.5437554000105895 Actual: 3.0\n",
      "Pred: 3.543839920313175 Actual: 4.5\n",
      "Pred: 3.5440285662480924 Actual: 4.0\n",
      "Pred: 3.5438959181234164 Actual: 4.0\n",
      "Pred: 3.543605537328945 Actual: 4.5\n",
      "Pred: 3.5437808513994407 Actual: 5.0\n",
      "Pred: 3.5431849036821603 Actual: 5.0\n",
      "Pred: 3.543829809006403 Actual: 5.0\n",
      "Pred: 3.543844962650486 Actual: 3.5\n",
      "Pred: 3.5439225913981547 Actual: 3.5\n",
      "Pred: 3.5437134609172913 Actual: 4.0\n",
      "Pred: 3.5438305579631777 Actual: 3.5\n",
      "Pred: 3.5438900597150624 Actual: 0.5\n",
      "Pred: 3.5440010763061576 Actual: 3.0\n",
      "Pred: 3.5439953670976894 Actual: 4.0\n",
      "Pred: 3.5439431345075314 Actual: 5.0\n",
      "Pred: 3.5436474065472687 Actual: 5.0\n",
      "Pred: 3.543839695371954 Actual: 3.0\n",
      "Pred: 3.5439758127481418 Actual: 2.0\n",
      "Pred: 3.54365580682397 Actual: 3.0\n",
      "Pred: 3.543752340534027 Actual: 3.0\n",
      "Pred: 3.543821641389098 Actual: 4.0\n",
      "Pred: 3.5434600763412782 Actual: 0.5\n",
      "Pred: 3.543946312969154 Actual: 4.0\n",
      "Pred: 3.5438481932705286 Actual: 4.0\n",
      "Pred: 3.543813634792958 Actual: 5.0\n",
      "Pred: 3.5438366611106624 Actual: 4.5\n",
      "Pred: 3.543934408191179 Actual: 5.0\n",
      "Pred: 3.5437037684182693 Actual: 4.0\n",
      "Pred: 3.543691217963507 Actual: 5.0\n",
      "Pred: 3.543592241691004 Actual: 4.5\n",
      "Pred: 3.5292618064617667 Actual: 2.0\n",
      "Pred: 3.543629374701989 Actual: 3.0\n",
      "Pred: 3.543683340808402 Actual: 3.0\n",
      "Pred: 3.544088555201781 Actual: 3.0\n",
      "Pred: 3.5438160024376515 Actual: 5.0\n",
      "Pred: 3.543794039913084 Actual: 3.5\n",
      "Pred: 3.5439187307865967 Actual: 5.0\n",
      "Pred: 3.543722520066277 Actual: 4.0\n",
      "Pred: 3.543822066358733 Actual: 4.0\n",
      "Pred: 3.5437906732900495 Actual: 3.0\n",
      "Pred: 3.543713254683119 Actual: 4.0\n",
      "Pred: 3.543726409315816 Actual: 4.0\n",
      "Pred: 3.543842008228632 Actual: 4.5\n",
      "Pred: 3.5435509852872578 Actual: 4.0\n",
      "Pred: 3.5435378619322364 Actual: 4.5\n",
      "Pred: 3.543753130347769 Actual: 5.0\n",
      "Pred: 3.544310115510991 Actual: 4.0\n",
      "Pred: 3.543805267161033 Actual: 3.0\n",
      "Pred: 3.543610272767327 Actual: 4.0\n",
      "Pred: 3.54374267599673 Actual: 3.0\n",
      "Pred: 3.543564661267312 Actual: 4.0\n",
      "Pred: 3.543830912203577 Actual: 3.5\n",
      "Pred: 3.543062169838912 Actual: 4.0\n",
      "Pred: 3.5438624102660397 Actual: 3.5\n",
      "Pred: 3.543922055048504 Actual: 5.0\n",
      "Pred: 3.5437753728754404 Actual: 2.5\n",
      "Pred: 3.5436147590787224 Actual: 3.0\n",
      "Pred: 3.5438021093911765 Actual: 4.5\n",
      "Pred: 3.543832919967861 Actual: 4.0\n",
      "Pred: 3.543684285737653 Actual: 2.0\n",
      "Pred: 3.543911953186484 Actual: 4.0\n",
      "Pred: 3.54352970448741 Actual: 2.0\n",
      "Pred: 3.543940448688043 Actual: 2.5\n",
      "Pred: 3.5438220667812366 Actual: 4.5\n",
      "Pred: 3.5435584924096295 Actual: 3.0\n",
      "Pred: 3.5438084018811566 Actual: 4.0\n",
      "Pred: 3.543601088714441 Actual: 4.0\n",
      "Pred: 3.543948055997575 Actual: 3.5\n",
      "Pred: 3.5442185260866186 Actual: 5.0\n",
      "Pred: 3.544149769274057 Actual: 5.0\n",
      "Pred: 3.543776151355247 Actual: 4.0\n",
      "Pred: 3.5438146768210577 Actual: 3.5\n",
      "Pred: 3.543928435410343 Actual: 5.0\n",
      "Pred: 3.543878521438494 Actual: 5.0\n",
      "Pred: 3.543766413163558 Actual: 3.0\n",
      "Pred: 3.5438282033916444 Actual: 4.0\n",
      "Pred: 3.543727405190682 Actual: 5.0\n",
      "Pred: 3.543785963988368 Actual: 4.0\n",
      "Pred: 3.5437705541338276 Actual: 5.0\n",
      "Pred: 3.543171169721332 Actual: 3.0\n",
      "Pred: 3.5439128177064836 Actual: 4.5\n",
      "Pred: 3.5438974249298094 Actual: 3.5\n",
      "Pred: 3.5440145046450957 Actual: 4.0\n",
      "Pred: 3.543437869972016 Actual: 5.0\n",
      "Pred: 3.543778475638967 Actual: 5.0\n",
      "Pred: 3.5437276065092647 Actual: 4.0\n",
      "Pred: 3.543749901950929 Actual: 4.0\n",
      "Pred: 3.5436101723641293 Actual: 3.5\n",
      "Pred: 3.5441676404814477 Actual: 3.5\n",
      "Pred: 3.543088401124924 Actual: 2.0\n",
      "Pred: 3.5436444153603865 Actual: 3.0\n",
      "Pred: 3.543691780026277 Actual: 4.5\n",
      "Pred: 3.5435280107767357 Actual: 1.5\n",
      "Pred: 3.5438205716668167 Actual: 4.0\n",
      "Pred: 3.543958132705229 Actual: 4.0\n",
      "Pred: 3.543888312156709 Actual: 4.0\n",
      "Pred: 3.5433160916151887 Actual: 2.0\n",
      "Pred: 3.5437109954218093 Actual: 3.0\n",
      "Pred: 3.543652579824848 Actual: 4.0\n",
      "Pred: 3.5436180664044152 Actual: 4.0\n",
      "Pred: 3.54408340809772 Actual: 4.0\n",
      "Pred: 3.543905920295369 Actual: 3.0\n",
      "Pred: 3.543608871242878 Actual: 3.0\n",
      "Pred: 3.54375711895424 Actual: 5.0\n",
      "Pred: 3.5436180143539717 Actual: 3.0\n",
      "Pred: 3.543766433402329 Actual: 4.0\n",
      "Pred: 3.543952238578904 Actual: 4.0\n",
      "Pred: 3.5436897887456897 Actual: 5.0\n",
      "Pred: 3.5438901265215743 Actual: 4.0\n",
      "Pred: 3.5440435507738295 Actual: 5.0\n",
      "Pred: 3.5438820403497995 Actual: 4.0\n",
      "Pred: 3.543854132815533 Actual: 4.0\n",
      "Pred: 3.544209220093264 Actual: 2.0\n",
      "Pred: 3.5439885010162566 Actual: 5.0\n",
      "Pred: 3.5441407588973846 Actual: 4.0\n",
      "Pred: 3.543374897496448 Actual: 5.0\n",
      "Pred: 3.5437752161166842 Actual: 3.0\n",
      "Pred: 3.543730056883271 Actual: 2.5\n",
      "Pred: 3.5438334038338177 Actual: 3.0\n",
      "Pred: 3.54390910213966 Actual: 3.0\n",
      "Pred: 3.5439130137186323 Actual: 2.0\n",
      "Pred: 3.5438593014695527 Actual: 4.5\n",
      "Pred: 3.5439744857555007 Actual: 4.0\n",
      "Pred: 3.5436584150796864 Actual: 4.0\n",
      "Pred: 3.5436789152865877 Actual: 4.0\n",
      "Pred: 3.5438762308906986 Actual: 4.0\n",
      "Pred: 3.5441143772845023 Actual: 3.0\n",
      "Pred: 3.5438997755390753 Actual: 3.5\n",
      "Pred: 3.5437575605874074 Actual: 5.0\n",
      "Pred: 3.543833705572328 Actual: 4.0\n",
      "Pred: 3.5438610584480736 Actual: 5.0\n",
      "Pred: 3.5437928977336415 Actual: 5.0\n",
      "Pred: 3.543489096739678 Actual: 3.0\n",
      "Pred: 3.543814702550726 Actual: 5.0\n",
      "Pred: 3.5438085329765334 Actual: 2.5\n",
      "Pred: 3.5436261420838 Actual: 4.0\n",
      "Pred: 3.5438196128240125 Actual: 4.0\n",
      "Pred: 3.5435497801272096 Actual: 4.0\n",
      "Pred: 3.5443011583905717 Actual: 5.0\n",
      "Pred: 3.5440069023240963 Actual: 4.0\n",
      "Pred: 3.5436549085797826 Actual: 4.0\n",
      "Pred: 3.5437763200980945 Actual: 3.0\n",
      "Pred: 3.54381007716663 Actual: 3.5\n",
      "Pred: 3.5439117340872968 Actual: 4.0\n",
      "Pred: 3.54380358171111 Actual: 5.0\n",
      "Pred: 3.5441688573271697 Actual: 5.0\n",
      "Pred: 3.543765412179069 Actual: 4.5\n",
      "Pred: 3.544034251688837 Actual: 5.0\n",
      "Pred: 3.544472891840134 Actual: 4.0\n",
      "Pred: 3.5439460955215263 Actual: 4.0\n",
      "Pred: 3.543943965709315 Actual: 3.0\n",
      "Pred: 3.5431797963260863 Actual: 1.0\n",
      "Pred: 3.5435498840931787 Actual: 3.5\n",
      "Pred: 3.5438792719384886 Actual: 4.0\n",
      "Pred: 3.5438639243646946 Actual: 3.0\n",
      "Pred: 3.5438064081170344 Actual: 3.5\n",
      "Pred: 3.543650422755404 Actual: 4.0\n",
      "Pred: 3.5437780321120833 Actual: 3.0\n",
      "Pred: 3.544107906294348 Actual: 3.0\n",
      "Pred: 3.544097770083088 Actual: 3.5\n",
      "Pred: 3.5438003625599603 Actual: 4.0\n",
      "Pred: 3.543724542691422 Actual: 2.0\n",
      "Pred: 3.543741566818712 Actual: 4.5\n",
      "Pred: 3.5437098537101264 Actual: 5.0\n",
      "Pred: 3.543548286661448 Actual: 5.0\n",
      "Pred: 3.543828965913716 Actual: 4.0\n",
      "Pred: 3.544052816876869 Actual: 3.5\n",
      "Pred: 3.262299954788648 Actual: 1.0\n",
      "Pred: 3.5441337783594826 Actual: 3.0\n",
      "Pred: 3.5438672411963714 Actual: 5.0\n",
      "Pred: 3.5437387584046807 Actual: 4.0\n",
      "Pred: 3.543783631046735 Actual: 4.5\n",
      "Pred: 3.5436796122223098 Actual: 4.0\n",
      "Pred: 3.5437672360824988 Actual: 3.5\n",
      "Pred: 3.5438395983543023 Actual: 5.0\n",
      "Pred: 3.5435702704082215 Actual: 4.0\n",
      "Pred: 3.5438775667769793 Actual: 3.5\n",
      "Pred: 3.5439031785465787 Actual: 3.0\n",
      "Pred: 3.54382209747095 Actual: 5.0\n",
      "Pred: 3.5438724870550704 Actual: 1.5\n",
      "Pred: 3.5442012392078306 Actual: 3.5\n",
      "Pred: 3.5437250470346835 Actual: 1.0\n",
      "Pred: 3.543648764147625 Actual: 2.0\n",
      "Pred: 3.543767843372143 Actual: 3.0\n",
      "Pred: 3.5438407943492014 Actual: 4.0\n",
      "Pred: 3.5435865961887947 Actual: 1.0\n",
      "Pred: 3.5438491743419767 Actual: 4.0\n",
      "Pred: 3.543838874004569 Actual: 3.0\n",
      "Pred: 3.543728725807573 Actual: 4.0\n",
      "Pred: 3.5435430325385684 Actual: 4.0\n",
      "Pred: 3.5438198220270873 Actual: 3.0\n",
      "Pred: 3.54395257024378 Actual: 5.0\n",
      "Pred: 3.5436885499505486 Actual: 3.0\n",
      "Pred: 3.5438346174921334 Actual: 3.0\n",
      "Pred: 3.54381736636976 Actual: 4.0\n",
      "Pred: 3.5444702432103297 Actual: 5.0\n",
      "Pred: 3.543664723317132 Actual: 4.0\n",
      "Pred: 3.5438627049349196 Actual: 3.5\n",
      "Pred: 3.5436962399791043 Actual: 3.0\n",
      "Pred: 3.543840834924394 Actual: 4.0\n",
      "Pred: 3.5437446353803144 Actual: 3.5\n",
      "Pred: 3.543952932654346 Actual: 5.0\n",
      "Pred: 3.5435205313403086 Actual: 5.0\n",
      "Pred: 3.5438535307798955 Actual: 3.0\n",
      "Pred: 3.5437918896593907 Actual: 3.0\n",
      "Pred: 3.5435750391304754 Actual: 2.0\n",
      "Pred: 3.549592861735084 Actual: 3.0\n",
      "Pred: 3.5437209360142385 Actual: 3.0\n",
      "Pred: 3.544016416329746 Actual: 4.0\n",
      "Pred: 3.543870306567078 Actual: 5.0\n",
      "Pred: 3.5437014098308057 Actual: 4.0\n",
      "Pred: 3.5436208747039357 Actual: 4.5\n",
      "Pred: 3.5442162791653544 Actual: 4.0\n",
      "Pred: 3.543665984494693 Actual: 5.0\n",
      "Pred: 3.5439587357229003 Actual: 2.0\n",
      "Pred: 3.5438295889531615 Actual: 3.0\n",
      "Pred: 3.54390952777498 Actual: 4.0\n",
      "Pred: 3.544093419759535 Actual: 3.0\n",
      "Pred: 3.543720259755191 Actual: 4.5\n",
      "Pred: 3.5438470951037306 Actual: 3.0\n",
      "Pred: 3.5439520360933203 Actual: 3.5\n",
      "Pred: 3.544032914763044 Actual: 4.0\n",
      "Pred: 3.543763313073558 Actual: 3.5\n",
      "Pred: 3.5438285161532574 Actual: 4.0\n",
      "Pred: 3.5439398386854273 Actual: 4.0\n",
      "Pred: 3.5436534805765625 Actual: 4.0\n",
      "Pred: 3.5438161603915765 Actual: 3.0\n",
      "Pred: 3.5438142238240453 Actual: 3.5\n",
      "Pred: 3.5434782805453837 Actual: 4.5\n",
      "Pred: 3.543732838361668 Actual: 4.0\n",
      "Pred: 3.5439118203328146 Actual: 3.5\n",
      "Pred: 3.5436093744581467 Actual: 5.0\n",
      "Pred: 3.543775883809025 Actual: 3.0\n",
      "Pred: 3.5437978567459916 Actual: 4.0\n",
      "Pred: 3.543968085478859 Actual: 3.0\n",
      "Pred: 3.543682814493741 Actual: 3.0\n",
      "Pred: 3.5440707920017336 Actual: 4.0\n",
      "Pred: 3.5439340275395987 Actual: 4.5\n",
      "Pred: 3.543771351837731 Actual: 3.5\n",
      "Pred: 3.5437044998773573 Actual: 5.0\n",
      "Pred: 3.5435906922624896 Actual: 3.5\n",
      "Pred: 3.5439085411689204 Actual: 4.0\n",
      "Pred: 3.5439275085475663 Actual: 5.0\n",
      "Pred: 3.5438788567448496 Actual: 3.0\n",
      "Pred: 3.5438785060583027 Actual: 3.0\n",
      "Pred: 3.5436074576205594 Actual: 4.0\n",
      "Pred: 3.54387963198923 Actual: 3.0\n",
      "Pred: 3.5433641029762644 Actual: 3.5\n",
      "Pred: 3.5437123843878675 Actual: 4.0\n",
      "Pred: 3.5437530810836537 Actual: 4.0\n",
      "Pred: 3.5440368965737954 Actual: 1.0\n",
      "Pred: 3.5441593503912165 Actual: 4.0\n",
      "Pred: 3.5438514521295876 Actual: 5.0\n",
      "Pred: 3.543794205333898 Actual: 3.0\n",
      "Pred: 3.543778351042814 Actual: 4.0\n",
      "Pred: 3.5440183834670282 Actual: 4.0\n",
      "Pred: 3.5437030868463917 Actual: 5.0\n",
      "Pred: 3.544460885954789 Actual: 3.0\n",
      "Pred: 3.5438460766771405 Actual: 4.0\n",
      "Pred: 3.5439397642138384 Actual: 3.0\n",
      "Pred: 3.543918415101221 Actual: 4.0\n",
      "Pred: 3.5434585430298475 Actual: 2.5\n",
      "Pred: 3.543565831796749 Actual: 3.0\n",
      "Pred: 3.54382044843045 Actual: 4.0\n",
      "Pred: 3.543856518397371 Actual: 3.0\n",
      "Pred: 3.5435715248002873 Actual: 3.5\n",
      "Pred: 3.544008949306891 Actual: 4.0\n",
      "Pred: 3.543792305048435 Actual: 3.0\n",
      "Pred: 3.54387222146344 Actual: 3.0\n",
      "Pred: 3.5438441406517214 Actual: 5.0\n",
      "Pred: 3.5440848273977936 Actual: 4.0\n",
      "Pred: 3.544071236768066 Actual: 1.0\n",
      "Pred: 3.5436600934225475 Actual: 3.0\n",
      "Pred: 3.5436587057902997 Actual: 4.5\n",
      "Pred: 3.5438217210935883 Actual: 3.0\n",
      "Pred: 3.543598540880983 Actual: 4.0\n",
      "Pred: 3.5438095477989013 Actual: 4.0\n",
      "Pred: 3.5438067106151747 Actual: 3.5\n",
      "Pred: 3.5437662459386465 Actual: 3.0\n",
      "Pred: 3.5434956627856464 Actual: 5.0\n",
      "Pred: 3.543922371660086 Actual: 4.5\n",
      "Pred: 3.5438975323504938 Actual: 3.0\n",
      "Pred: 3.5437975668936326 Actual: 3.5\n",
      "Pred: 3.5439397959892385 Actual: 3.0\n",
      "Pred: 3.5437884109107145 Actual: 3.0\n",
      "Pred: 3.5438822117178783 Actual: 1.0\n",
      "Pred: 3.5437866377244327 Actual: 5.0\n",
      "Pred: 3.5442356573572824 Actual: 4.5\n",
      "Pred: 3.54327896302418 Actual: 4.5\n",
      "Pred: 3.5438066369513375 Actual: 3.0\n",
      "Pred: 3.543851543266622 Actual: 3.0\n",
      "Pred: 3.543575408440566 Actual: 5.0\n",
      "Pred: 3.5437739140151456 Actual: 2.0\n",
      "Pred: 3.543663339856359 Actual: 5.0\n",
      "Pred: 3.544059510729933 Actual: 3.0\n",
      "Pred: 3.543691141122237 Actual: 5.0\n",
      "Pred: 3.543782759196804 Actual: 3.0\n",
      "Pred: 3.543982384940313 Actual: 5.0\n",
      "Pred: 3.5437207328868365 Actual: 3.0\n",
      "Pred: 3.5437580097670125 Actual: 5.0\n",
      "Pred: 3.543799563784998 Actual: 3.0\n",
      "Pred: 3.544269041561337 Actual: 4.0\n",
      "Pred: 3.5437221167850153 Actual: 4.0\n",
      "Pred: 3.5435522299699884 Actual: 5.0\n",
      "Pred: 3.543775481453656 Actual: 3.0\n",
      "Pred: 3.543588165445732 Actual: 2.0\n",
      "Pred: 3.5442908527714487 Actual: 3.0\n",
      "Pred: 3.544021153631545 Actual: 2.0\n",
      "Pred: 3.5438965418081727 Actual: 2.0\n",
      "Pred: 3.543827200839074 Actual: 4.5\n",
      "Pred: 3.5438238096182477 Actual: 4.0\n",
      "Pred: 3.543592070742284 Actual: 4.5\n",
      "Pred: 3.5434923541477814 Actual: 4.0\n",
      "Pred: 3.543791608824554 Actual: 4.0\n",
      "Pred: 3.5436702147036527 Actual: 5.0\n",
      "Pred: 3.543801313469776 Actual: 3.5\n",
      "Pred: 3.5437904637723543 Actual: 5.0\n",
      "Pred: 3.543891660467362 Actual: 2.0\n",
      "Pred: 3.543984327897182 Actual: 5.0\n",
      "Pred: 3.543336540192388 Actual: 2.0\n",
      "Pred: 3.5439953221779823 Actual: 3.0\n",
      "Pred: 3.5436105825591175 Actual: 4.0\n",
      "Pred: 3.5438253906355746 Actual: 4.0\n",
      "Pred: 3.5438721331844634 Actual: 2.0\n",
      "Pred: 3.5438928870520665 Actual: 3.0\n",
      "Pred: 3.5439439175870713 Actual: 5.0\n",
      "Pred: 3.544138128708285 Actual: 4.0\n",
      "Pred: 3.543812596474714 Actual: 4.0\n",
      "Pred: 3.543857310117264 Actual: 4.5\n",
      "Pred: 3.5438222075555443 Actual: 4.0\n",
      "Pred: 3.5435473893891367 Actual: 2.0\n",
      "Pred: 3.5445607325989186 Actual: 4.0\n",
      "Pred: 3.5439380675300094 Actual: 5.0\n",
      "Pred: 3.543884740293164 Actual: 4.0\n",
      "Pred: 3.5436149512411865 Actual: 3.0\n",
      "Pred: 3.543955299380731 Actual: 3.0\n",
      "Pred: 3.5439466803131596 Actual: 3.0\n",
      "Pred: 3.5438088119066364 Actual: 2.0\n",
      "Pred: 3.5438656093766037 Actual: 4.0\n",
      "Pred: 3.5438088001831445 Actual: 3.5\n",
      "Pred: 3.5437928705962904 Actual: 5.0\n",
      "Pred: 3.5443227510085387 Actual: 3.0\n",
      "Pred: 3.5440176836838257 Actual: 2.0\n",
      "Pred: 3.544129793725114 Actual: 3.0\n",
      "Pred: 3.5436658131353935 Actual: 3.0\n",
      "Pred: 3.543785709359039 Actual: 4.0\n",
      "Pred: 3.5436749909023364 Actual: 3.0\n",
      "Pred: 3.5437748267919473 Actual: 2.0\n",
      "Pred: 3.543795898707852 Actual: 5.0\n",
      "Pred: 3.5441196752805793 Actual: 3.0\n",
      "Pred: 3.543937731459011 Actual: 3.0\n",
      "Pred: 3.5439785835895092 Actual: 4.0\n",
      "Pred: 3.543797766988468 Actual: 3.0\n",
      "Pred: 3.5438796630367992 Actual: 3.0\n",
      "Pred: 3.5440498969607845 Actual: 1.5\n",
      "Pred: 3.5437917871802456 Actual: 3.5\n",
      "Pred: 3.5436540414166875 Actual: 1.0\n",
      "Pred: 3.543595388468403 Actual: 2.0\n",
      "Pred: 3.543651799482671 Actual: 5.0\n",
      "Pred: 3.5437569021674156 Actual: 4.0\n",
      "Pred: 3.5441113112428178 Actual: 3.0\n",
      "Pred: 3.543618461026314 Actual: 2.0\n",
      "Pred: 3.5437361105861056 Actual: 2.5\n",
      "Pred: 3.5437855508553864 Actual: 5.0\n",
      "Pred: 3.543616140796864 Actual: 2.5\n",
      "Pred: 3.5438251323221714 Actual: 3.5\n",
      "Pred: 3.5431088973214298 Actual: 5.0\n",
      "Pred: 3.5439180654520306 Actual: 5.0\n",
      "Pred: 3.5436488271135844 Actual: 3.0\n",
      "Pred: 3.5438401942036344 Actual: 4.5\n",
      "Pred: 3.543696147505856 Actual: 4.0\n",
      "Pred: 3.543906281717645 Actual: 3.0\n",
      "Pred: 3.544733970868806 Actual: 1.0\n",
      "Pred: 3.543669711119276 Actual: 0.5\n",
      "Pred: 3.5440730731193044 Actual: 3.5\n",
      "Pred: 3.543858024321631 Actual: 2.0\n",
      "Pred: 3.5435363265300555 Actual: 5.0\n",
      "Pred: 3.543930023325882 Actual: 2.0\n",
      "Pred: 3.5445303756100985 Actual: 3.5\n",
      "Pred: 3.5437120574277894 Actual: 5.0\n",
      "Pred: 3.5438185514464564 Actual: 1.0\n",
      "Pred: 3.5436016542472037 Actual: 4.0\n",
      "Pred: 3.5435605981863483 Actual: 4.0\n",
      "Pred: 3.543976103769144 Actual: 1.0\n",
      "Pred: 3.5436292534547107 Actual: 4.0\n",
      "Pred: 3.5436716040332725 Actual: 4.0\n",
      "Pred: 3.544197154768922 Actual: 2.5\n",
      "Pred: 3.5440139373538133 Actual: 4.0\n",
      "Pred: 3.543512887161345 Actual: 4.5\n",
      "Pred: 3.5437938296600686 Actual: 2.0\n",
      "Pred: 3.543842791467786 Actual: 3.5\n",
      "Pred: 3.543686518001086 Actual: 5.0\n",
      "Pred: 3.5439299352461764 Actual: 3.0\n",
      "Pred: 3.5439363817808607 Actual: 3.0\n",
      "Pred: 3.5437494487442973 Actual: 3.0\n",
      "Pred: 3.54422723187096 Actual: 4.5\n",
      "Pred: 3.5436412373001076 Actual: 3.0\n",
      "Pred: 3.5438386385598846 Actual: 5.0\n",
      "Pred: 3.5459500807979887 Actual: 3.0\n",
      "Pred: 3.5439492320981425 Actual: 3.0\n",
      "Pred: 3.5439849877865974 Actual: 3.0\n",
      "Pred: 3.5435513675410184 Actual: 3.0\n",
      "Pred: 3.543386194367589 Actual: 5.0\n",
      "Pred: 3.543909166245023 Actual: 3.0\n",
      "Pred: 3.5439524389364734 Actual: 3.0\n",
      "Pred: 3.543844485986173 Actual: 3.0\n",
      "Pred: 3.543854178151792 Actual: 3.0\n",
      "Pred: 3.5443073009350874 Actual: 4.5\n",
      "Pred: 3.5438181628093592 Actual: 3.0\n",
      "Pred: 3.5442266040905865 Actual: 3.0\n",
      "Pred: 3.543435355709463 Actual: 4.0\n",
      "Pred: 3.5437959056841715 Actual: 3.0\n",
      "Pred: 3.5438933288824166 Actual: 4.5\n",
      "Pred: 3.543653859174238 Actual: 4.0\n",
      "Pred: 3.543811038839328 Actual: 3.0\n",
      "Pred: 3.5438365781750245 Actual: 3.0\n",
      "Pred: 3.543227676544322 Actual: 3.5\n",
      "Pred: 3.5441816805243302 Actual: 2.0\n",
      "Pred: 3.54370438757981 Actual: 5.0\n",
      "Pred: 3.5437976386755077 Actual: 4.0\n",
      "Pred: 3.543838909332855 Actual: 3.0\n",
      "Pred: 3.543947115155308 Actual: 3.0\n",
      "Pred: 3.5439360982916526 Actual: 5.0\n",
      "Pred: 3.5434885474281903 Actual: 4.0\n",
      "Pred: 3.5444930326961073 Actual: 3.0\n",
      "Pred: 3.543821598555302 Actual: 1.0\n",
      "Pred: 3.543867427372308 Actual: 3.0\n",
      "Pred: 3.543758229104459 Actual: 4.5\n",
      "Pred: 3.543612035450927 Actual: 5.0\n",
      "Pred: 3.5438803521768825 Actual: 3.5\n",
      "Pred: 3.5437241101673567 Actual: 4.5\n",
      "Pred: 3.5438963680563025 Actual: 1.0\n",
      "Pred: 3.543569223213015 Actual: 1.0\n",
      "Pred: 3.544277997046301 Actual: 4.5\n",
      "Pred: 3.543885748946037 Actual: 1.5\n",
      "Pred: 3.543948198917462 Actual: 5.0\n",
      "Pred: 3.543771328290887 Actual: 3.5\n",
      "Pred: 3.5438516855241753 Actual: 3.0\n",
      "Pred: 3.5448700008505254 Actual: 4.0\n",
      "Pred: 3.5435268576356673 Actual: 2.0\n",
      "Pred: 3.5437436813902083 Actual: 2.5\n",
      "Pred: 3.543812989987526 Actual: 3.0\n",
      "Pred: 3.5435989126499945 Actual: 5.0\n",
      "Pred: 3.5437570471274595 Actual: 3.0\n",
      "Pred: 3.543779245530813 Actual: 4.0\n",
      "Pred: 3.543907162347374 Actual: 5.0\n",
      "Pred: 3.5441048367351073 Actual: 5.0\n",
      "Pred: 3.5439145523666205 Actual: 5.0\n",
      "Pred: 3.543922678632152 Actual: 4.5\n",
      "Pred: 3.5439510157058844 Actual: 4.0\n",
      "Pred: 3.5437721725324187 Actual: 1.5\n",
      "Pred: 3.544005805536364 Actual: 3.0\n",
      "Pred: 3.543956372673856 Actual: 3.5\n",
      "Pred: 3.5438031744619956 Actual: 3.0\n",
      "Pred: 3.5439455195774547 Actual: 4.0\n",
      "Pred: 3.5436685796085725 Actual: 4.0\n",
      "Pred: 3.5438715784416632 Actual: 4.0\n",
      "Pred: 3.5438487708335074 Actual: 4.0\n",
      "Pred: 3.5436478848914126 Actual: 5.0\n",
      "Pred: 3.54403918062316 Actual: 4.0\n",
      "Pred: 3.543935867906568 Actual: 5.0\n",
      "Pred: 3.543799224552997 Actual: 3.0\n",
      "Pred: 3.544364071843052 Actual: 3.5\n",
      "Pred: 3.5437794065013324 Actual: 3.0\n",
      "Pred: 3.5439012154575855 Actual: 3.0\n",
      "Pred: 3.5436078103614905 Actual: 5.0\n",
      "Pred: 3.5434244154984835 Actual: 4.0\n",
      "Pred: 3.5440049392721007 Actual: 5.0\n",
      "Pred: 3.543693068665983 Actual: 3.0\n",
      "Pred: 3.54368174850547 Actual: 5.0\n",
      "Pred: 3.5443328955675235 Actual: 5.0\n",
      "Pred: 3.543768242337378 Actual: 4.0\n",
      "Pred: 3.5440967728798807 Actual: 4.0\n",
      "Pred: 3.5435561474533386 Actual: 4.0\n",
      "Pred: 3.543833252938378 Actual: 5.0\n",
      "Pred: 3.5439380544391272 Actual: 4.0\n",
      "Pred: 3.5438590095361495 Actual: 4.0\n",
      "Pred: 3.5432958453441814 Actual: 4.5\n",
      "Pred: 3.5433931443786437 Actual: 1.0\n",
      "Pred: 3.5437593906505342 Actual: 2.0\n",
      "Pred: 3.5437658415674704 Actual: 2.5\n",
      "Pred: 3.5438750242075727 Actual: 5.0\n",
      "Pred: 3.543805814900548 Actual: 4.0\n",
      "Pred: 3.543779240260805 Actual: 5.0\n",
      "Pred: 3.543705197291865 Actual: 2.0\n",
      "Pred: 3.5435064243326324 Actual: 3.0\n",
      "Pred: 3.5445317777655805 Actual: 2.0\n",
      "Pred: 3.543829116064469 Actual: 3.0\n",
      "Pred: 3.5442165823230978 Actual: 4.0\n",
      "Pred: 3.543695863474874 Actual: 4.0\n",
      "Pred: 3.543982806965237 Actual: 4.0\n",
      "Pred: 3.543919498855128 Actual: 4.0\n",
      "Pred: 3.5441674565456056 Actual: 1.5\n",
      "Pred: 3.543960858264431 Actual: 3.0\n",
      "Pred: 3.5439751612426162 Actual: 3.0\n",
      "Pred: 3.5436449587723433 Actual: 3.0\n",
      "Pred: 3.5435545578006287 Actual: 3.0\n",
      "Pred: 3.5438446098085303 Actual: 3.0\n",
      "Pred: 3.5438494036812536 Actual: 4.5\n",
      "Pred: 3.5438013548566105 Actual: 5.0\n",
      "Pred: 3.543797743855041 Actual: 3.0\n",
      "Pred: 3.5438491850904916 Actual: 5.0\n",
      "Pred: 3.54375853362544 Actual: 3.0\n",
      "Pred: 3.543877622925138 Actual: 4.0\n",
      "Pred: 3.5442256737279174 Actual: 2.0\n",
      "Pred: 3.543984215159744 Actual: 4.0\n",
      "Pred: 3.5436842594090407 Actual: 3.0\n",
      "Pred: 3.543796534357484 Actual: 4.0\n",
      "Pred: 3.5438775413837362 Actual: 4.5\n",
      "Pred: 3.5439108394321788 Actual: 5.0\n",
      "Pred: 3.5439970587992957 Actual: 0.5\n",
      "Pred: 3.5437763222420875 Actual: 3.0\n",
      "Pred: 3.543900872561668 Actual: 4.5\n",
      "Pred: 3.5434869480307203 Actual: 2.0\n",
      "Pred: 3.5439831004823614 Actual: 5.0\n",
      "Pred: 3.543773997081846 Actual: 3.0\n",
      "Pred: 3.5436649986225324 Actual: 4.0\n",
      "Pred: 3.543626677436141 Actual: 4.0\n",
      "Pred: 3.543248671713263 Actual: 1.5\n",
      "Pred: 3.5436426514371036 Actual: 4.5\n",
      "Pred: 3.544074400007814 Actual: 3.5\n",
      "Pred: 3.544415705309243 Actual: 1.0\n",
      "Pred: 3.5437174301490977 Actual: 3.0\n",
      "Pred: 3.5437505453051195 Actual: 4.0\n",
      "Pred: 3.5437726641832206 Actual: 4.0\n",
      "Pred: 3.543641444339689 Actual: 4.0\n",
      "Pred: 3.544125524318059 Actual: 5.0\n",
      "Pred: 3.5446266102878057 Actual: 4.0\n",
      "Pred: 3.5435978302194227 Actual: 5.0\n",
      "Pred: 3.5438643531013603 Actual: 5.0\n",
      "Pred: 3.543872563707209 Actual: 3.5\n",
      "Pred: 3.543813591122864 Actual: 3.0\n",
      "Pred: 3.543836074335895 Actual: 5.0\n",
      "Pred: 3.5438937391571574 Actual: 3.0\n",
      "Pred: 3.5435262507185845 Actual: 4.0\n",
      "Pred: 3.543872091626997 Actual: 3.0\n",
      "Pred: 3.54409969763558 Actual: 3.0\n",
      "Pred: 3.5438362501163807 Actual: 5.0\n",
      "Pred: 3.543830072700435 Actual: 4.5\n",
      "Pred: 3.543686670016389 Actual: 3.0\n",
      "Pred: 3.543974309709663 Actual: 4.0\n",
      "Pred: 3.5436733299222123 Actual: 5.0\n",
      "Pred: 3.5440094965325586 Actual: 4.5\n",
      "Pred: 3.543753885101197 Actual: 5.0\n",
      "Pred: 3.5437486164919583 Actual: 3.0\n",
      "Pred: 3.543978665361432 Actual: 3.0\n",
      "Pred: 3.5439848719630267 Actual: 3.0\n",
      "Pred: 3.5438261569179845 Actual: 3.5\n",
      "Pred: 3.543913941606152 Actual: 4.0\n",
      "Pred: 3.543625604116524 Actual: 4.5\n",
      "Pred: 3.5440318942813605 Actual: 4.0\n",
      "Pred: 3.543844315881564 Actual: 4.5\n",
      "Pred: 3.543939397331322 Actual: 3.0\n",
      "Pred: 3.544155971225984 Actual: 4.0\n",
      "Pred: 3.5436347963012156 Actual: 4.0\n",
      "Pred: 3.5434081642297444 Actual: 4.0\n",
      "Pred: 3.543882991002973 Actual: 3.0\n",
      "Pred: 3.543851384850049 Actual: 4.0\n",
      "Pred: 3.543682050721048 Actual: 5.0\n",
      "Pred: 3.5439222484928106 Actual: 4.0\n",
      "Pred: 3.544273224781573 Actual: 4.0\n",
      "Pred: 3.5439306005396523 Actual: 3.0\n",
      "Pred: 3.5439433508463623 Actual: 3.0\n",
      "Pred: 3.543985595555501 Actual: 3.0\n",
      "Pred: 3.5436648779196327 Actual: 4.0\n",
      "Pred: 3.543820552874064 Actual: 2.5\n",
      "Pred: 3.543800257282515 Actual: 3.0\n",
      "Pred: 3.543923371011158 Actual: 4.0\n",
      "Pred: 3.5434292735941977 Actual: 2.5\n",
      "Pred: 3.54364123408076 Actual: 4.5\n",
      "Pred: 3.54397420187351 Actual: 1.5\n",
      "Pred: 3.543700921039739 Actual: 4.0\n",
      "Pred: 3.5437891491493305 Actual: 4.0\n",
      "Pred: 3.5438767880555453 Actual: 4.0\n",
      "Pred: 3.5442121867616994 Actual: 3.0\n",
      "Pred: 3.5437144332924273 Actual: 3.0\n",
      "Pred: 3.544082050960774 Actual: 5.0\n",
      "Pred: 3.54355670735507 Actual: 5.0\n",
      "Pred: 3.5437316197522515 Actual: 4.0\n",
      "Pred: 3.544013753869106 Actual: 5.0\n",
      "Pred: 3.5437150934039816 Actual: 4.5\n",
      "Pred: 3.5436295939391416 Actual: 3.0\n",
      "Pred: 3.543881367349992 Actual: 3.0\n",
      "Pred: 3.5436730448303995 Actual: 4.0\n",
      "Pred: 3.5437178250080184 Actual: 3.0\n",
      "Pred: 3.543921015331284 Actual: 4.0\n",
      "Pred: 3.5435268762781216 Actual: 3.5\n",
      "Pred: 3.5436707184045892 Actual: 5.0\n",
      "Pred: 3.543856586091041 Actual: 5.0\n",
      "Pred: 3.5438797003270204 Actual: 3.0\n",
      "Pred: 3.543673218547014 Actual: 1.5\n",
      "Pred: 3.54374911087671 Actual: 4.0\n",
      "Pred: 3.544235190732957 Actual: 2.0\n",
      "Pred: 3.5436598557058527 Actual: 5.0\n",
      "Pred: 3.5442370467260775 Actual: 4.5\n",
      "Pred: 3.543962062464759 Actual: 3.0\n",
      "Pred: 3.5437244488672333 Actual: 4.0\n",
      "Pred: 3.543854474911045 Actual: 4.0\n",
      "Pred: 3.543834152887696 Actual: 3.0\n",
      "Pred: 3.5438725327156084 Actual: 3.0\n",
      "Pred: 3.5440453660403706 Actual: 5.0\n",
      "Pred: 3.543222942117297 Actual: 5.0\n",
      "Pred: 3.54377629831141 Actual: 3.5\n",
      "Pred: 3.5435979206886987 Actual: 5.0\n",
      "Pred: 3.5436760320056875 Actual: 5.0\n",
      "Pred: 3.54388762757641 Actual: 4.0\n",
      "Pred: 3.543887309412725 Actual: 3.0\n",
      "Pred: 3.54378184282599 Actual: 2.0\n",
      "Pred: 3.5436923762635244 Actual: 5.0\n",
      "Pred: 3.5436318757802274 Actual: 3.5\n",
      "Pred: 3.5436076277492976 Actual: 3.0\n",
      "Pred: 3.543728256557536 Actual: 3.5\n",
      "Pred: 3.5437171438750337 Actual: 3.5\n",
      "Pred: 3.543725838182817 Actual: 4.0\n",
      "Pred: 3.543832455436828 Actual: 4.0\n",
      "Pred: 3.5436464198267914 Actual: 2.0\n",
      "Pred: 3.543693863678805 Actual: 3.0\n",
      "Pred: 3.5443360063039817 Actual: 3.0\n",
      "Pred: 3.543884834276392 Actual: 5.0\n",
      "Pred: 3.54380375122399 Actual: 5.0\n",
      "Pred: 3.543793600311025 Actual: 4.0\n",
      "Pred: 3.5439472452152105 Actual: 3.0\n",
      "Pred: 3.5437182020371543 Actual: 4.0\n",
      "Pred: 3.543672392475694 Actual: 3.0\n",
      "Pred: 3.5437883543683126 Actual: 3.0\n",
      "Pred: 3.5438466533183193 Actual: 4.5\n",
      "Pred: 3.544074696157673 Actual: 2.5\n",
      "Pred: 3.543797848279254 Actual: 4.0\n",
      "Pred: 3.544116530498894 Actual: 3.0\n",
      "Pred: 3.5438669998387904 Actual: 4.0\n",
      "Pred: 3.5440479336266577 Actual: 4.0\n",
      "Pred: 3.543935213272344 Actual: 5.0\n",
      "Pred: 3.543696593220911 Actual: 5.0\n",
      "Pred: 3.5437454257062164 Actual: 3.0\n",
      "Pred: 3.543593366234054 Actual: 5.0\n",
      "Pred: 3.543973115228029 Actual: 3.0\n",
      "Pred: 3.5437780577031357 Actual: 3.0\n",
      "Pred: 3.5443275122274955 Actual: 5.0\n",
      "Pred: 3.5434950488284986 Actual: 2.0\n",
      "Pred: 3.5438803653437936 Actual: 4.0\n",
      "Pred: 3.543763503039845 Actual: 1.5\n",
      "Pred: 3.543493485915772 Actual: 3.5\n",
      "Pred: 3.543725327379228 Actual: 4.0\n",
      "Pred: 3.543856485126275 Actual: 3.0\n",
      "Pred: 3.544106087972086 Actual: 3.0\n",
      "Pred: 3.543554654825571 Actual: 3.0\n",
      "Pred: 3.543810402845123 Actual: 4.0\n",
      "Pred: 3.543823487640846 Actual: 3.5\n",
      "Pred: 3.5438711829936813 Actual: 3.0\n",
      "Pred: 3.5436819971222007 Actual: 2.0\n",
      "Pred: 3.5440437424291673 Actual: 4.0\n",
      "Pred: 3.5437797211088933 Actual: 3.5\n",
      "Pred: 3.5439923326108307 Actual: 3.0\n",
      "Pred: 3.543670864154593 Actual: 4.0\n",
      "Pred: 3.5439382552538774 Actual: 3.5\n",
      "Pred: 3.5437150086553575 Actual: 3.0\n",
      "Pred: 3.5437812114036333 Actual: 5.0\n",
      "Pred: 3.5433889651360246 Actual: 3.0\n",
      "Pred: 3.543799232957731 Actual: 3.5\n",
      "Pred: 3.5439372567395027 Actual: 5.0\n",
      "Pred: 3.5441811162970436 Actual: 0.5\n",
      "Pred: 3.543747042418846 Actual: 4.0\n",
      "Pred: 3.5436053284496856 Actual: 5.0\n",
      "Pred: 3.543885359672179 Actual: 4.0\n",
      "Pred: 3.5442480851638245 Actual: 3.5\n",
      "Pred: 3.5445241979166706 Actual: 3.0\n",
      "Pred: 3.5437976852803246 Actual: 4.0\n",
      "Pred: 3.5439595799829493 Actual: 3.0\n",
      "Pred: 3.5435759938250864 Actual: 3.0\n",
      "Pred: 3.5438648662204293 Actual: 4.0\n",
      "Pred: 3.5441473096873684 Actual: 5.0\n",
      "Pred: 3.543986833014401 Actual: 3.0\n",
      "Pred: 3.5437707209901976 Actual: 5.0\n",
      "Pred: 3.543778006881765 Actual: 3.0\n",
      "Pred: 3.5436586119557214 Actual: 1.0\n",
      "Pred: 3.5438829322982963 Actual: 5.0\n",
      "Pred: 3.543942723363776 Actual: 3.0\n",
      "Pred: 3.5436914006178637 Actual: 2.5\n",
      "Pred: 3.543912637965536 Actual: 4.0\n",
      "Pred: 3.543987077584541 Actual: 3.0\n",
      "Pred: 3.543791700569577 Actual: 5.0\n",
      "Pred: 3.543903646069964 Actual: 4.0\n",
      "Pred: 3.5440345556454096 Actual: 3.0\n",
      "Pred: 3.5438003060738064 Actual: 3.0\n",
      "Pred: 3.5440147739739296 Actual: 4.0\n",
      "Pred: 3.5442714635957895 Actual: 4.0\n",
      "Pred: 3.543909758300652 Actual: 4.0\n",
      "Pred: 3.543853706228007 Actual: 3.5\n",
      "Pred: 3.5438174074790965 Actual: 4.5\n",
      "Pred: 3.543742288921062 Actual: 2.5\n",
      "Pred: 3.543884807208903 Actual: 5.0\n",
      "Pred: 3.54388074133745 Actual: 4.0\n",
      "Pred: 3.543943022821416 Actual: 3.0\n",
      "Pred: 3.5441299535416593 Actual: 4.0\n",
      "Pred: 3.5435147523304886 Actual: 5.0\n",
      "Pred: 3.5437060809924823 Actual: 3.0\n",
      "Pred: 3.5440486583603534 Actual: 3.0\n",
      "Pred: 3.543723789696878 Actual: 2.5\n",
      "Pred: 3.5436635702523094 Actual: 4.0\n",
      "Pred: 3.544075275527245 Actual: 4.0\n",
      "Pred: 3.543681068699434 Actual: 4.5\n",
      "Pred: 3.5442156409843903 Actual: 3.0\n",
      "Pred: 3.543880646018342 Actual: 1.0\n",
      "Pred: 3.543960467463965 Actual: 5.0\n",
      "Pred: 3.543797805378335 Actual: 3.5\n",
      "Pred: 3.5436980790070516 Actual: 4.5\n",
      "Pred: 3.5438023532585503 Actual: 3.0\n",
      "Pred: 3.5439069982613938 Actual: 3.0\n",
      "Pred: 3.5439753089441526 Actual: 3.0\n",
      "Pred: 3.5435119332651213 Actual: 3.0\n",
      "Pred: 3.54360354579405 Actual: 4.0\n",
      "Pred: 3.5438210956093816 Actual: 4.0\n",
      "Pred: 3.5438885157238498 Actual: 5.0\n",
      "Pred: 3.543648347397339 Actual: 5.0\n",
      "Pred: 3.543876472764543 Actual: 4.5\n",
      "Pred: 3.54410202085883 Actual: 4.5\n",
      "Pred: 3.54380316744664 Actual: 4.5\n",
      "Pred: 3.54395725290226 Actual: 3.0\n",
      "Pred: 3.5439547285340622 Actual: 0.5\n",
      "Pred: 3.5437264759313676 Actual: 5.0\n",
      "Pred: 3.5444054375256204 Actual: 4.5\n",
      "Pred: 3.543729978283635 Actual: 4.0\n",
      "Pred: 3.543495101004435 Actual: 1.0\n",
      "Pred: 3.5437224721721647 Actual: 3.0\n",
      "Pred: 3.543891582739792 Actual: 3.0\n",
      "Pred: 3.543803339163355 Actual: 3.0\n",
      "Pred: 3.5438422662175215 Actual: 2.5\n",
      "Pred: 3.5439473534924533 Actual: 5.0\n",
      "Pred: 3.54357491496616 Actual: 4.0\n",
      "Pred: 3.543853491135024 Actual: 4.0\n",
      "Pred: 3.5438103977151263 Actual: 1.5\n",
      "Pred: 3.5437863188940977 Actual: 3.0\n",
      "Pred: 3.5437571627386117 Actual: 5.0\n",
      "Pred: 3.543777335728124 Actual: 3.0\n",
      "Pred: 3.544070402135298 Actual: 3.5\n",
      "Pred: 3.54359837039391 Actual: 3.0\n",
      "Pred: 3.543991079725702 Actual: 1.0\n",
      "Pred: 3.5438837921601984 Actual: 4.0\n",
      "Pred: 3.543910856531745 Actual: 4.0\n",
      "Pred: 3.5442531150707413 Actual: 4.0\n",
      "Pred: 3.5438714120478583 Actual: 2.0\n",
      "Pred: 3.5438359303448985 Actual: 2.0\n",
      "Pred: 3.543654734303135 Actual: 3.0\n",
      "Pred: 3.543721995736966 Actual: 4.0\n",
      "Pred: 3.5437739324885262 Actual: 3.0\n",
      "Pred: 3.5454250596694146 Actual: 1.0\n",
      "Pred: 3.543774529171905 Actual: 4.0\n",
      "Pred: 3.544356559855703 Actual: 3.0\n",
      "overall score: -0.0015696354314620464\n",
      "Minutes: 0.14021661281585693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "#test the time taken to train and predict\n",
    "start = time.time()\n",
    "\n",
    "#this is where you may select certain features to be used to build the model\n",
    "new_user_to_features = dict()\n",
    "\n",
    "for key in user_to_features.keys():\n",
    "    new_list = []\n",
    "    for item in user_to_features[key]:\n",
    "        # can try reducing the features like below:\n",
    "        # old inputs...\n",
    "        # item[0:4]+ item[6:7]+ item[10:11]\n",
    "        # item[0:4]\n",
    "        # item[2:8] + item[8:]\n",
    "        #new inputs:\n",
    "        \n",
    "        new_list.append(item)\n",
    "    new_user_to_features[key] = new_list\n",
    "\n",
    "#seed\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#instead of using test train split...\n",
    "user_to_X_train = dict()\n",
    "user_to_y_train = dict()\n",
    "user_to_X_test = dict()\n",
    "user_to_y_test = dict()\n",
    "\n",
    "#There is a problem with using the same users in training and testing and this code ensures that it doesn't happen\n",
    "#The model should beable to be used effectively for new users and not just memorized for existing users\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "for key in new_user_to_features.keys():\n",
    "    if(random.randint(0,10) == 0):\n",
    "        user_to_X_test[key] = new_user_to_features[key]\n",
    "        user_to_y_test[key] = user_to_rand_rating[key]\n",
    "        c1+=1\n",
    "\n",
    "    else:\n",
    "        user_to_X_train[key] = new_user_to_features[key]\n",
    "        user_to_y_train[key] = user_to_rand_rating[key]\n",
    "        c2+=1\n",
    "\n",
    "#used to train model\n",
    "X_train = [] \n",
    "y_train = []\n",
    "\n",
    "#populate X_train and y_train\n",
    "for key in user_to_X_train.keys():\n",
    "    for item in user_to_X_train[key]:\n",
    "        X_train.append(item)\n",
    "        y_train.append(user_to_y_train[key])\n",
    "\n",
    "\n",
    "# scale training features...\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "\n",
    "#data transformation\n",
    "#https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n",
    "\n",
    "\n",
    "#train model\n",
    "#orginal model layers\n",
    "#layers = (2,2,2)\n",
    "layers = (2,2,2)\n",
    "# act = \"tanh\"\n",
    "# solve = \"adam\"\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# act = \"relu\"\n",
    "# solve = \"sgd\"\n",
    "# act = \"tanh\"\n",
    "# solve = \"sgd\"\n",
    "act = \"relu\"\n",
    "solve = \"adam\"\n",
    "\n",
    "\n",
    "regr = MLPRegressor(hidden_layer_sizes=layers,activation =act, solver =solve,  max_iter=10000, random_state =seed_int)\n",
    "fitted = regr.fit(X_train, y_train)\n",
    "\n",
    "#this needs to run before the final model is determined so that the best features are used\n",
    "#the results can also be displayed with a bar shart showing how each feature cotributes to a percentage of the models accuracy\n",
    "result = permutation_importance(fitted, X_train, y_train,random_state=seed_int)\n",
    "\n",
    "print(result[\"importances_mean\"])\n",
    "\n",
    "print(regr.n_iter_)\n",
    "\n",
    "#dictionary of users to test features that have been scaled\n",
    "new_user_to_X_test = dict()\n",
    "\n",
    "# used to scale test features then the new scaled features are returned ...\n",
    "# as the values of the approriate user key in new_user_to_X_test \n",
    "X_test = []\n",
    "\n",
    "#populate X_test, key, and counts that are later used to build new_user_to_X_test, a verison of...\n",
    "#user_to_X_test with scaled features \n",
    "#need to decompose then recompose\n",
    "keys = []\n",
    "counts = []\n",
    "for key in user_to_X_test.keys():\n",
    "    cnt = 0\n",
    "    for item in user_to_X_test[key]:\n",
    "        X_test.append(item)\n",
    "        cnt+=1\n",
    "    counts.append(cnt)\n",
    "    keys.append(key)\n",
    "\n",
    "#scale test features...\n",
    "scalar = StandardScaler()\n",
    "X_test = scalar.fit_transform(X_test)\n",
    "\n",
    "#populate new_user_to_X_test with scaled test features\n",
    "cnt = 0\n",
    "for num, key in zip(counts, keys):\n",
    "    new_user_to_X_test[key] = []\n",
    "    for i in range(num):\n",
    "        new_user_to_X_test[key].append(X_test[cnt])\n",
    "        cnt+=1\n",
    "\n",
    "\n",
    "# user id to the average predicted rating for the randomly chosen movie\n",
    "user_to_avg_rating = dict()\n",
    "\n",
    "# populate user_to_avg_rating by averaging the predictions from all the feature inputs of the...\n",
    "# movies a user has watched that are not the randomly chosen movie itself\n",
    "for key in new_user_to_X_test.keys():\n",
    "    sum =0\n",
    "    cnt =0 \n",
    "    predicted = regr.predict(new_user_to_X_test[key])\n",
    "    for item in predicted:\n",
    "        sum+=item\n",
    "        cnt+=1\n",
    "    user_to_avg_rating[key] = float(sum/cnt)\n",
    "\n",
    "\n",
    "#outputs\n",
    "actuals_list = []\n",
    "preds_list = []\n",
    "for key in user_to_avg_rating.keys():\n",
    "    print(\"Pred: \"+str(user_to_avg_rating[key]) , \"Actual: \"+str(user_to_y_test[key]))\n",
    "    actuals_list.append(user_to_y_test[key])\n",
    "    preds_list.append(user_to_avg_rating[key])\n",
    "print(\"overall score:\", r2_score(actuals_list, preds_list))\n",
    "\n",
    "#test the time taken to train and predict\n",
    "end = time.time()\n",
    "print(\"Minutes:\", float((end - start)/60))\n",
    "\n",
    "\n",
    "#feature importance scores:\n",
    "#https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance\n",
    "\n",
    "#introduction:\n",
    "#https://www.kaggle.com/code/dansbecker/permutation-importance\n",
    "\n",
    "#types of feature importance:\n",
    "#https://towardsdatascience.com/6-types-of-feature-importance-any-data-scientist-should-master-1bfd566f21c9\n",
    "\n",
    "\n",
    "#perhaps there is a way to visualize this of the model outputs below in a systematic way???\n",
    "\n",
    "# Tests:\n",
    "\n",
    "# full features:\n",
    "# with linear regression:\n",
    "# overall score: 0.2657455495660592\n",
    "\n",
    "# with mlp...:\n",
    "\n",
    "# first fours features:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2667063296881431\n",
    "\n",
    "#all features: \n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.26606490897808244\n",
    "\n",
    "#all features: \n",
    "# layers: (2,2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.260461734737799\n",
    "\n",
    "#all features: \n",
    "# layers: (4,4,4)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.22932745175064528\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2482634902547255\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (2,2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2616102684471122\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (3,3,3)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.25487207187202243\n",
    "\n",
    "#first two featurs:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: -0.00430015574935827\n",
    "\n",
    "#first two featurs:\n",
    "# layers: (2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.04468421358737418\n",
    "\n",
    "#3rd and 4th features:\n",
    "# layers: (2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2546480453878024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
