{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "moviesFull = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(moviesFull, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "#used to filter the rows of a data\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    length = len(array[8])\n",
    "    if(array[8][length-4:]==\"<NA>\"):\n",
    "        return False\n",
    "    length = len(array[9])\n",
    "    if(array[9][length-4:]==\"<NA>\"):\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names from string of list of json formats\n",
    "def populateNames(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            tempDict = ast.literal_eval(item)\n",
    "            names+=str(tempDict[\"name\"])\n",
    "        else:\n",
    "            tempDict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(tempDict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "\n",
    "def provideData(array):\n",
    "    movieData = []\n",
    "    movieData.append(int(array[0]))\n",
    "    movieData.append(int(array[1]))\n",
    "    movieData.append(float(array[2]))\n",
    "    movieData.append(array[3])  \n",
    "\n",
    "    movieData.append(populateNames(array[4]))\n",
    "    movieData.append(populateNames(array[5]))\n",
    "    movieData.append(populateNames(array[6]))\n",
    "    movieData.append(populateNames(array[7]))\n",
    "\n",
    "    movieData.append(str(array[8]))\n",
    "    movieData.append(str(array[9]))\n",
    "    return movieData\n",
    "    \n",
    "\n",
    "\n",
    "#convert the dataframe into an array\n",
    "#and then build a dictionary\n",
    "completeDict = dict()\n",
    "completeArray = complete.to_numpy()\n",
    "arrayOfUserIds = []\n",
    "\n",
    "\n",
    "#get all unique user ids\n",
    "lastId  = -1\n",
    "for item in completeArray:\n",
    "    if(item[0]!= lastId):\n",
    "        arrayOfUserIds.append(item[0])\n",
    "        lastId = item[0]\n",
    "\n",
    "\n",
    "index  = 0\n",
    "nofUsers = 5000\n",
    "#5000 users are tested and potentially added to the dict\n",
    "for i in range(0, nofUsers):\n",
    "    completeDict[arrayOfUserIds[i]] = []\n",
    "    for j in range(index, len(completeArray)):\n",
    "        if completeArray[j][0] == arrayOfUserIds[i]:\n",
    "            #this is where conditions are checked in completeArray[j]\n",
    "            if(condition(completeArray[j])):\n",
    "                #this is where data is tranformed\n",
    "                transformed = provideData(completeArray[j])\n",
    "                completeDict[arrayOfUserIds[i]].append(transformed)         \n",
    "        else:\n",
    "            #ignore if the number of ratings is too small\n",
    "            if (len(completeDict[arrayOfUserIds[i]])<10):\n",
    "                del completeDict[arrayOfUserIds[i]]\n",
    "            index = j+1\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running the above\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for item in completeDict.keys():\n",
    "        writer.writerows(completeDict[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the csv file\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "dataList =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    dataList = list(csv_reader)\n",
    "\n",
    "dataList = dataList[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie id to list of ratings\n",
    "movieDict = dict()\n",
    "\n",
    "#user id to the rated movies by that user\n",
    "userDict = dict()\n",
    "\n",
    "#The list created by the constructed data is in order by user id\n",
    "#this code makes a dictionary out of the data (user id to a list of text from all movies rated by the user)\n",
    "#it also makes a dictionary of movies to their ratings\n",
    "userId = -1\n",
    "for row in dataList:\n",
    "    if (row[0]!=userId):\n",
    "        userId = row[0]\n",
    "        userDict[row[0]] = [row]\n",
    "    else:\n",
    "        userDict[row[0]].append(row)\n",
    "\n",
    "    if(row[1] in movieDict.keys()):\n",
    "        movieDict[row[1]].append(row[2])\n",
    "    else:\n",
    "        movieDict[row[1]] = [row[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "#dictionary of user id to a list of string of combined textual features for each movie \n",
    "#does not include ratings or movie id\n",
    "\n",
    "combinedCorpus = dict()\n",
    "\n",
    "i = 0\n",
    "for key in userDict.keys():\n",
    "    movieStrings = []\n",
    "    for movieData in userDict[key]:\n",
    "        movieString = \"\"\n",
    "        #avoid the first three data points (user id, movieid, and rating)\n",
    "        for index in range (3,len(movieData)):\n",
    "            if(index!= len(movieData)-1):\n",
    "                movieString+= movieData[index]+\" \"\n",
    "            else:\n",
    "                movieString+= movieData[index]\n",
    "        cleaned = remove_stopwords(movieString)\n",
    "        movieStrings.append(cleaned)\n",
    "    combinedCorpus[key] = movieStrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "#seed\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#get average rating for a single movie amoung all users who rated it\n",
    "def getAverageMovieRating(movieId):\n",
    "    ret =0 \n",
    "    cnt = 0\n",
    "    for item in movieDict[movieId]:\n",
    "        ret+= float(item)\n",
    "        cnt+=1\n",
    "    return float(ret/cnt)\n",
    "\n",
    "\n",
    "#get all the user ratings \n",
    "def getUserRatings(userId):\n",
    "    ret = []\n",
    "    for item in userDict[userId]:\n",
    "        ret.append(float(item[2]))\n",
    "    return ret\n",
    "\n",
    "user_to_inputs = dict()\n",
    "user_to_rand_ratings = dict()\n",
    "\n",
    "#need to normalized data features\n",
    "#note: ratings and similairty scores for a given user need to be ordered the same:\n",
    "for key in combinedCorpus.keys():\n",
    "\n",
    "    count_matrix = CountVectorizer().fit_transform(combinedCorpus[key]).toarray().tolist()\n",
    "    #note: len(count_matrix)-1 included\n",
    "    randIndex = random.randint(0, len(count_matrix)-1)\n",
    "    randTestItem = count_matrix[randIndex]\n",
    "    del count_matrix[randIndex]\n",
    "\n",
    "    #find similarity with the count of each word between the random selected movie and the other movies rated by the user\n",
    "    cosine_sim = cosine_similarity(X = count_matrix ,Y = [randTestItem])\n",
    "\n",
    "    ratings = copy.deepcopy(getUserRatings(key))\n",
    "\n",
    "    randomRating = ratings[randIndex]\n",
    "    user_to_rand_ratings[key] = randomRating\n",
    "    del ratings[randIndex]\n",
    "\n",
    "    similairities = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "    #float symbol removed...\n",
    "    averageRatingForUser =  float(np.sum(ratings)/(len(ratings)))\n",
    "    #the movie in question is the randomly selected movie\n",
    "    avergaeRatingForMovie = getAverageMovieRating(userDict[key][randIndex][1])\n",
    "\n",
    "    #need to normalize model inputs so one feature does not dominate the other\n",
    "    #https://www.youtube.com/watch?v=Bc2dWI3vnE0&ab_channel=KrishNaik\n",
    "    #try adding the curve defining features of the users scores besides mean...\n",
    "\n",
    "    #testing removal of sim and rating...\n",
    "    #testting removal of averages\n",
    "    for sim, rating in zip(similairities, ratings):\n",
    "        if key not in user_to_inputs:\n",
    "            user_to_inputs[key] = [[sim, rating]]\n",
    "        else:\n",
    "            user_to_inputs[key].append([sim, rating])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random test: 642 1945\n",
      "23\n",
      "Pred: 3.623149617527699 Actual: 3.0\n",
      "Pred: 3.574126872348588 Actual: 4.0\n",
      "Pred: 3.4572251497973134 Actual: 3.0\n",
      "Pred: 3.5534356265062046 Actual: 4.0\n",
      "Pred: 3.543368859104209 Actual: 3.5\n",
      "Pred: 3.4880972108910546 Actual: 3.0\n",
      "Pred: 3.2062417816366917 Actual: 3.0\n",
      "Pred: 3.503919318877813 Actual: 3.0\n",
      "Pred: 3.509058668191241 Actual: 4.0\n",
      "Pred: 3.4447375682908286 Actual: 1.0\n",
      "Pred: 3.4654617862683317 Actual: 3.0\n",
      "Pred: 3.5413110422504706 Actual: 3.0\n",
      "Pred: 3.33010867169398 Actual: 3.0\n",
      "Pred: 3.4461318638797693 Actual: 4.5\n",
      "Pred: 3.512116985403862 Actual: 1.5\n",
      "Pred: 3.470952997692286 Actual: 4.0\n",
      "Pred: 3.392301481495895 Actual: 0.5\n",
      "Pred: 3.5916476347302217 Actual: 5.0\n",
      "Pred: 3.5738985627189503 Actual: 5.0\n",
      "Pred: 3.4418771629872196 Actual: 3.0\n",
      "Pred: 3.6848257365415384 Actual: 5.0\n",
      "Pred: 3.590617469301948 Actual: 5.0\n",
      "Pred: 3.520222187364846 Actual: 4.0\n",
      "Pred: 3.5905987495323335 Actual: 4.0\n",
      "Pred: 3.5221707530479827 Actual: 4.0\n",
      "Pred: 3.503023192506815 Actual: 5.0\n",
      "Pred: 3.558724881108671 Actual: 4.0\n",
      "Pred: 3.5900507099848356 Actual: 2.5\n",
      "Pred: 3.6425376018376054 Actual: 1.0\n",
      "Pred: 3.490022524852791 Actual: 5.0\n",
      "Pred: 3.5816652359333885 Actual: 5.0\n",
      "Pred: 3.5940547656174573 Actual: 3.0\n",
      "Pred: 3.5370694553428237 Actual: 4.0\n",
      "Pred: 3.469044942087494 Actual: 5.0\n",
      "Pred: 3.4973358336531137 Actual: 3.5\n",
      "Pred: 3.5581416930083307 Actual: 3.0\n",
      "Pred: 3.5097322109278224 Actual: 4.0\n",
      "Pred: 3.5959179133955947 Actual: 3.0\n",
      "Pred: 3.5233377635529024 Actual: 3.0\n",
      "Pred: 3.5938771009353436 Actual: 4.0\n",
      "Pred: 3.5037215389634992 Actual: 5.0\n",
      "Pred: 3.5973210352590326 Actual: 4.0\n",
      "Pred: 3.4885189189227366 Actual: 4.0\n",
      "Pred: 3.3938577802222363 Actual: 3.5\n",
      "Pred: 3.580538829504964 Actual: 4.0\n",
      "Pred: 3.660606119240625 Actual: 4.0\n",
      "Pred: 3.5185532183062143 Actual: 4.5\n",
      "Pred: 3.638066655260116 Actual: 3.5\n",
      "Pred: 3.5680106005001115 Actual: 4.5\n",
      "Pred: 3.721414858131574 Actual: 5.0\n",
      "Pred: 3.603223564340579 Actual: 3.0\n",
      "Pred: 3.378924145452606 Actual: 2.0\n",
      "Pred: 3.5440129156091205 Actual: 3.5\n",
      "Pred: 3.705133693716505 Actual: 3.5\n",
      "Pred: 3.5836521718809013 Actual: 5.0\n",
      "Pred: 3.6596738152198 Actual: 3.0\n",
      "Pred: 3.533191209142395 Actual: 4.5\n",
      "Pred: 3.5118343618529027 Actual: 4.0\n",
      "Pred: 3.4941524167571076 Actual: 5.0\n",
      "Pred: 3.4959842470618203 Actual: 4.0\n",
      "Pred: 3.3812715742938737 Actual: 4.0\n",
      "Pred: 3.59577337509027 Actual: 4.0\n",
      "Pred: 3.524918672549069 Actual: 3.0\n",
      "Pred: 3.574173822289228 Actual: 1.0\n",
      "Pred: 3.6045854089049567 Actual: 3.0\n",
      "Pred: 3.537698130262423 Actual: 3.0\n",
      "Pred: 3.4931644797772106 Actual: 3.0\n",
      "Pred: 3.5209091926492286 Actual: 5.0\n",
      "Pred: 3.619276071069939 Actual: 4.0\n",
      "Pred: 3.5930070922275634 Actual: 3.0\n",
      "Pred: 3.4749455199355537 Actual: 3.0\n",
      "Pred: 3.5896345340447433 Actual: 4.5\n",
      "Pred: 3.5850136740725618 Actual: 3.0\n",
      "Pred: 3.4207776696030954 Actual: 3.0\n",
      "Pred: 3.479396938853255 Actual: 1.0\n",
      "Pred: 3.5589603876095373 Actual: 5.0\n",
      "Pred: 3.618547023271214 Actual: 4.5\n",
      "Pred: 3.632067687864211 Actual: 5.0\n",
      "Pred: 3.429648537882761 Actual: 3.0\n",
      "Pred: 3.6217244438710288 Actual: 5.0\n",
      "Pred: 3.6265392846284903 Actual: 4.5\n",
      "Pred: 3.5223403038494223 Actual: 4.0\n",
      "Pred: 3.5370993187103017 Actual: 3.0\n",
      "Pred: 3.546469534047106 Actual: 4.5\n",
      "Pred: 3.5135896456763978 Actual: 2.0\n",
      "Pred: 3.4896411661892084 Actual: 5.0\n",
      "Pred: 3.5922834704728706 Actual: 3.0\n",
      "Pred: 3.492630270077894 Actual: 0.5\n",
      "Pred: 3.5730160049243542 Actual: 5.0\n",
      "Pred: 3.401496634337285 Actual: 2.5\n",
      "Pred: 3.7119797594778485 Actual: 4.0\n",
      "Pred: 3.600631078566782 Actual: 4.0\n",
      "Pred: 3.584232077313563 Actual: 3.0\n",
      "Pred: 3.5447031421624624 Actual: 4.0\n",
      "Pred: 3.539849080601156 Actual: 3.0\n",
      "Pred: 3.2625486440555287 Actual: 0.5\n",
      "Pred: 3.6411235983081616 Actual: 5.0\n",
      "Pred: 3.451776673104314 Actual: 4.0\n",
      "Pred: 3.5802334124331274 Actual: 5.0\n",
      "Pred: 3.500538241497352 Actual: 3.0\n",
      "Pred: 3.4680145395863695 Actual: 4.0\n",
      "Pred: 3.601208038303365 Actual: 4.5\n",
      "Pred: 3.383785129154995 Actual: 3.5\n",
      "Pred: 3.5350051873131227 Actual: 4.0\n",
      "Pred: 3.628005096475717 Actual: 4.0\n",
      "Pred: 3.4856920660358144 Actual: 3.0\n",
      "Pred: 3.6720718938663732 Actual: 1.0\n",
      "Pred: 3.5648237914740344 Actual: 4.0\n",
      "Pred: 3.563774788892721 Actual: 4.0\n",
      "Pred: 3.527674977554226 Actual: 3.5\n",
      "Pred: 3.603810056959854 Actual: 4.0\n",
      "Pred: 3.5703782272246047 Actual: 5.0\n",
      "Pred: 3.534154233342822 Actual: 5.0\n",
      "Pred: 3.3948277467127808 Actual: 2.5\n",
      "Pred: 3.361253262837599 Actual: 3.5\n",
      "Pred: 3.427817783243997 Actual: 2.0\n",
      "Pred: 3.5829951140648357 Actual: 3.5\n",
      "Pred: 3.5732440416964018 Actual: 4.0\n",
      "Pred: 3.5581392597120565 Actual: 4.0\n",
      "Pred: 3.528609343711031 Actual: 1.5\n",
      "Pred: 3.5818079086700774 Actual: 3.0\n",
      "Pred: 3.673701612166752 Actual: 5.0\n",
      "Pred: 3.578227013783977 Actual: 4.0\n",
      "Pred: 3.6508909528174525 Actual: 4.5\n",
      "Pred: 3.5447723775271296 Actual: 4.0\n",
      "Pred: 3.639173466030689 Actual: 4.0\n",
      "Pred: 3.503706327228799 Actual: 3.5\n",
      "Pred: 3.535870677778345 Actual: 3.5\n",
      "Pred: 3.5129276880859037 Actual: 4.0\n",
      "Pred: 3.4350650541552246 Actual: 5.0\n",
      "Pred: 3.5202148548376884 Actual: 3.0\n",
      "Pred: 3.6294336291507308 Actual: 4.0\n",
      "Pred: 3.600770776274116 Actual: 3.0\n",
      "Pred: 3.4704744428820087 Actual: 5.0\n",
      "Pred: 3.6238348389042874 Actual: 5.0\n",
      "Pred: 3.5723547849572976 Actual: 4.0\n",
      "Pred: 3.491169956215529 Actual: 4.0\n",
      "Pred: 3.6382225931061387 Actual: 4.0\n",
      "Pred: 3.54472205919439 Actual: 2.5\n",
      "Pred: 3.592116071103447 Actual: 4.5\n",
      "Pred: 3.6432582909137414 Actual: 5.0\n",
      "Pred: 3.5022767542346878 Actual: 4.5\n",
      "Pred: 3.5412812674890506 Actual: 4.0\n",
      "Pred: 3.52403657473444 Actual: 2.5\n",
      "Pred: 3.5563930556016605 Actual: 5.0\n",
      "Pred: 3.6517011441715916 Actual: 5.0\n",
      "Pred: 3.5164830351053764 Actual: 5.0\n",
      "Pred: 3.559127251410588 Actual: 4.0\n",
      "Pred: 3.4610297021504026 Actual: 4.0\n",
      "Pred: 3.571184311201267 Actual: 4.0\n",
      "Pred: 3.4605730363759357 Actual: 3.0\n",
      "Pred: 3.4335110494854564 Actual: 4.0\n",
      "Pred: 3.6211778165515107 Actual: 4.0\n",
      "Pred: 3.4270242221359317 Actual: 3.0\n",
      "Pred: 3.5829989516510063 Actual: 2.0\n",
      "Pred: 3.554358247284907 Actual: 5.0\n",
      "Pred: 3.5951961155095087 Actual: 5.0\n",
      "Pred: 3.55295921233285 Actual: 4.0\n",
      "Pred: 3.616580647694545 Actual: 4.0\n",
      "Pred: 3.539275394303214 Actual: 3.0\n",
      "Pred: 3.625732961388745 Actual: 4.0\n",
      "Pred: 3.5114946099498265 Actual: 4.0\n",
      "Pred: 3.4834675089199596 Actual: 3.0\n",
      "Pred: 3.531879587400382 Actual: 4.0\n",
      "Pred: 3.48913791874041 Actual: 3.0\n",
      "Pred: 3.542062776365252 Actual: 4.0\n",
      "Pred: 3.5041339855589166 Actual: 2.5\n",
      "Pred: 3.6212495505182623 Actual: 4.0\n",
      "Pred: 3.5581564866482123 Actual: 3.0\n",
      "Pred: 3.559506554421153 Actual: 4.0\n",
      "Pred: 3.5550898438927963 Actual: 4.0\n",
      "Pred: 3.425474810571532 Actual: 4.0\n",
      "Pred: 3.411833480832135 Actual: 3.0\n",
      "Pred: 3.6237135958189874 Actual: 4.0\n",
      "Pred: 3.508119976969974 Actual: 3.0\n",
      "Pred: 3.5260127122527476 Actual: 3.5\n",
      "Pred: 3.6195054503915736 Actual: 4.0\n",
      "Pred: 3.608353300102711 Actual: 3.0\n",
      "Pred: 3.5393817178216542 Actual: 4.0\n",
      "Pred: 3.4041795891978546 Actual: 3.0\n",
      "Pred: 3.409720594585589 Actual: 4.0\n",
      "Pred: 3.509863294872617 Actual: 4.0\n",
      "Pred: 3.495008205202024 Actual: 4.0\n",
      "Pred: 3.613753645270709 Actual: 5.0\n",
      "Pred: 3.517540749506937 Actual: 4.0\n",
      "Pred: 3.5858019815076245 Actual: 4.0\n",
      "Pred: 3.6111685027901324 Actual: 4.0\n",
      "Pred: 3.240309040050181 Actual: 2.0\n",
      "Pred: 3.5196148799909475 Actual: 1.5\n",
      "Pred: 3.5657201094766005 Actual: 5.0\n",
      "Pred: 3.5935270264481587 Actual: 5.0\n",
      "Pred: 3.577677942103307 Actual: 3.0\n",
      "Pred: 3.5871586202717003 Actual: 5.0\n",
      "Pred: 3.5285425583999097 Actual: 4.0\n",
      "Pred: 3.550718914046773 Actual: 4.0\n",
      "Pred: 3.6397174975194924 Actual: 3.0\n",
      "Pred: 3.175301632152322 Actual: 5.0\n",
      "Pred: 3.435155846238082 Actual: 3.0\n",
      "Pred: 3.5833120151711375 Actual: 5.0\n",
      "Pred: 3.4949378070170214 Actual: 4.0\n",
      "Pred: 3.3963010448499187 Actual: 3.0\n",
      "Pred: 3.587396012497016 Actual: 3.5\n",
      "Pred: 3.3753980911624244 Actual: 2.0\n",
      "Pred: 3.616458045357111 Actual: 5.0\n",
      "Pred: 3.473956038741502 Actual: 4.5\n",
      "Pred: 3.5004213993588347 Actual: 1.0\n",
      "Pred: 3.615038305100921 Actual: 4.0\n",
      "Pred: 3.599571120544739 Actual: 3.0\n",
      "Pred: 3.5269515656415713 Actual: 5.0\n",
      "Pred: 3.474786997550839 Actual: 3.0\n",
      "Pred: 3.4763270213831183 Actual: 3.0\n",
      "Pred: 3.5277227862580838 Actual: 4.0\n",
      "Pred: 3.6433174312011225 Actual: 4.0\n",
      "Pred: 3.5683960257217664 Actual: 4.0\n",
      "Pred: 3.6130835051780923 Actual: 3.5\n",
      "Pred: 3.434485862904421 Actual: 4.0\n",
      "Pred: 3.525829102588933 Actual: 4.0\n",
      "Pred: 3.5797147452600884 Actual: 5.0\n",
      "Pred: 3.5708599188656347 Actual: 4.0\n",
      "Pred: 3.510046330915935 Actual: 3.5\n",
      "Pred: 3.409227802491621 Actual: 2.0\n",
      "Pred: 3.4820668599762175 Actual: 3.0\n",
      "Pred: 3.631241995806601 Actual: 4.0\n",
      "Pred: 3.5544657018130468 Actual: 3.5\n",
      "Pred: 3.4715084624354664 Actual: 3.0\n",
      "Pred: 3.696298083537639 Actual: 4.5\n",
      "Pred: 3.4248026414663193 Actual: 4.0\n",
      "Pred: 3.546068249687886 Actual: 3.0\n",
      "Pred: 3.5876031790802076 Actual: 3.0\n",
      "Pred: 3.5134786082271545 Actual: 5.0\n",
      "Pred: 3.403390767702097 Actual: 4.0\n",
      "Pred: 3.4706076007923383 Actual: 5.0\n",
      "Pred: 3.4592860909250325 Actual: 2.0\n",
      "Pred: 3.5568562279831166 Actual: 5.0\n",
      "Pred: 3.615081354018897 Actual: 4.0\n",
      "Pred: 3.5711392910334214 Actual: 4.0\n",
      "Pred: 3.4974131624043827 Actual: 3.0\n",
      "Pred: 3.5326594824362387 Actual: 4.5\n",
      "Pred: 3.5845740902544034 Actual: 3.5\n",
      "Pred: 3.373120703359108 Actual: 3.5\n",
      "Pred: 3.565141541297743 Actual: 4.5\n",
      "Pred: 3.5966577045697656 Actual: 5.0\n",
      "Pred: 3.578143022564438 Actual: 5.0\n",
      "Pred: 3.501401741265217 Actual: 3.0\n",
      "Pred: 3.519852454890309 Actual: 5.0\n",
      "Pred: 3.568357633314162 Actual: 4.5\n",
      "Pred: 3.668609204087592 Actual: 3.0\n",
      "Pred: 3.4827527760874526 Actual: 3.5\n",
      "Pred: 3.4959484020844784 Actual: 3.0\n",
      "Pred: 3.421522183075354 Actual: 2.0\n",
      "Pred: 3.448576224829789 Actual: 4.0\n",
      "Pred: 3.5784376116244223 Actual: 5.0\n",
      "Pred: 3.47387152323806 Actual: 4.0\n",
      "Pred: 3.5249632507658855 Actual: 3.0\n",
      "Pred: 3.6621619772873424 Actual: 5.0\n",
      "Pred: 3.4572949557991035 Actual: 4.0\n",
      "Pred: 3.5366852282735386 Actual: 3.0\n",
      "Pred: 3.517373686411293 Actual: 3.0\n",
      "Pred: 3.560162558165447 Actual: 4.0\n",
      "Pred: 3.58682993395258 Actual: 5.0\n",
      "Pred: 3.5737663423147814 Actual: 4.5\n",
      "Pred: 3.5116903215964967 Actual: 3.0\n",
      "Pred: 3.625946805939264 Actual: 3.0\n",
      "Pred: 3.532660059302831 Actual: 4.0\n",
      "Pred: 3.47498547859755 Actual: 3.0\n",
      "Pred: 3.6024340749190458 Actual: 5.0\n",
      "Pred: 3.5345140115705234 Actual: 4.0\n",
      "Pred: 3.5195603631724963 Actual: 3.0\n",
      "Pred: 3.508025077616686 Actual: 3.5\n",
      "Pred: 3.497577944203463 Actual: 3.0\n",
      "Pred: 3.399502230301919 Actual: 3.0\n",
      "Pred: 3.541749091551411 Actual: 4.0\n",
      "Pred: 3.4521408800917075 Actual: 4.0\n",
      "Pred: 3.57419884761016 Actual: 4.0\n",
      "Pred: 3.5990630827535757 Actual: 4.0\n",
      "Pred: 3.648106481848056 Actual: 4.0\n",
      "Pred: 3.5989753990768274 Actual: 5.0\n",
      "Pred: 3.58096096017763 Actual: 4.0\n",
      "Pred: 3.5617793720422637 Actual: 4.0\n",
      "Pred: 3.6507564930199465 Actual: 4.0\n",
      "Pred: 3.6158278405969244 Actual: 3.0\n",
      "Pred: 3.574986170418457 Actual: 4.0\n",
      "Pred: 3.458922158497226 Actual: 3.0\n",
      "Pred: 3.406232388066769 Actual: 4.0\n",
      "Pred: 3.648323846205182 Actual: 5.0\n",
      "Pred: 3.7176941859889703 Actual: 5.0\n",
      "Pred: 3.5976676037198647 Actual: 4.5\n",
      "Pred: 3.4401305056191145 Actual: 3.0\n",
      "Pred: 3.6297592917564834 Actual: 4.0\n",
      "Pred: 3.501342736535973 Actual: 5.0\n",
      "Pred: 3.520402872024394 Actual: 3.0\n",
      "Pred: 3.648056322267307 Actual: 4.5\n",
      "Pred: 3.420795130159553 Actual: 2.0\n",
      "Pred: 3.6106815336837395 Actual: 5.0\n",
      "Pred: 3.2863213865718732 Actual: 3.0\n",
      "Pred: 3.47384784350509 Actual: 3.0\n",
      "Pred: 3.4580936846369417 Actual: 2.0\n",
      "Pred: 3.470897359758014 Actual: 3.0\n",
      "Pred: 3.6250572507753804 Actual: 5.0\n",
      "Pred: 3.594419622076898 Actual: 5.0\n",
      "Pred: 3.482605457437894 Actual: 3.0\n",
      "Pred: 3.4492943184264564 Actual: 3.5\n",
      "Pred: 3.4706594626724887 Actual: 5.0\n",
      "Pred: 3.5053170472907422 Actual: 2.0\n",
      "Pred: 3.497665024725435 Actual: 4.0\n",
      "Pred: 3.6211613123662283 Actual: 4.0\n",
      "Pred: 3.4877682567031507 Actual: 5.0\n",
      "Pred: 3.5485607395827534 Actual: 3.0\n",
      "Pred: 3.3963589917403842 Actual: 5.0\n",
      "Pred: 3.503274838004636 Actual: 1.0\n",
      "Pred: 3.502275383184273 Actual: 4.0\n",
      "Pred: 3.6530190635024473 Actual: 4.0\n",
      "Pred: 3.6136302965240645 Actual: 5.0\n",
      "Pred: 3.557958236478414 Actual: 3.5\n",
      "Pred: 3.545833823503185 Actual: 2.5\n",
      "Pred: 3.5608101375133105 Actual: 4.0\n",
      "Pred: 3.532434803879194 Actual: 5.0\n",
      "Pred: 3.6032093514284513 Actual: 4.0\n",
      "Pred: 3.4843958693387154 Actual: 4.0\n",
      "Pred: 3.478507767589735 Actual: 4.0\n",
      "Pred: 3.559145029040787 Actual: 2.5\n",
      "Pred: 3.585869119477556 Actual: 3.0\n",
      "Pred: 3.564901294825837 Actual: 3.0\n",
      "Pred: 3.627403479428395 Actual: 5.0\n",
      "Pred: 3.4726974591845168 Actual: 3.0\n",
      "Pred: 3.5430394820696867 Actual: 4.0\n",
      "Pred: 3.421163124448074 Actual: 3.5\n",
      "Pred: 3.4521316853752877 Actual: 3.0\n",
      "Pred: 3.5298889857807727 Actual: 4.0\n",
      "Pred: 3.7076341315772967 Actual: 5.0\n",
      "Pred: 3.6263216218937546 Actual: 5.0\n",
      "Pred: 3.637424274361098 Actual: 3.0\n",
      "Pred: 3.561410840244888 Actual: 4.5\n",
      "Pred: 3.4450432829255027 Actual: 3.0\n",
      "Pred: 3.522375182376495 Actual: 4.0\n",
      "Pred: 3.5052128754169187 Actual: 5.0\n",
      "Pred: 3.490877282492024 Actual: 1.0\n",
      "Pred: 3.5022591579848434 Actual: 3.0\n",
      "Pred: 3.5944449783678905 Actual: 3.5\n",
      "Pred: 3.4813589099640163 Actual: 3.0\n",
      "Pred: 3.4906555803526493 Actual: 4.0\n",
      "Pred: 3.4706397338668733 Actual: 4.0\n",
      "Pred: 3.5423071062977534 Actual: 3.0\n",
      "Pred: 3.4730459451059246 Actual: 3.0\n",
      "Pred: 3.615214558159178 Actual: 5.0\n",
      "Pred: 3.495785188778498 Actual: 1.0\n",
      "Pred: 3.3830403755272824 Actual: 3.0\n",
      "Pred: 3.553703680024426 Actual: 3.0\n",
      "Pred: 3.5639309184956356 Actual: 4.5\n",
      "Pred: 3.5846586844007566 Actual: 3.0\n",
      "Pred: 3.639259482005302 Actual: 4.5\n",
      "Pred: 3.60565729579024 Actual: 4.0\n",
      "Pred: 3.4144782492085177 Actual: 2.0\n",
      "Pred: 3.6477675268667102 Actual: 5.0\n",
      "Pred: 3.56250687064257 Actual: 3.0\n",
      "Pred: 3.511960003784959 Actual: 3.5\n",
      "Pred: 3.529961674088959 Actual: 2.0\n",
      "Pred: 3.544773127888262 Actual: 3.0\n",
      "Pred: 3.531194020845668 Actual: 4.0\n",
      "Pred: 3.5433230720963333 Actual: 3.0\n",
      "Pred: 3.5846852333352093 Actual: 4.0\n",
      "Pred: 3.59589603226069 Actual: 5.0\n",
      "Pred: 3.5754286008523013 Actual: 4.5\n",
      "Pred: 3.6372099714860107 Actual: 2.0\n",
      "Pred: 3.409349617180778 Actual: 3.5\n",
      "Pred: 3.44943353001773 Actual: 3.5\n",
      "Pred: 3.621197553653628 Actual: 4.0\n",
      "Pred: 3.5450136104703005 Actual: 5.0\n",
      "Pred: 3.636633556977547 Actual: 4.0\n",
      "Pred: 3.524054182749822 Actual: 4.0\n",
      "Pred: 3.5129277976764746 Actual: 4.0\n",
      "Pred: 3.459492982908885 Actual: 3.0\n",
      "Pred: 3.597608878159787 Actual: 5.0\n",
      "Pred: 3.5181564647659926 Actual: 4.0\n",
      "Pred: 3.6737694511610473 Actual: 5.0\n",
      "Pred: 3.614752354759974 Actual: 2.0\n",
      "Pred: 3.5142583973176156 Actual: 3.0\n",
      "Pred: 3.5299340076929187 Actual: 5.0\n",
      "Pred: 3.518733921941681 Actual: 3.0\n",
      "Pred: 3.5110417016473887 Actual: 1.0\n",
      "Pred: 3.4670864700994466 Actual: 1.0\n",
      "Pred: 3.4585283459052576 Actual: 3.0\n",
      "Pred: 3.4920463003165367 Actual: 1.5\n",
      "Pred: 3.5635112844776664 Actual: 3.5\n",
      "Pred: 3.509864477601277 Actual: 5.0\n",
      "Pred: 3.611356600302294 Actual: 5.0\n",
      "Pred: 3.5930989471423604 Actual: 2.5\n",
      "Pred: 3.556506905665346 Actual: 4.0\n",
      "Pred: 3.611295090414559 Actual: 4.0\n",
      "Pred: 3.6685152051964707 Actual: 5.0\n",
      "Pred: 3.5342106763659533 Actual: 3.5\n",
      "Pred: 3.5751954342198347 Actual: 3.0\n",
      "Pred: 3.444897636721149 Actual: 3.0\n",
      "Pred: 3.660181677738449 Actual: 4.5\n",
      "Pred: 3.5239025040299294 Actual: 4.0\n",
      "Pred: 3.4636927604260914 Actual: 3.0\n",
      "Pred: 3.529628750841579 Actual: 4.0\n",
      "Pred: 3.5202774403508026 Actual: 4.0\n",
      "Pred: 3.5472247327324444 Actual: 5.0\n",
      "Pred: 3.626620438789751 Actual: 3.5\n",
      "Pred: 3.5437352421962895 Actual: 3.5\n",
      "Pred: 3.4943310990231384 Actual: 3.5\n",
      "Pred: 3.48474137690267 Actual: 3.0\n",
      "Pred: 3.6888593159493968 Actual: 5.0\n",
      "Pred: 3.5231987121241466 Actual: 3.0\n",
      "Pred: 3.456930759861093 Actual: 3.5\n",
      "Pred: 3.507079428745663 Actual: 3.5\n",
      "Pred: 3.573126510998767 Actual: 3.5\n",
      "Pred: 3.594325500096298 Actual: 4.5\n",
      "Pred: 3.5456230462227722 Actual: 5.0\n",
      "Pred: 3.5913044536586276 Actual: 5.0\n",
      "Pred: 3.5541170574325047 Actual: 3.5\n",
      "Pred: 3.660275741000673 Actual: 2.0\n",
      "Pred: 3.424813442675161 Actual: 3.5\n",
      "Pred: 3.5914801684901896 Actual: 3.0\n",
      "Pred: 3.6083018048634585 Actual: 5.0\n",
      "Pred: 3.5236919225434864 Actual: 2.5\n",
      "Pred: 3.623156822125404 Actual: 3.5\n",
      "Pred: 3.5659262084365744 Actual: 5.0\n",
      "Pred: 3.456983729327975 Actual: 3.0\n",
      "Pred: 3.5225105551927154 Actual: 4.5\n",
      "Pred: 3.322963314309042 Actual: 3.0\n",
      "Pred: 3.5492779591041135 Actual: 3.0\n",
      "Pred: 3.61956990156419 Actual: 5.0\n",
      "Pred: 3.54597849942463 Actual: 3.5\n",
      "Pred: 3.564796162590504 Actual: 4.0\n",
      "Pred: 3.641439587528738 Actual: 4.0\n",
      "Pred: 3.588706428511826 Actual: 4.0\n",
      "Pred: 3.639931309587566 Actual: 4.0\n",
      "Pred: 3.7505569712790985 Actual: 4.5\n",
      "Pred: 3.5212227675963628 Actual: 3.0\n",
      "Pred: 3.5465424445296736 Actual: 3.0\n",
      "Pred: 3.5293677784726376 Actual: 1.0\n",
      "Pred: 3.5829911853444676 Actual: 3.0\n",
      "Pred: 3.5331249923294474 Actual: 3.5\n",
      "Pred: 3.5441914771567817 Actual: 5.0\n",
      "Pred: 3.578272950195663 Actual: 5.0\n",
      "Pred: 3.593192269934853 Actual: 4.5\n",
      "Pred: 3.5536274438215334 Actual: 3.0\n",
      "Pred: 3.5175065760586612 Actual: 5.0\n",
      "Pred: 3.4898728206244805 Actual: 4.0\n",
      "Pred: 3.5138646048915203 Actual: 3.0\n",
      "Pred: 3.555980752445794 Actual: 3.0\n",
      "Pred: 3.504322178614576 Actual: 5.0\n",
      "Pred: 3.4607092313327614 Actual: 3.0\n",
      "Pred: 3.513595977246093 Actual: 4.0\n",
      "Pred: 3.553537595443491 Actual: 4.0\n",
      "Pred: 3.5685883810698185 Actual: 5.0\n",
      "Pred: 3.6412660268344204 Actual: 2.0\n",
      "Pred: 3.528246363399732 Actual: 3.0\n",
      "Pred: 3.5465386596776964 Actual: 5.0\n",
      "Pred: 3.5473354618738333 Actual: 3.0\n",
      "Pred: 3.5201793518922733 Actual: 2.0\n",
      "Pred: 3.4918806033039327 Actual: 2.5\n",
      "Pred: 3.6072345219016473 Actual: 4.0\n",
      "Pred: 3.509661929368242 Actual: 4.0\n",
      "Pred: 3.706405755845146 Actual: 2.0\n",
      "Pred: 3.5153296935330323 Actual: 5.0\n",
      "Pred: 3.668566439149802 Actual: 5.0\n",
      "Pred: 3.5038126658373367 Actual: 3.5\n",
      "Pred: 3.595223351684777 Actual: 4.0\n",
      "Pred: 3.554585530817528 Actual: 5.0\n",
      "Pred: 3.597518928840156 Actual: 4.0\n",
      "Pred: 3.465340167057023 Actual: 3.0\n",
      "Pred: 3.6321636526523404 Actual: 5.0\n",
      "Pred: 3.520518834216293 Actual: 4.0\n",
      "Pred: 3.5612185521014017 Actual: 5.0\n",
      "Pred: 3.44372827583419 Actual: 3.5\n",
      "Pred: 3.6247821865545977 Actual: 1.0\n",
      "Pred: 3.6423311989198686 Actual: 4.0\n",
      "Pred: 3.5447151399555454 Actual: 3.5\n",
      "Pred: 3.6079873313063375 Actual: 3.0\n",
      "Pred: 3.583698428604422 Actual: 1.0\n",
      "Pred: 3.5972908966272206 Actual: 3.0\n",
      "Pred: 3.606535781271257 Actual: 4.0\n",
      "Pred: 3.566342845540415 Actual: 5.0\n",
      "Pred: 3.597590794031555 Actual: 3.0\n",
      "Pred: 3.4628167871248943 Actual: 4.0\n",
      "Pred: 3.6000118600246833 Actual: 4.0\n",
      "Pred: 3.538071725950191 Actual: 4.5\n",
      "Pred: 3.541622586474123 Actual: 3.5\n",
      "Pred: 3.551650254224889 Actual: 2.0\n",
      "Pred: 3.523397063784959 Actual: 4.5\n",
      "Pred: 3.4871012737908202 Actual: 3.0\n",
      "Pred: 3.6208275769819074 Actual: 4.0\n",
      "Pred: 3.5631525042708563 Actual: 3.5\n",
      "Pred: 3.541285404789304 Actual: 3.0\n",
      "Pred: 3.38974318138711 Actual: 0.5\n",
      "Pred: 3.473678863141536 Actual: 3.0\n",
      "Pred: 3.607248021583015 Actual: 3.5\n",
      "Pred: 3.4803267425957123 Actual: 4.0\n",
      "Pred: 3.6110635851575075 Actual: 5.0\n",
      "Pred: 3.4563144734581654 Actual: 3.0\n",
      "Pred: 3.632647807916488 Actual: 4.0\n",
      "Pred: 3.5478908271005998 Actual: 3.0\n",
      "Pred: 3.6017096985845587 Actual: 3.0\n",
      "Pred: 3.5784331138948082 Actual: 4.0\n",
      "Pred: 3.6512005018992073 Actual: 3.0\n",
      "Pred: 3.4127891938630666 Actual: 1.5\n",
      "Pred: 3.4207835538962503 Actual: 5.0\n",
      "Pred: 3.483826643011286 Actual: 4.0\n",
      "Pred: 3.604766300769573 Actual: 4.0\n",
      "Pred: 3.495604242982275 Actual: 3.0\n",
      "Pred: 3.571016292459295 Actual: 3.0\n",
      "Pred: 3.4789698162319067 Actual: 3.0\n",
      "Pred: 3.548725032792384 Actual: 3.0\n",
      "Pred: 3.4575185592829234 Actual: 4.0\n",
      "Pred: 3.414690116304876 Actual: 3.0\n",
      "Pred: 3.542936605074807 Actual: 4.0\n",
      "Pred: 3.44833996193101 Actual: 3.5\n",
      "Pred: 3.5021661238223 Actual: 4.0\n",
      "Pred: 3.654072328311787 Actual: 4.5\n",
      "Pred: 3.5223222919743855 Actual: 3.0\n",
      "Pred: 3.5909402709157567 Actual: 3.0\n",
      "Pred: 3.675643542954556 Actual: 4.0\n",
      "Pred: 3.455005167108937 Actual: 4.0\n",
      "Pred: 3.4411929416035445 Actual: 5.0\n",
      "Pred: 3.54804815488697 Actual: 5.0\n",
      "Pred: 3.4186131726492204 Actual: 2.5\n",
      "Pred: 3.4951287564184286 Actual: 1.0\n",
      "Pred: 3.619881016582945 Actual: 4.0\n",
      "Pred: 3.6245773611647683 Actual: 2.0\n",
      "Pred: 3.353966006361943 Actual: 3.5\n",
      "Pred: 3.638222539025526 Actual: 5.0\n",
      "Pred: 3.6668009360656315 Actual: 2.5\n",
      "Pred: 3.5006808625385784 Actual: 4.0\n",
      "Pred: 3.5869611185818653 Actual: 4.0\n",
      "Pred: 3.683665965805205 Actual: 4.0\n",
      "Pred: 3.554339920652418 Actual: 4.0\n",
      "Pred: 3.607081974786018 Actual: 2.0\n",
      "Pred: 3.5191213638762053 Actual: 3.0\n",
      "Pred: 3.6326244978277527 Actual: 5.0\n",
      "Pred: 3.5918110873031712 Actual: 3.0\n",
      "Pred: 3.6034479090635254 Actual: 1.0\n",
      "Pred: 3.668539500287569 Actual: 4.0\n",
      "Pred: 3.5100459348940802 Actual: 4.0\n",
      "Pred: 3.560060206727525 Actual: 1.0\n",
      "Pred: 3.451840201872224 Actual: 2.0\n",
      "Pred: 3.3832007098143895 Actual: 1.0\n",
      "Pred: 3.581755224642717 Actual: 4.5\n",
      "Pred: 3.4430007890309042 Actual: 3.0\n",
      "Pred: 3.4753104146289195 Actual: 4.0\n",
      "Pred: 3.5857497789015276 Actual: 4.0\n",
      "Pred: 3.5017351257182803 Actual: 2.0\n",
      "Pred: 3.6354210499617663 Actual: 4.5\n",
      "Pred: 3.4264682596585514 Actual: 3.0\n",
      "Pred: 3.5193386204372907 Actual: 3.0\n",
      "Pred: 3.6689162981966255 Actual: 5.0\n",
      "Pred: 3.579394455271977 Actual: 4.0\n",
      "Pred: 3.3762304586336556 Actual: 1.0\n",
      "Pred: 3.6189619384898495 Actual: 4.0\n",
      "Pred: 3.473554536835467 Actual: 3.0\n",
      "Pred: 3.511248233661144 Actual: 3.0\n",
      "Pred: 3.548275563090762 Actual: 3.0\n",
      "Pred: 3.631430075003526 Actual: 5.0\n",
      "Pred: 3.5490324313729595 Actual: 5.0\n",
      "Pred: 3.5967814674423573 Actual: 4.0\n",
      "Pred: 3.6488592995528695 Actual: 4.0\n",
      "Pred: 3.507892663659353 Actual: 3.5\n",
      "Pred: 3.3993183106661875 Actual: 3.5\n",
      "Pred: 3.599518667758945 Actual: 4.0\n",
      "Pred: 3.4690717229071395 Actual: 2.0\n",
      "Pred: 3.5911426233765473 Actual: 4.0\n",
      "Pred: 3.5336701859415887 Actual: 4.5\n",
      "Pred: 3.69328648855433 Actual: 4.5\n",
      "Pred: 3.5397634911516476 Actual: 3.0\n",
      "Pred: 3.560597138339367 Actual: 4.5\n",
      "Pred: 3.4761718485072723 Actual: 5.0\n",
      "Pred: 3.525080360560755 Actual: 4.0\n",
      "Pred: 3.5509900170694126 Actual: 0.5\n",
      "Pred: 3.6613663272053714 Actual: 4.0\n",
      "Pred: 3.635873292261334 Actual: 4.0\n",
      "Pred: 3.62570095963176 Actual: 3.0\n",
      "Pred: 3.6767076454120144 Actual: 0.5\n",
      "Pred: 3.4426144471061373 Actual: 3.0\n",
      "Pred: 3.5467480866273866 Actual: 4.0\n",
      "Pred: 3.7037405287446177 Actual: 4.0\n",
      "Pred: 3.6249405748686847 Actual: 4.0\n",
      "Pred: 3.5268840666919536 Actual: 4.0\n",
      "Pred: 3.6196609938646076 Actual: 3.0\n",
      "Pred: 3.42972823327125 Actual: 4.0\n",
      "Pred: 3.553929367343239 Actual: 2.0\n",
      "Pred: 3.6334016034667447 Actual: 4.0\n",
      "Pred: 3.619306775632877 Actual: 2.0\n",
      "Pred: 3.449188871044262 Actual: 4.0\n",
      "Pred: 3.484832159800907 Actual: 4.5\n",
      "Pred: 3.496243807409277 Actual: 4.5\n",
      "Pred: 3.487310639722356 Actual: 2.0\n",
      "Pred: 3.5458739682907816 Actual: 5.0\n",
      "Pred: 3.5963373659131346 Actual: 5.0\n",
      "Pred: 3.4921991768285956 Actual: 3.0\n",
      "Pred: 3.4846370194817475 Actual: 5.0\n",
      "Pred: 3.5973625832856193 Actual: 4.5\n",
      "Pred: 3.537583708837846 Actual: 4.0\n",
      "Pred: 3.5622740144167997 Actual: 4.0\n",
      "Pred: 3.563058513062542 Actual: 4.0\n",
      "Pred: 3.5213124386281653 Actual: 3.0\n",
      "Pred: 3.5713570574653937 Actual: 4.0\n",
      "Pred: 3.5172011897563618 Actual: 3.0\n",
      "Pred: 3.454749818134398 Actual: 3.5\n",
      "Pred: 3.4848444533408816 Actual: 5.0\n",
      "Pred: 3.4322516827837535 Actual: 5.0\n",
      "Pred: 3.40819151412254 Actual: 3.0\n",
      "Pred: 3.7008687124621362 Actual: 4.5\n",
      "Pred: 3.474920718845592 Actual: 4.0\n",
      "Pred: 3.630393866771172 Actual: 4.0\n",
      "Pred: 3.5924325125978576 Actual: 5.0\n",
      "Pred: 3.5358483972671357 Actual: 3.0\n",
      "Pred: 3.5599796995177315 Actual: 3.0\n",
      "Pred: 3.5573456905140284 Actual: 3.5\n",
      "Pred: 3.4817630947340272 Actual: 3.0\n",
      "Pred: 3.499731727915863 Actual: 4.0\n",
      "Pred: 3.3782026973501513 Actual: 3.5\n",
      "Pred: 3.5236338642540925 Actual: 5.0\n",
      "Pred: 3.6928540570037764 Actual: 5.0\n",
      "Pred: 3.7013787113621026 Actual: 2.0\n",
      "Pred: 3.655549016875831 Actual: 4.5\n",
      "Pred: 3.265731310732955 Actual: 2.5\n",
      "Pred: 3.531373649993128 Actual: 4.0\n",
      "Pred: 3.691473974032928 Actual: 4.0\n",
      "Pred: 3.6319834890636162 Actual: 4.5\n",
      "Pred: 3.542018889086455 Actual: 5.0\n",
      "Pred: 3.535900956940355 Actual: 5.0\n",
      "Pred: 3.5745709552978275 Actual: 5.0\n",
      "Pred: 3.5123354160424167 Actual: 3.0\n",
      "Pred: 3.655122290582447 Actual: 5.0\n",
      "Pred: 3.4044493580481214 Actual: 3.0\n",
      "Pred: 3.5961308467984012 Actual: 5.0\n",
      "Pred: 3.5914010925442086 Actual: 4.0\n",
      "Pred: 3.5417488327111792 Actual: 5.0\n",
      "Pred: 3.6598216412853795 Actual: 5.0\n",
      "Pred: 3.5269421092855664 Actual: 1.0\n",
      "Pred: 3.4561686053920124 Actual: 3.0\n",
      "Pred: 3.640607359297502 Actual: 5.0\n",
      "Pred: 3.616062695909585 Actual: 2.0\n",
      "Pred: 3.5170427116558365 Actual: 4.0\n",
      "Pred: 3.5388912448302814 Actual: 3.0\n",
      "Pred: 3.5307218021672058 Actual: 4.0\n",
      "Pred: 3.5791540806561595 Actual: 3.5\n",
      "Pred: 3.6239789476843978 Actual: 3.5\n",
      "Pred: 3.3411513136839566 Actual: 3.5\n",
      "Pred: 3.651925073225182 Actual: 4.0\n",
      "overall score: 0.02526772169834124\n",
      "Minutes: 0.07766722043355306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#seed\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#instead of using test train split...\n",
    "user_to_X_train = dict()\n",
    "user_to_y_train = dict()\n",
    "user_to_X_test = dict()\n",
    "user_to_y_test = dict()\n",
    "\n",
    "\n",
    "#LOOK!!!\n",
    "#Is there a problem with using the same users in training and testing???\n",
    "#instead users should be in either test or train data not both\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "for key in user_to_inputs.keys():\n",
    "    if(random.randint(0,3) == 0):\n",
    "        user_to_X_test[key] = user_to_inputs[key]\n",
    "        user_to_y_test[key] = user_to_rand_ratings[key]\n",
    "        c1+=1\n",
    "\n",
    "    else:\n",
    "        user_to_X_train[key] = user_to_inputs[key]\n",
    "        user_to_y_train[key] = user_to_rand_ratings[key]\n",
    "        c2+=1\n",
    "\n",
    "X_train = [] \n",
    "y_train = []\n",
    "print(\"Random test:\", c1, c2)\n",
    "\n",
    "#note: there is no user_to_X_train[key] of length 0\n",
    "for key in user_to_X_train.keys():\n",
    "    for item in user_to_X_train[key]:\n",
    "        X_train.append(item)\n",
    "        y_train.append(user_to_y_train[key])\n",
    "\n",
    "\n",
    "#should you standardize outputs (y_test and y_train)?\n",
    "\n",
    "#tranform training features...\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "\n",
    "#tranform test features...\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "keys = []\n",
    "counts = []\n",
    "for key in user_to_X_test.keys():\n",
    "    cnt = 0\n",
    "    #note: user_to_X_test[key] has at least one item...\n",
    "    for item in user_to_X_test[key]:\n",
    "        X_test.append(item)\n",
    "        y_test.append(user_to_y_test[key])\n",
    "        cnt+=1\n",
    "    counts.append(cnt)\n",
    "    keys.append(key)\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X_test = scalar.fit_transform(X_test)\n",
    "\n",
    "new_user_to_X_test = dict()\n",
    "new_user_to_y_test = dict()\n",
    "\n",
    "cnt = 0\n",
    "for num, key in zip(counts, keys):\n",
    "    new_user_to_X_test[key] = []\n",
    "    for i in range(num):\n",
    "        new_user_to_X_test[key].append(X_test[cnt])\n",
    "        cnt+=1\n",
    "    new_user_to_y_test[key] = user_to_y_test[key]\n",
    "\n",
    "\n",
    "#need to use featurs scaling for X...\n",
    "#also needs to apply a special function to feature scale for test data...\n",
    "#user_to_X_test is not in list format it is a dictionary\n",
    "#what should happend is creating a list out of the lists in the dictionary values\n",
    "#Then returning the standardized values to a new dictionary with standardized inputs bu the same user mappin\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "#https://stackoverflow.com/questions/55296675/is-is-necessary-to-normalize-data-before-using-mlpregressor\n",
    "#https://www.youtube.com/watch?v=uet8ZQpyJV8&ab_channel=NeuralNine\n",
    "#https://stats.stackexchange.com/questions/278566/if-you-standardize-x-must-you-always-standardize-y\n",
    "\n",
    "#LOOK!!!\n",
    "#Is there a problem with using the same users in training and testing???\n",
    "\n",
    "\n",
    "#try higher demensionality\n",
    "#try more layers\n",
    "#try adding the curve defining features of the users scores besides mean\n",
    "#(10,10,10,10)\n",
    "#(10,10,10)\n",
    "#(10,10)\n",
    "#(5,5,5)\n",
    "layers = (5, 5,5)\n",
    "act = \"tanh\"\n",
    "solve = \"adam\"\n",
    "regr = MLPRegressor(hidden_layer_sizes=layers,activation =act, solver =solve,  max_iter=10000, random_state =seed_int)\n",
    "regr.fit(X_train, y_train)\n",
    "print(regr.n_iter_)\n",
    "\n",
    "\n",
    "# note: there could also be a weigthed average applied\n",
    "# Assume there is no guaranteed order for the list of keys returned by the keys() function.\n",
    "\n",
    "user_to_avg_rating = dict()\n",
    "\n",
    "for key in new_user_to_X_test.keys():\n",
    "    sum =0\n",
    "    cnt =0 \n",
    "    predicted = regr.predict(new_user_to_X_test[key])\n",
    "    for item in predicted:\n",
    "        sum+=item\n",
    "        cnt+=1\n",
    "    user_to_avg_rating[key] = float(sum/cnt)\n",
    "\n",
    "\n",
    "actuals_list = []\n",
    "preds_list = []\n",
    "for key in user_to_avg_rating.keys():\n",
    "    print(\"Pred: \"+str(user_to_avg_rating[key]) , \"Actual: \"+str(new_user_to_y_test[key]))\n",
    "    actuals_list.append(new_user_to_y_test[key])\n",
    "    preds_list.append(user_to_avg_rating[key])\n",
    "\n",
    "\n",
    "print(\"overall score:\", r2_score(actuals_list, preds_list))\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Minutes:\", float((end - start)/60))\n",
    "\n",
    "#layers: (10,10)\n",
    "#avg only: 0.1344422405859914\n",
    "#all features: 0.1263325462370427\n",
    "\n",
    "#layers: (10, 10,10)\n",
    "#avg only: 0.049823527336465334\n",
    "#all features: -0.015054211264476702\n",
    "\n",
    "#layers: (5,5,5)\n",
    "#avg only: 0.14120225901404682\n",
    "#all features: 0.16311449260270638\n",
    "#no averages: 0.02526772169834124\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
