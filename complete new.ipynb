{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False \n",
    "    #note: these can probably be omitted due to the dropna function above\n",
    "    length = len(array[8])\n",
    "    if(array[8][length-4:]==\"<NA>\"):\n",
    "        return False\n",
    "    length = len(array[9])\n",
    "    if(array[9][length-4:]==\"<NA>\"):\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#convert the dataframe into an array and build a dictionary\n",
    "user_to_data = dict()\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "\n",
    "#get all unique user ids\n",
    "list_of_user_ids = []\n",
    "last_id  = -1\n",
    "for item in complete_array:\n",
    "    if(item[0]!= last_id):\n",
    "        list_of_user_ids.append(item[0])\n",
    "        last_id = item[0]\n",
    "\n",
    "\n",
    "index  = 0\n",
    "#this has been tested with 5000, 10000, 20000, 100000\n",
    "nof_users = 20000\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, nof_users):\n",
    "    user_to_data[list_of_user_ids[i]] = []\n",
    "    for j in range(index, len(complete_array)):\n",
    "        if complete_array[j][0] == list_of_user_ids[i]:\n",
    "            #condition is checked for complete_array[j]\n",
    "            if(condition(complete_array[j])):\n",
    "                #this is where data is tranformed\n",
    "                transformed = provide_data(complete_array[j])\n",
    "                user_to_data[list_of_user_ids[i]].append(transformed)         \n",
    "        else:\n",
    "            #ignore if the number of ratings for a user is too small\n",
    "            #this can be a higher threshold\n",
    "            if (len(user_to_data[list_of_user_ids[i]])<10):\n",
    "                del user_to_data[list_of_user_ids[i]]\n",
    "            index = j+1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for key in user_to_data.keys():\n",
    "        writer.writerows(user_to_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie id to list of its ratings by all users\n",
    "movie_to_ratings = dict()\n",
    "\n",
    "#user id to the ratings of movies by the user\n",
    "user_to_ratings = dict()\n",
    "\n",
    "#The list created by the constructed data csv is in order by user id\n",
    "#This code populates movie_to_ratings and user_to_ratings\n",
    "user_id = -1\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_ratings[row[0]] = [row]\n",
    "    else:\n",
    "        user_to_ratings[row[0]].append(row)\n",
    "\n",
    "    if(row[1] in movie_to_ratings.keys()):\n",
    "        movie_to_ratings[row[1]].append(row[2])\n",
    "    else:\n",
    "        movie_to_ratings[row[1]] = [row[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "#dictionary of user id to a list of strings of combined textual features for each movie rated by the user\n",
    "#the strings do not include ratings or movie id\n",
    "user_to_corpus_list = dict()\n",
    "\n",
    "for key in user_to_ratings.keys():\n",
    "    movie_strings = []\n",
    "    for movie_data in user_to_ratings[key]:\n",
    "        movie_string = \"\"\n",
    "        #avoid the first three data points (user id, movieid, and rating)\n",
    "        #use only the text data\n",
    "        for index in range (3,len(movie_data)):\n",
    "            if(index!= len(movie_data)-1):\n",
    "                movie_string+= movie_data[index]+\" \"\n",
    "            else:\n",
    "                movie_string+= movie_data[index]\n",
    "        #note: cleaned is a string and no lemmatizaion occurs here...\n",
    "        cleaned = remove_stopwords(movie_string)\n",
    "        movie_strings.append(cleaned)\n",
    "    user_to_corpus_list[key] = movie_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 1\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#get average rating for a single movie amoung all users who rated it\n",
    "#note: this should omit the rating for the user in question\n",
    "def get_avg_movie_rating(movie_id):\n",
    "    ret =0 \n",
    "    cnt = 0\n",
    "    for item in movie_to_ratings[movie_id]:\n",
    "        ret+= float(item)\n",
    "        cnt+=1\n",
    "    return float(ret/cnt)\n",
    "\n",
    "\n",
    "#get all the movie ratings from a single user\n",
    "def get_user_ratings(user_id):\n",
    "    ret = []\n",
    "    for item in user_to_ratings[user_id]:\n",
    "        ret.append(float(item[2]))\n",
    "    return ret\n",
    "\n",
    "\n",
    "#user to model independent var X\n",
    "user_to_features = dict()\n",
    "#user to model dependent var y\n",
    "user_to_rand_rating = dict()\n",
    "\n",
    "\n",
    "#start of k-means generation (or up one cell???):\n",
    "#need to start at 1 cluster and work way up while noting the difference in inertia using the elbow method\n",
    "#or the max number of clusters can be arbitrary\n",
    "#each model with its numebr of clutsers needs to be saved so that the random test movie...\n",
    "#can be grouped into a group that the user has a documented rating in (this means a user must have at least two ratings)\n",
    "\n",
    "#problem: to guess a new users rating requires that none of the users ratings have been used to train the model\n",
    "#but the cluster algorithm by default does take the entire dataset which is eventually grouped into test and train.\n",
    "#in realiy the data needs to be split into test and train before clustering\n",
    "\n",
    "#inputs feature 1: \n",
    "#predict the cluster of the movie and find the user in questions average rating for that cluster\n",
    "\n",
    "#inputs feature 2: \n",
    "#the movies rating for all users \n",
    "\n",
    "#other features: \n",
    "#distribution statistics for the users cluster that the random movie is a part of\n",
    "#distribution statistics for the movie itself amoung all other users\n",
    "\n",
    "\n",
    "\n",
    "#populate user_to_features and user_to_rand_rating\n",
    "for key in user_to_corpus_list.keys():\n",
    "    #note: this is the advantage of keeping  user_to_corpus_list[key] as a list of strings...\n",
    "    #ability to use CountVectorizor\n",
    "\n",
    "    count_matrix = CountVectorizer().fit_transform(user_to_corpus_list[key]).toarray().tolist()\n",
    "    rand_index = random.randint(0, len(count_matrix)-1)\n",
    "    rand_test_item = count_matrix[rand_index]\n",
    "    del count_matrix[rand_index]\n",
    "\n",
    "    #find similarity by the count of each word between the random selected movie and the other movies rated by the user\n",
    "    cosine_sim = cosine_similarity(X = count_matrix ,Y = [rand_test_item])\n",
    "\n",
    "    #technically this should not include the current users rating for the randomly selected movie...\n",
    "    #that is what we want to find out...\n",
    "    ratings = copy.deepcopy(get_user_ratings(key))\n",
    "    similairities = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "    random_rating = ratings[rand_index]\n",
    "    user_to_rand_rating[key] = random_rating\n",
    "    del ratings[rand_index]\n",
    "\n",
    "    #technically this should not include the current users rating for the randomly selected movie...\n",
    "    #that is what we want to find out...\n",
    "    movie_rating_avg = get_avg_movie_rating(user_to_ratings[key][rand_index][1])\n",
    "\n",
    "    user_rating_avg =  float(np.sum(ratings)/(len(ratings)))\n",
    "    user_rating_skew = skew(ratings)\n",
    "    if(math.isnan(user_rating_skew)):\n",
    "        user_rating_skew = 0\n",
    "    user_rating_kurt = kurtosis(ratings)\n",
    "    if(math.isnan(user_rating_kurt)):\n",
    "        user_rating_kurt = 0\n",
    "    user_rating_var = statistics.variance(ratings)\n",
    "\n",
    "\n",
    "    sim_average = float(np.sum(similairities)/(len(similairities)))\n",
    "    sim_skew = skew(similairities) \n",
    "    if(math.isnan(sim_skew)):\n",
    "        sim_skew = 0\n",
    "    sim_kurt = kurtosis(similairities)\n",
    "    if(math.isnan(sim_kurt)):\n",
    "        sim_kurt = 0\n",
    "    sim_var = statistics.variance(similairities)\n",
    "\n",
    "\n",
    "    # there are many curve defining features used here that may be impotent and can be cut or kept in the next cell...\n",
    "    # there may stil be other distribution measures that improve the model...\n",
    "    # might try inputing some function of sim and rating rather than incluing them on their own\n",
    "\n",
    "\n",
    "    #guthub note: this is the start to a complete model overall\n",
    "\n",
    "    #(LOOK)\n",
    "    #instead of a features having a (rating - user_rating_avg)*sim term which is a week indicator of rating (not sure why)\n",
    "    #before this loop the k-mean algorithm is trained on the entire data set...\n",
    "    #and the elbow method can be used to find the right number of clusters..\n",
    "    #then, inside this loop the rand movie which rating is to be predicted is grouped in a cluster according to k means\n",
    "    #ideally the user has an average rating for movies in the assigned cluster\n",
    "    #if the user does not have a rating for the specified cluster than see if the\n",
    "    #user has a rating for the k-means cluster of k-1 with the clusetr of the random movie reaccessed\n",
    "    #repeat this process until the random movie is part of a cluster or k = 1 meaning it has to be part of that cluster\n",
    "    #this process still assumes that the corpus is a good indicator of rating\n",
    "\n",
    "\n",
    "\n",
    "    #what is the difference between using this and k-means//\n",
    "    #model = AgglomerativeClustering(n_clusters=i, affinity=\"euclidean\", linkage=\"single\")    \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    for sim, rating in zip(similairities, ratings):\n",
    "        if key not in user_to_features:\n",
    "            # user_to_features[key] = [[(rating - user_rating_avg)*sim, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var]]\n",
    "            user_to_features[key] = [[user_rating_avg, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var]]\n",
    "        else:\n",
    "            # user_to_features[key].append([(rating - user_rating_avg)*sim, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var])\n",
    "            # user_to_features[key].append([rating, sim, user_rating_avg, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var])\n",
    "            user_to_features[key].append([user_rating_avg, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31077813 0.40534428 0.01181351 0.0096373  0.04163006 0.02121139\n",
      " 0.25362815 0.17351571 0.02097077]\n",
      "79\n",
      "Pred: 2.133233757067547 Actual: 3.0\n",
      "Pred: 2.3036635415358417 Actual: 3.0\n",
      "Pred: 4.21654929365804 Actual: 3.5\n",
      "Pred: 4.675950415353803 Actual: 3.0\n",
      "Pred: 3.4097745860096516 Actual: 3.5\n",
      "Pred: 3.57676844342984 Actual: 2.0\n",
      "Pred: 3.6534256597567203 Actual: 4.0\n",
      "Pred: 3.4924393768669875 Actual: 4.0\n",
      "Pred: 3.040758547030578 Actual: 2.0\n",
      "Pred: 4.277531782455525 Actual: 5.0\n",
      "Pred: 2.8005161889304944 Actual: 3.5\n",
      "Pred: 3.1056330987460203 Actual: 4.0\n",
      "Pred: 3.5079222090070448 Actual: 3.0\n",
      "Pred: 3.358482877027911 Actual: 5.0\n",
      "Pred: 4.401036192733158 Actual: 4.0\n",
      "Pred: 3.629638246502501 Actual: 3.5\n",
      "Pred: 3.2357149625902966 Actual: 3.0\n",
      "Pred: 4.415845930901873 Actual: 5.0\n",
      "Pred: 4.670628232527948 Actual: 3.5\n",
      "Pred: 3.9594427405137305 Actual: 4.0\n",
      "Pred: 5.0762740693850175 Actual: 3.0\n",
      "Pred: 4.270727213138118 Actual: 4.0\n",
      "Pred: 4.245589532217444 Actual: 4.0\n",
      "Pred: 4.002398532907798 Actual: 4.0\n",
      "Pred: 3.8065514546362844 Actual: 4.0\n",
      "Pred: 2.9073422682599146 Actual: 5.0\n",
      "Pred: 4.956555886017082 Actual: 5.0\n",
      "Pred: 4.25802611770387 Actual: 3.0\n",
      "Pred: 2.8856482653488933 Actual: 4.0\n",
      "Pred: 5.1522216265621275 Actual: 4.5\n",
      "Pred: 3.5659160690125593 Actual: 4.0\n",
      "Pred: 3.546287229454918 Actual: 5.0\n",
      "Pred: 1.9265147172764296 Actual: 2.0\n",
      "Pred: 4.483471057956988 Actual: 5.0\n",
      "Pred: 4.238578627113808 Actual: 3.0\n",
      "Pred: 3.360777321138413 Actual: 5.0\n",
      "Pred: 5.853119499623467 Actual: 2.5\n",
      "Pred: 2.737035404992215 Actual: 2.5\n",
      "Pred: 3.259898905065135 Actual: 4.0\n",
      "Pred: 3.5548509797634367 Actual: 4.0\n",
      "Pred: 3.6270073783392713 Actual: 3.0\n",
      "Pred: 4.076669726076153 Actual: 4.5\n",
      "Pred: 4.7624825966548405 Actual: 5.0\n",
      "Pred: 4.763960074570129 Actual: 4.0\n",
      "Pred: 2.9357016356750854 Actual: 3.5\n",
      "Pred: 3.425814302549583 Actual: 3.0\n",
      "Pred: 2.9198547426647394 Actual: 3.0\n",
      "Pred: 3.375256615956258 Actual: 3.5\n",
      "Pred: 4.502176246537206 Actual: 4.5\n",
      "Pred: 3.356558263792587 Actual: 2.5\n",
      "Pred: 5.629115291137027 Actual: 4.5\n",
      "Pred: 4.928105869874913 Actual: 3.0\n",
      "Pred: 4.9189599773593224 Actual: 5.0\n",
      "Pred: 2.151186825745074 Actual: 2.0\n",
      "Pred: 5.830233137353469 Actual: 4.0\n",
      "Pred: 3.22478020323035 Actual: 4.0\n",
      "Pred: 3.5177434922104793 Actual: 4.0\n",
      "Pred: 3.6205920308362867 Actual: 4.0\n",
      "Pred: 4.597130400600293 Actual: 4.0\n",
      "Pred: 3.3244533486562777 Actual: 3.0\n",
      "Pred: 5.3155376978999325 Actual: 4.0\n",
      "Pred: 3.3315416476933426 Actual: 2.0\n",
      "Pred: 4.39140080479203 Actual: 3.0\n",
      "overall score: -0.35513461077926944\n",
      "Minutes: 0.06348371505737305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "#test the time taken to train and predict\n",
    "start = time.time()\n",
    "\n",
    "#this is where you may select certain features to be used to build the model\n",
    "new_user_to_features = dict()\n",
    "\n",
    "for key in user_to_features.keys():\n",
    "    new_list = []\n",
    "    for item in user_to_features[key]:\n",
    "        # can try reducing the features like below:\n",
    "        # old inputs...\n",
    "        # item[0:4]+ item[6:7]+ item[10:11]\n",
    "        # item[0:4]\n",
    "        # item[2:8] + item[8:]\n",
    "        #new inputs:\n",
    "        \n",
    "        new_list.append(item)\n",
    "    new_user_to_features[key] = new_list\n",
    "\n",
    "#seed\n",
    "# seed_int = 1\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#instead of using test train split...\n",
    "user_to_X_train = dict()\n",
    "user_to_y_train = dict()\n",
    "user_to_X_test = dict()\n",
    "user_to_y_test = dict()\n",
    "\n",
    "#There is a problem with using the same users in training and testing and this code ensures that it doesn't happen\n",
    "#The model should beable to be used effectively for new users and not just memorized for existing users\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "for key in new_user_to_features.keys():\n",
    "    if(random.randint(0,10) == 0):\n",
    "        user_to_X_test[key] = new_user_to_features[key]\n",
    "        user_to_y_test[key] = user_to_rand_rating[key]\n",
    "        c1+=1\n",
    "\n",
    "    else:\n",
    "        user_to_X_train[key] = new_user_to_features[key]\n",
    "        user_to_y_train[key] = user_to_rand_rating[key]\n",
    "        c2+=1\n",
    "\n",
    "#used to train model\n",
    "X_train = [] \n",
    "y_train = []\n",
    "\n",
    "#populate X_train and y_train\n",
    "for key in user_to_X_train.keys():\n",
    "    for item in user_to_X_train[key]:\n",
    "        X_train.append(item)\n",
    "        y_train.append(user_to_y_train[key])\n",
    "\n",
    "\n",
    "# scale training features...\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "\n",
    "#data transformation\n",
    "#https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n",
    "\n",
    "\n",
    "#train model\n",
    "#orginal model layers\n",
    "#layers = (2,2,2)\n",
    "layers = (2,2,2)\n",
    "# act = \"tanh\"\n",
    "# solve = \"adam\"\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# act = \"relu\"\n",
    "# solve = \"sgd\"\n",
    "# act = \"tanh\"\n",
    "# solve = \"sgd\"\n",
    "act = \"relu\"\n",
    "solve = \"adam\"\n",
    "\n",
    "#note: random_state=seed_int removed\n",
    "regr = MLPRegressor(hidden_layer_sizes=layers,activation =act, solver =solve,  max_iter=10000)\n",
    "fitted = regr.fit(X_train, y_train)\n",
    "\n",
    "#this needs to run before the final model is determined so that the best features are used\n",
    "#the results can also be displayed with a bar shart showing how each feature cotributes to a percentage of the models accuracy\n",
    "\n",
    "#note: random_state=seed_int removed\n",
    "result = permutation_importance(fitted, X_train, y_train)\n",
    "\n",
    "print(result[\"importances_mean\"])\n",
    "\n",
    "print(regr.n_iter_)\n",
    "\n",
    "#dictionary of users to test features that have been scaled\n",
    "new_user_to_X_test = dict()\n",
    "\n",
    "# used to scale test features then the new scaled features are returned ...\n",
    "# as the values of the approriate user key in new_user_to_X_test \n",
    "X_test = []\n",
    "\n",
    "#populate X_test, key, and counts that are later used to build new_user_to_X_test, a verison of...\n",
    "#user_to_X_test with scaled features \n",
    "#need to decompose then recompose\n",
    "keys = []\n",
    "counts = []\n",
    "for key in user_to_X_test.keys():\n",
    "    cnt = 0\n",
    "    for item in user_to_X_test[key]:\n",
    "        X_test.append(item)\n",
    "        cnt+=1\n",
    "    counts.append(cnt)\n",
    "    keys.append(key)\n",
    "\n",
    "#scale test features...\n",
    "scalar = StandardScaler()\n",
    "X_test = scalar.fit_transform(X_test)\n",
    "\n",
    "#populate new_user_to_X_test with scaled test features\n",
    "cnt = 0\n",
    "for num, key in zip(counts, keys):\n",
    "    new_user_to_X_test[key] = []\n",
    "    for i in range(num):\n",
    "        new_user_to_X_test[key].append(X_test[cnt])\n",
    "        cnt+=1\n",
    "\n",
    "\n",
    "# user id to the average predicted rating for the randomly chosen movie\n",
    "user_to_avg_rating = dict()\n",
    "\n",
    "# populate user_to_avg_rating by averaging the predictions from all the feature inputs of the...\n",
    "# movies a user has watched that are not the randomly chosen movie itself\n",
    "for key in new_user_to_X_test.keys():\n",
    "    sum =0\n",
    "    cnt =0 \n",
    "    predicted = regr.predict(new_user_to_X_test[key])\n",
    "    for item in predicted:\n",
    "        sum+=item\n",
    "        cnt+=1\n",
    "    user_to_avg_rating[key] = float(sum/cnt)\n",
    "\n",
    "\n",
    "#outputs\n",
    "actuals_list = []\n",
    "preds_list = []\n",
    "for key in user_to_avg_rating.keys():\n",
    "    print(\"Pred: \"+str(user_to_avg_rating[key]) , \"Actual: \"+str(user_to_y_test[key]))\n",
    "    actuals_list.append(user_to_y_test[key])\n",
    "    preds_list.append(user_to_avg_rating[key])\n",
    "    \n",
    "print(\"overall score:\", r2_score(actuals_list, preds_list))\n",
    "\n",
    "#test the time taken to train and predict\n",
    "end = time.time()\n",
    "print(\"Minutes:\", float((end - start)/60))\n",
    "\n",
    "\n",
    "#feature importance scores:\n",
    "#https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance\n",
    "\n",
    "#introduction:\n",
    "#https://www.kaggle.com/code/dansbecker/permutation-importance\n",
    "\n",
    "#types of feature importance:\n",
    "#https://towardsdatascience.com/6-types-of-feature-importance-any-data-scientist-should-master-1bfd566f21c9\n",
    "\n",
    "\n",
    "#perhaps there is a way to visualize this of the model outputs below in a systematic way???\n",
    "\n",
    "# Tests:\n",
    "\n",
    "# full features:\n",
    "# with linear regression:\n",
    "# overall score: 0.2657455495660592\n",
    "\n",
    "# with mlp...:\n",
    "\n",
    "# first fours features:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2667063296881431\n",
    "\n",
    "#all features: \n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.26606490897808244\n",
    "\n",
    "#all features: \n",
    "# layers: (2,2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.260461734737799\n",
    "\n",
    "#all features: \n",
    "# layers: (4,4,4)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.22932745175064528\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2482634902547255\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (2,2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2616102684471122\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (3,3,3)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.25487207187202243\n",
    "\n",
    "#first two featurs:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: -0.00430015574935827\n",
    "\n",
    "#first two featurs:\n",
    "# layers: (2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.04468421358737418\n",
    "\n",
    "#3rd and 4th features:\n",
    "# layers: (2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2546480453878024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
