{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a simple model at the moment:\n",
    "#the sum1 value is fair but somewhat arbitrary\n",
    "#it should take into account the variance of the users rating data and the variance of similarity data\n",
    "\n",
    "#idea: visualize data\n",
    "\n",
    "#the simalirity scores of movies to the movie that the user rated has a distribution\n",
    "#the users ratings to the other movies also has a distribution\n",
    "\n",
    "#simple model:\n",
    "#sum all the (similairity*ratings) pairs\n",
    "#then divide by the number of pairs\n",
    "\n",
    "\n",
    "#advanced model:\n",
    "\n",
    "#apply variance to make very high similiarty scores weigh much more\n",
    "#and the low similairty scores weigh much less\n",
    "#The abs min is 0 and the abs max is 1\n",
    "#https://numpy.org/doc/stable/reference/generated/numpy.var.html\n",
    "#https://numpy.org/doc/stable/reference/generated/numpy.std.html\n",
    "\n",
    "#compare heights of the distribution if its normal:\n",
    "#https://guides.fscj.edu/Statistics/standardnormal#:~:text=The%20Normal%20Curve,and%20width%20of%20the%20graph.\n",
    "\n",
    "#if you divide two hieghts you get how much more likely is one verse the other\n",
    "\n",
    "#divide each similarity point by the sum of all items of the previous step\n",
    "#now the entire similairty distribution sums to one \n",
    "\n",
    "\n",
    "#note: should also try clustering\n",
    "\n",
    "\n",
    "#fitter\n",
    "#https://medium.com/the-researchers-guide/finding-the-best-distribution-that-fits-your-data-using-pythons-fitter-library-319a5a0972e9\n",
    "\n",
    "# plt.hist(ratings)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pick a single point in the test training set to predict its rating using the cos siliarty to it from all the different data points \n",
    "#in the train Data set\n",
    "\n",
    "# cv = CountVectorizer() \n",
    "\n",
    "# count_matrix = cv.fit_transform(moviesFull[\"combined_features\"])\n",
    "\n",
    "# cosine_sim = cosine_similarity(count_matrix)\n",
    "\n",
    "\n",
    "# #copied code for vectorization\n",
    "# #used to find simlair movies to avatar\n",
    "# features = ['keywords','cast','genres','director']\n",
    "\n",
    "# def combine_features(row):\n",
    "#     return row['keywords']+\" \"+row['cast']+\" \"+row['genres']+\" \"+row['director']\n",
    "\n",
    "# for feature in features:\n",
    "#     moviesFull[feature] = moviesFull[feature].fillna('')\n",
    "\n",
    "# moviesFull[\"combined_features\"] = moviesFull.apply(combine_features,axis=1)\n",
    "\n",
    "# cv = CountVectorizer() \n",
    "\n",
    "# count_matrix = cv.fit_transform(moviesFull[\"combined_features\"])\n",
    "\n",
    "# cosine_sim = cosine_similarity(count_matrix)\n",
    "\n",
    "# def get_title_from_index(index):\n",
    "#     return moviesFull[moviesFull.index == index][\"title\"].values[0]\n",
    "# def get_index_from_title(title):\n",
    "#     return moviesFull[moviesFull.title == title][\"index\"].values[0]\n",
    "\n",
    "# movie_user_likes = \"Avatar\"\n",
    "# movie_index = get_index_from_title(movie_user_likes)\n",
    "\n",
    "# similar_movies = list(enumerate(cosine_sim[movie_index]))\n",
    "\n",
    "# sorted_similar_movies = sorted(similar_movies,key=lambda x:x[1], reverse=True)[1:6]\n",
    "\n",
    "#print(sorted_similar_movies)\n",
    "\n",
    "#end of copied code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#remember that all movies being chosen to add with their rating must\n",
    "#exist in both movie Half and moviesFull datasets\n",
    "#they should be matched by name since the ids are different \n",
    "#the inputs of both the collaborative anaylis as well as content based analysis\n",
    "#will be used in the regresion model\n",
    "\n",
    "#the inputs of both the collaborative analysis as well as conten based analysis\n",
    "#will be used in the regresion model\n",
    "\n",
    "#how do we get the training dataset for this?\n",
    "\n",
    "#for each user:\n",
    "#take a random selection of half the users movie ratings and use the other half in the...\n",
    "#content based analysis and collaborative analysis  \n",
    "#generate the expected ratings for the content based analysis and collaborative using one half to predict\n",
    "#the ratings for the other half\n",
    "#furthermore use linear regression with the content based and the collaborative based output as inputs\n",
    "#use gradient descent to update the linear model\n",
    "\n",
    "#for a user:\n",
    "#since the content based analysis is simply comparing movies together for how simlair they are...\n",
    "#for each movie in the first half(unknown rating)...\n",
    "#there will be a similarity score for each item of the second half of the movies (known rating)\n",
    "#each movie in the second half will test its simlairity for an item in the first half...\n",
    "#and will contribute to that items predicted rating based on its similairty score and rating\n",
    "\n",
    "#how will it contribute?:\n",
    "#example:\n",
    "\n",
    "#three sim scores: 1,1,1\n",
    "#three corresponding ratings: 2,3,4\n",
    "\n",
    "#(1*(2-2.5)+1*(3-2.5)+1*(4-2.5))/3 = (-.5+.5+1.5)/3 = .5 + 2.5 = 3\n",
    "\n",
    "#three sim scores: .8,.75,.9\n",
    "#three corresponding ratings: 2,3,4\n",
    "\n",
    "#(.8*(2-2.5)+.75*(3-2.5) +.9*(4-2.5))/3 = (-.4+.375+1.35)/3 + 2.5 = .44167 + 2.5 = 2.94167\n",
    "\n",
    "#note: cosine similairity is betwen (-1,1) if not simly map from (0,1) it before calling\n",
    "\n",
    "#note: average user ratin may be important to incorporate"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
