{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data source\n",
    "\n",
    "* Download the data needed for this jupyter notebook from kaggle and store it in a new folder (the-movies-dataset) in the current directory.\n",
    "\n",
    "\n",
    "* Upon running this cell, the user will be asked for their username and key which can be found in a fresh api token from kaggle.\n",
    "\n",
    "* Instructions to get api token to authenticate the data request (Note: kaggle account required):\n",
    "    1. Sign into kaggle.\n",
    "    2. Go to the 'Account' tab of your user profile and select 'Create New Token'. \n",
    "    3. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
    "\n",
    "* If the folder has been created and the files are already in that folder, than this cell does nothing and requires no credentials.\n",
    "\n",
    "* Data Source Information: https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Raw Data\n",
    "\n",
    "Combining certain data from the necessary csv files into a single dataframe (complete_df).\n",
    "\n",
    "* Rows are removed from each dataframe when they do not have sufficient data for a column or the data from a column does not exist.\n",
    "* This kind of row removal is done before multiple copies of the same movie data becomes present in multiple rows, to save time and space.\n",
    "* Iteration through rows of a dataframe at this level is inefficient compared to list iteration.\n",
    "* This is why the dataframes are converted into lists before iteration and then back again to dataframes, so the merge function can be applied to combine the data into a single dataframe (complete_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minutes taken: 0.6067105770111084\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_df = pd.read_csv('./the-movies-dataset/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={'genres':\"string\",\"id\":\"string\",\"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\",\n",
    "                                    \"production_companies\" :\"string\"})[[\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"]]\n",
    "movies_df.dropna(inplace = True)\n",
    "movies_lst = [row for row in movies_df.values.tolist() if not (row[0][len(row[0])  - 2:] == \"[]\" or row[5][len(row[5]) - 2:] == \"[]\")]\n",
    "movies_df = pd.DataFrame(movies_lst, columns = (\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"), dtype = str)\n",
    "\n",
    "\n",
    "\n",
    "ratings_df = pd.read_csv('./the-movies-dataset/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"),\n",
    "                       dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})[[\"userId\", \"movieId\", \"rating\"]]\n",
    "ratings_df.rename(columns={\"movieId\": \"id\"}, inplace = True)\n",
    "ratings_df.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Question: What if the removal of duplicate movie ids per user was processed here instead of the cell below???\n",
    "# Answer: The duplicate removal function can be ran here,...\n",
    "# but the complete_list in the cell below can also be iterated over with relative complexity in order to remove duplicates.\n",
    "# The iteration in the next cell also populates the gap list...\n",
    "# which is critical to be ran directly before the function that determines bounds for users rated movies.\n",
    "# So, omitting the no duplicate function in this cell and making it run in the next cell avoids redundant iteration.\n",
    "\n",
    "\n",
    "# Question: What if the test and train ratings bounds was enforced here instead of the cell below???\n",
    "# Answer: The merge functions below needs to be executed before determining test and train users, because merge will remove rows and ratings from users...\n",
    "# before enforcing the users to be in a certain bounds for the number of their ratings. \n",
    "# The current timing of this function will ensure that the final users are within the set train or test bounds.\n",
    "\n",
    "\n",
    "keywords_df = pd.read_csv('./the-movies-dataset/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})[[\"id\", \"keywords\"]]\n",
    "keywords_df.dropna(inplace = True)\n",
    "keywords_lst = [row for row in keywords_df.values.tolist() if not (row[1][len(row[1])  - 2:] == \"[]\")]\n",
    "keywords_df = pd.DataFrame(keywords_lst, columns = (\"id\", \"keywords\"), dtype = str)\n",
    "\n",
    "\n",
    "credits_df = pd.read_csv(\"./the-movies-dataset/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})[[\"cast\", \"id\"]]\n",
    "credits_df.dropna(inplace = True)\n",
    "credits_lst = [row for row in credits_df.values.tolist() if (not row[0][len(row[0])  - 2:] == \"[]\")]\n",
    "credits_df = pd.DataFrame(credits_lst, columns = (\"cast\", \"id\"), dtype = str)\n",
    "\n",
    "\n",
    "# Default merge is inner: This only keeps movies that have the id existing in both dataframes.\n",
    "complete_df =  pd.merge(movies_df, ratings_df, on =\"id\")\n",
    "complete_df =  pd.merge(complete_df,keywords_df, on =\"id\")\n",
    "complete_df  = pd.merge(complete_df,credits_df, on =\"id\")\n",
    "\n",
    "\n",
    "complete_df.sort_values(by = 'userId', inplace = True)\n",
    "\n",
    "\n",
    "# Master dataframe: For each (user id, movie id) row combination there is the combined movie data from movies_df, ratings_df, keywords_df, and credits_df for the movie id in question.\n",
    "# The columns are reordered.\n",
    "complete_df  = complete_df.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "# For testing:\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "\n",
    "# Notice: With the movies, keywords, and credits dataframes, list conversion happens before dropping empty entries\n",
    "# Tested on personal machine:\n",
    "# tested without list conversion (old code): 1 minute and 5.7 seconds\n",
    "# tested with list conversion (current code): 37.1 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Selection and Data Extraction\n",
    "\n",
    "1. Remove duplicate movies rated by the same user\n",
    "2. Randomly choose users that fall into the appropriate bounds for the number of ratings to be a svd user, train user, or test user\n",
    "3. Extract the data from those users and structure it into a list to be written too a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete number of users: 260788\n",
      "nof svd users met\n",
      "nof train users met\n",
      "nof test users met\n",
      "Minutes taken: 6.106799916426341\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import time\n",
    "from numpy.random import Generator, PCG64\n",
    "\n",
    "def populate_names(item):\n",
    "    \"\"\"Extract names from the syntax of certain data entries:\"\"\"\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    index = 0\n",
    "    for item in jsons:\n",
    "        if(index == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        index += 1\n",
    "    return names\n",
    "\n",
    "\n",
    "def provide_data(row):\n",
    "    \"\"\"Extract data from row of complete_list:\"\"\"\n",
    "    movie_data = []\n",
    "    movie_data.append(int(row[0]))\n",
    "    movie_data.append(int(row[1]))\n",
    "    movie_data.append(float(row[2]))\n",
    "    movie_data.append(row[3])  \n",
    "\n",
    "    movie_data.append(populate_names(row[4]))\n",
    "    movie_data.append(populate_names(row[5]))\n",
    "    movie_data.append(populate_names(row[6]))\n",
    "    movie_data.append(populate_names(row[7]))\n",
    "\n",
    "    movie_data.append(str(row[8]))\n",
    "    movie_data.append(str(row[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "# main:\n",
    "start_time = time.time()\n",
    "\n",
    "SEED_INT = 42\n",
    "outer_gen = Generator(PCG64(SEED_INT))\n",
    "# The list of rows with users id, the users rating for the movie, and metadata for the movie:\n",
    "# Note: It is sorted by user_id.\n",
    "complete_list = complete_df.values.tolist()\n",
    "\n",
    "print(\"Complete number of users:\", len(list(complete_df[\"userId\"].unique()))) # 260788\n",
    "\n",
    "# The same as complete_list where data is omitted for movies that have already been rated by the user in a previous row\n",
    "complete_list_no_dups = []\n",
    "\n",
    "# Distinguish the user the row belongs to:\n",
    "last_id = complete_list[0][0]\n",
    "\n",
    "# The set of movies that a user has rated:\n",
    "# It is used to omit later ratings of a movie that the user has already rated.\n",
    "movie_set = set()\n",
    "\n",
    "# The number of rows of movie data a single user takes up for each user:\n",
    "gaps = []\n",
    "\n",
    "# Appended to gaps when all of a users rows of movie data have been counted:\n",
    "gap_len = 0\n",
    "\n",
    "\n",
    "# Populates gaps and complete_list_no_dups by omitting movies that already have a rating in respect to each user:\n",
    "# Note: This code is faster than using dataframe methods.\n",
    "# Example: Filter data by user and then remove duplicate movie ids for each user.\n",
    "# This avoids slow dataframe iteration, but the filter method is also slow.\n",
    "for row in complete_list:\n",
    "    if last_id != row[0]:\n",
    "        movie_set= set()\n",
    "        complete_list_no_dups.append(row)\n",
    "        movie_set.add(row[1])\n",
    "        gaps.append(gap_len)\n",
    "        gap_len = 1\n",
    "    else:\n",
    "        if row[1] not in movie_set:\n",
    "            complete_list_no_dups.append(row)\n",
    "            gap_len+=1\n",
    "            movie_set.add(row[1])\n",
    "    last_id = row[0]\n",
    "\n",
    "# Add the last gap_len:\n",
    "gaps.append(gap_len)\n",
    "\n",
    "\n",
    "\n",
    "# Bounds represents the first index and last index(non inclusive) of the range of ratings for a user in the sorted complete_list_no_dups\n",
    "full_index = 0 \n",
    "bounds = [] \n",
    "\n",
    "for user_index in range(len(gaps)):\n",
    "    bounds.append([full_index, full_index+gaps[user_index]])\n",
    "    full_index+=gaps[user_index]    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# These set the rating requirements for svd, train, and test users:\n",
    "SVD_USER_RATING_LB = 20\n",
    "SVD_USER_RATING_UB = 30\n",
    "USER_RATING_LB = 5\n",
    "USER_RATING_UB = 10\n",
    "\n",
    "# Makes selection of user bounds random:\n",
    "outer_gen.shuffle(bounds)\n",
    "\n",
    "NOF_SVD_USERS = 10000\n",
    "NOF_TRAIN_USERS = 10000\n",
    "NOF_TEST_USERS = 10000\n",
    "\n",
    "\n",
    "last_index = -1\n",
    "bounds_svd_users = []\n",
    "bounds_train_users = []\n",
    "bounds_test_users = []\n",
    "\n",
    "\n",
    "index = 0\n",
    "for item in bounds:\n",
    "    if item[1]-item[0] >=SVD_USER_RATING_LB and item[1]-item[0] <=SVD_USER_RATING_UB:\n",
    "        bounds_svd_users.append(item)\n",
    "        if len(bounds_svd_users) == NOF_SVD_USERS:\n",
    "            last_index = index\n",
    "            print(\"nof svd users met\")\n",
    "            break\n",
    "    index+=1\n",
    "\n",
    "\n",
    "\n",
    "index+=1\n",
    "for item in bounds[last_index:]:\n",
    "    if item[1]-item[0] >=USER_RATING_LB and item[1]-item[0] <=USER_RATING_UB:\n",
    "        bounds_train_users.append(item)\n",
    "        if len(bounds_train_users) == NOF_TRAIN_USERS:\n",
    "            last_index = index\n",
    "            print(\"nof train users met\")\n",
    "            break\n",
    "    index+=1\n",
    "\n",
    "index+=1\n",
    "for item in bounds[last_index:]:\n",
    "    if item[1]-item[0] >=USER_RATING_LB and item[1]-item[0] <=USER_RATING_UB:\n",
    "        bounds_test_users.append(item)\n",
    "        if len(bounds_test_users) == NOF_TEST_USERS:\n",
    "            print(\"nof test users met\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "# Sample the data from complete_list_no_dups once the bounds (low memory) have been randomly selected:\n",
    "sampled_data = []\n",
    "\n",
    "\n",
    "for bound in bounds_svd_users:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        sampled_data.append(movie_data)\n",
    "\n",
    "\n",
    "for bound in bounds_train_users:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        sampled_data.append(movie_data)\n",
    "\n",
    "\n",
    "\n",
    "for bound in bounds_test_users:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        sampled_data.append(movie_data)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data\n",
    "\n",
    "Save selected data in constructed_data.csv file so that cells below it can run without running this cell and above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "final_directory = os.path.join(current_directory, 'constructed_data')\n",
    "if not os.path.exists(final_directory):\n",
    "   os.makedirs(final_directory)\n",
    "\n",
    "output_path = os.path.join(\"constructed_data\", \"constructed_data.csv\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    writer.writerows(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "This is the starting cell to run if the data is already saved to the constructed_data.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "\n",
    "input_path = os.path.join(\"constructed_data\", \"constructed_data.csv\")\n",
    "\n",
    "with open(input_path, 'r', encoding=\"utf-8\") as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize Data:\n",
    "\n",
    "Extract data from data_list into user_to_data_svd, user_to_data_train, and user_to_data_test.\n",
    "\n",
    "Each of these can be thought of a list of users and each user can be though of a list of movies ratings by the users and the movie data for that movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOF_SVD_USERS = 10000\n",
    "NOF_TRAIN_USERS = 10000\n",
    "NOF_TEST_USERS = 10000\n",
    "\n",
    "\n",
    "user_to_data_svd = []\n",
    "user_to_data_train= []\n",
    "user_to_data_test = []\n",
    "\n",
    "user_id = data_list[0][0]\n",
    "ratings = []\n",
    "user_index = 0\n",
    "\n",
    "\n",
    "\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        if(user_index<NOF_SVD_USERS and user_index>=0):\n",
    "            user_to_data_svd.append(ratings)\n",
    "        elif(user_index<NOF_TRAIN_USERS+NOF_TRAIN_USERS and user_index>=NOF_SVD_USERS):\n",
    "            user_to_data_train.append(ratings)\n",
    "        else:\n",
    "            user_to_data_test.append(ratings)         \n",
    "        user_id = row[0]\n",
    "        ratings = [row]\n",
    "        user_index+=1\n",
    "    else:\n",
    "        ratings.append(row)\n",
    "\n",
    "\n",
    "\n",
    "user_to_data_test.append(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n",
    "* The following cell tries to find the best set of hyperparameters within the given bounds for the svd function.\n",
    "* After this process, the SVD function with the best performing hyperparameters is used in the full_model notebook where the actual model takes place.\n",
    "* Since the bounds are not all inclusive and the iterations of the gp.minimize function is limited, there is potential for improvement if the time was taken to run a deeper search.\n",
    "* A very high number of iterations are ran with mixed data to test each set of hyperparameters before evaluation, to reduce the factor of noise attributing to the performance of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\distributed\\client.py:3162: UserWarning: Sending large graph of size 234.19 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "d:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\distributed\\client.py:3162: UserWarning: Sending large graph of size 216.89 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "d:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\distributed\\client.py:3162: UserWarning: Sending large graph of size 228.30 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: x [311, 183, 173, 394, 0.015838367452944445, 0.020440093655358895]\n",
      "Result: y 1.0476970911026\n",
      "Minutes taken: 2.2690266489982607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from numba import njit\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy.random import Generator, PCG64\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from skopt.space import Integer\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.array as da\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@njit\n",
    "def epoch(list, b1, b2, p, q, overall_average, lr, rt):\n",
    "    \"\"\"\n",
    "    Update the parameters (b1, b2, q, and p) for each row in the list using stochastic gradient descent.\n",
    "    \"\"\"\n",
    "    for row in list:\n",
    "        u = int(row[0])\n",
    "        i = int(row[1])\n",
    "        r = row[2]\n",
    "\n",
    "        pred = overall_average+b1[u]+b2[i]+np.dot(p[u],q[i])\n",
    "        error = r-pred\n",
    "        b1[u] += lr*(error- rt*b1[u])\n",
    "        b2[i] += lr*(error- rt*b2[i])\n",
    "        temp = lr*(error*q[i] -rt*p[u])\n",
    "        q[i] += lr*(error*p[u] -rt*q[i])\n",
    "        p[u] += temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def svd_iterative(gen_input, list, n, epochs, rt, lr, overall_average, nof_users, nof_movies):\n",
    "    \"\"\"\n",
    "    An iterative SVD method that has been shown to out perform non-iterative svd methods\n",
    "    \"\"\"\n",
    "    \n",
    "    q = gen_input.normal(0, .1, (nof_movies, n))\n",
    "    p = gen_input.normal(0, .1, (nof_users, n))\n",
    "\n",
    "\n",
    "    b1 = np.zeros(nof_users)\n",
    "    b2 = np.zeros(nof_movies)\n",
    "\n",
    "    np_array = np.array(list)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        epoch(np_array, b1, b2, p, q, overall_average, lr, rt)\n",
    "\n",
    "    return b1, b2, p, q\n",
    "\n",
    "\n",
    "def rmse_sum(block):\n",
    "    \"\"\"\n",
    "    This function computes the rmse for each row in the block then returns the sum of them.\n",
    "    \"\"\"\n",
    "    total_sum = 0 \n",
    "\n",
    "    for row in block:\n",
    "        seed_input, user_to_data_svd_temp, user_to_data_test_temp, nof_latent_features, epochs, rt, lr = row\n",
    "\n",
    "        gen = Generator(PCG64(seed_input))\n",
    "\n",
    "\n",
    "        # re-index the user ids and the movie ids in the order of their occurrence:\n",
    "        old_to_new_svd  = dict()\n",
    "        last_index_svd = 0\n",
    "        svd_cnt = 0\n",
    "\n",
    "        for user in user_to_data_svd_temp:\n",
    "            for movie in user: \n",
    "                if(movie[1] in old_to_new_svd.keys()):\n",
    "                    movie[1] = old_to_new_svd[movie[1]]\n",
    "                else:\n",
    "                    old_to_new_svd[movie[1]] = last_index_svd\n",
    "                    movie[1] = last_index_svd\n",
    "                    last_index_svd+=1      \n",
    "                movie[0] = svd_cnt\n",
    "            svd_cnt+=1\n",
    "\n",
    "        old_to_new_test = copy.deepcopy(old_to_new_svd)\n",
    "        last_index_test = last_index_svd\n",
    "        test_cnt = svd_cnt\n",
    "\n",
    "        for user in user_to_data_test_temp:\n",
    "            for movie in user: \n",
    "                if(movie[1] in old_to_new_test.keys()):\n",
    "                    movie[1] = old_to_new_test[movie[1]]\n",
    "                else:\n",
    "                    old_to_new_test[movie[1]] = last_index_test\n",
    "                    movie[1] = last_index_test\n",
    "                    last_index_test+=1      \n",
    "                movie[0] = test_cnt\n",
    "            test_cnt+=1\n",
    "\n",
    "\n",
    "        # Populate the variables that are needed for the svd method to make predictions:\n",
    "        target_rating_test = []\n",
    "        test_list = []\n",
    "\n",
    "        movies_order_svd = set()\n",
    "        overall_average_svd = 0 \n",
    "        cnt_svd = 0\n",
    "\n",
    "        for user in user_to_data_svd_temp:\n",
    "            for movie in user:\n",
    "                movies_order_svd.add(movie[1])\n",
    "                test_list.append([int(movie[0]), int(movie[1]), float(movie[2])])\n",
    "                overall_average_svd+=float(movie[2])\n",
    "                cnt_svd += 1\n",
    "\n",
    "        movies_order_test = copy.deepcopy(movies_order_svd)\n",
    "        overall_average_test = overall_average_svd \n",
    "        cnt_test = cnt_svd\n",
    "        test_rating_to_predict = []\n",
    "\n",
    "        for user in user_to_data_test_temp:\n",
    "            rand_num  = gen.integers(0, len(user))\n",
    "            index = 0\n",
    "            for movie in user:\n",
    "                movies_order_test.add(movie[1])\n",
    "                if(index == rand_num):\n",
    "                    test_rating_to_predict.append([int(movie[0]), int(movie[1])])\n",
    "                    target_rating_test.append(float(movie[2]))\n",
    "                else:\n",
    "                    overall_average_test+=float(movie[2])\n",
    "                    cnt_test += 1\n",
    "                    test_list.append([int(movie[0]), int(movie[1]), float(movie[2])])\n",
    "                index+=1\n",
    "\n",
    "        overall_average_test = overall_average_test/cnt_test\n",
    "\n",
    "        gen.shuffle(test_list)\n",
    "\n",
    "\n",
    "        # Make predictions with the svd method and add the rmse to total_sum\n",
    "        b1, b2, p, q = svd_iterative(gen, test_list, nof_latent_features, epochs, rt, lr,\n",
    "                                    overall_average_test, len(user_to_data_svd_temp)+len(user_to_data_test_temp), len(movies_order_test))\n",
    "\n",
    "        prediction = [overall_average_test + b1[pair[0]]+b2[pair[1]]\n",
    "                                    +np.dot(p[pair[0]],q[pair[1]]) for pair in test_rating_to_predict]\n",
    "    \n",
    "        total_sum+=mean_squared_error(target_rating_test, prediction, squared = False)\n",
    "\n",
    "    return (np.array([[total_sum]], dtype=\"float32\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective_function(vars):\n",
    "    \"\"\"\n",
    "    This function is responsible for testing the optimization hyperparameters...\n",
    "    for many iterations on a great variety of users to reduce the noise of the rmse metric.\n",
    "    It makes sure that the best hyperparameter are not just the best by chance but by raw effectiveness.\n",
    "    \"\"\"\n",
    "    nof_svd_users, nof_test_users,nof_latent_features, epochs, rt, lr = vars\n",
    "\n",
    "    mse_sum = 0\n",
    "\n",
    "    ITERATIONS = 320\n",
    "\n",
    "    parameters_list = []\n",
    "\n",
    "    # Note: \n",
    "    # This loop has a high cost because it makes a new choice of users every iteration.\n",
    "    # However, if the choice of users was repeated for a number of iterations before switching then there would be more noise.\n",
    "    # The more variety of users the less noise. Also, the cost is relatively low compared to the rest of the operations.\n",
    "\n",
    "    # Note:\n",
    "    # user_to_data_svd_copy and user_to_data_train_copy are converted from list to numpy array of objects, then sampled, and then convert back to a list. \n",
    "    # This is to avoid using more than one random type and suppresses the warning about arrays being a ragged nested sequences. \n",
    "\n",
    "    for _ in range(ITERATIONS):\n",
    "        parameters_list.append([outer_gen.integers(0,100000),\n",
    "                                list(copy.deepcopy(outer_gen.choice(np.array(user_to_data_svd, dtype='object'), nof_svd_users, replace = False))),\n",
    "                                list(copy.deepcopy(outer_gen.choice(np.array(user_to_data_test, dtype = \"object\"), nof_test_users, replace = False))),\n",
    "                                nof_latent_features, epochs, rt, lr])\n",
    "\n",
    "\n",
    "    parameters_arr = np.array(parameters_list, dtype=\"object\")\n",
    "    dask_array = da.from_array(parameters_arr, chunks=(int(ITERATIONS/8),7))\n",
    "    results = dask_array.map_blocks(rmse_sum, chunks = (1,1), dtype=\"float32\").compute()\n",
    "\n",
    "\n",
    "    for block in results:\n",
    "        mse_sum+= block[0]\n",
    "\n",
    "    return mse_sum/ITERATIONS\n",
    "\n",
    "# main:\n",
    "start = time.time()\n",
    "\n",
    "cluster = LocalCluster(n_workers=os.cpu_count())\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "SEED_INT = 5\n",
    "outer_gen = Generator(PCG64(SEED_INT))\n",
    "\n",
    "mid_points = [(300, 500),(100, 200),(100,300),(100,400),(.01, .075),(.001, .05)]\n",
    "\n",
    "# Note: This works because the midpoints between the integer bounds are integers.\n",
    "mid_points = [(lambda pair : int((pair[0]+pair[1])/2) if((pair[0]+pair[1])/2==int((pair[0]+pair[1])/2)) else (pair[0]+pair[1])/2)(item) for item in mid_points]\n",
    "\n",
    "\n",
    "bounds = [Integer(300, 500, name = 'nof_svd_users'),Integer(100, 200, name = 'nof_test_users'),\n",
    "          Integer(100,300, name = 'nof_latent_features'),Integer(100,400, name = 'epochs'),\n",
    "          Real(.01, .075, name = 'rt'),Real(.001, .05, name = 'lr')]\n",
    "\n",
    "\n",
    "res = gp_minimize(objective_function,                 \n",
    "                  bounds,      \n",
    "                  n_calls=30,    \n",
    "                  n_initial_points = 10, \n",
    "                  x0 = mid_points,    \n",
    "                  random_state= SEED_INT,\n",
    "                  n_points = 10000,\n",
    "                  )\n",
    "\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "\n",
    "print(\"Solution: x\", res.x)\n",
    "print(\"Result: y\", res.fun)\n",
    "print(\"Minutes taken:\", (time.time()-start)/60)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test hyperparameters\n",
    "* The following cell is an method to test the optimized hyperparameters found in the cell above.\n",
    "* These tests are used to make sure the hyperparameters are good at generalizing new data.\n",
    "\n",
    "* Worse performance on average in the output of this cell when using the same optimized hyperparameters show there was some sensitivity to noise that contributed to the selection of those hyperparameters. In other words, some of the performance was simply due to chance.\n",
    "\n",
    "* To mitigate this noise, a large number of tests are done in the cell above.\n",
    "* A large number of tests are done in the following cell to reduce the noise of this test itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\distributed\\client.py:3162: UserWarning: Sending large graph of size 504.51 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Parameters rmse score: 1.0430915474891662\n",
      "Time Taken 8.625545132160187\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numba import njit\n",
    "import copy\n",
    "from numpy.random import Generator, PCG64\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.array as da\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "@njit\n",
    "def epoch(list, b1, b2, p, q, overall_average, lr, rt):\n",
    "    \"\"\"\n",
    "    Update the parameters (b1, b2, q, and p) for each row in the list using stochastic gradient descent.\n",
    "    \"\"\"\n",
    "    for row in list:\n",
    "        u = int(row[0])\n",
    "        i = int(row[1])\n",
    "        r = row[2]\n",
    "\n",
    "        pred = overall_average+b1[u]+b2[i]+np.dot(p[u],q[i])\n",
    "        error = r-pred\n",
    "        b1[u] += lr*(error- rt*b1[u])\n",
    "        b2[i] += lr*(error- rt*b2[i])\n",
    "        temp = lr*(error*q[i] -rt*p[u])\n",
    "        q[i] += lr*(error*p[u] -rt*q[i])\n",
    "        p[u] += temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def svd_iterative(gen_input, list, n, epochs, rt, lr, overall_average, nof_users, nof_movies):\n",
    "    \"\"\"\n",
    "    An iterative SVD method that has been shown to out perform non-iterative svd methods\n",
    "    \"\"\"\n",
    "    \n",
    "    q = gen_input.normal(0, .1, (nof_movies, n))\n",
    "    p = gen_input.normal(0, .1, (nof_users, n))\n",
    "\n",
    "\n",
    "    b1 = np.zeros(nof_users)\n",
    "    b2 = np.zeros(nof_movies)\n",
    "\n",
    "    np_array = np.array(list)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        epoch(np_array, b1, b2, p, q, overall_average, lr, rt)\n",
    "\n",
    "    return b1, b2, p, q\n",
    "\n",
    "\n",
    "\n",
    "def rmse_sum(block):\n",
    "    \"\"\"\n",
    "    This function computes the rmse for each row in the block then returns the sum of them.\n",
    "    \"\"\"\n",
    "    total_sum = 0 \n",
    "\n",
    "    for row in block:\n",
    "        seed_input, user_to_data_svd_temp, user_to_data_test_temp, nof_latent_features, epochs, rt, lr = row\n",
    "\n",
    "        gen = Generator(PCG64(seed_input))\n",
    "\n",
    "\n",
    "        # re-index the user ids and the movie ids in the order of their occurrence:\n",
    "        old_to_new_svd  = dict()\n",
    "        last_index_svd = 0\n",
    "        svd_cnt = 0\n",
    "\n",
    "        for user in user_to_data_svd_temp:\n",
    "            for movie in user: \n",
    "                if(movie[1] in old_to_new_svd.keys()):\n",
    "                    movie[1] = old_to_new_svd[movie[1]]\n",
    "                else:\n",
    "                    old_to_new_svd[movie[1]] = last_index_svd\n",
    "                    movie[1] = last_index_svd\n",
    "                    last_index_svd+=1      \n",
    "                movie[0] = svd_cnt\n",
    "            svd_cnt+=1\n",
    "\n",
    "        old_to_new_test = copy.deepcopy(old_to_new_svd)\n",
    "        last_index_test = last_index_svd\n",
    "        test_cnt = svd_cnt\n",
    "\n",
    "        for user in user_to_data_test_temp:\n",
    "            for movie in user: \n",
    "                if(movie[1] in old_to_new_test.keys()):\n",
    "                    movie[1] = old_to_new_test[movie[1]]\n",
    "                else:\n",
    "                    old_to_new_test[movie[1]] = last_index_test\n",
    "                    movie[1] = last_index_test\n",
    "                    last_index_test+=1      \n",
    "                movie[0] = test_cnt\n",
    "            test_cnt+=1\n",
    "\n",
    "\n",
    "        # Populate the variables that are needed for the svd method to make predictions:\n",
    "        target_rating_test = []\n",
    "        test_list = []\n",
    "\n",
    "        movies_order_svd = set()\n",
    "        overall_average_svd = 0 \n",
    "        cnt_svd = 0\n",
    "\n",
    "        for user in user_to_data_svd_temp:\n",
    "            for movie in user:\n",
    "                movies_order_svd.add(movie[1])\n",
    "                test_list.append([int(movie[0]), int(movie[1]), float(movie[2])])\n",
    "                overall_average_svd+=float(movie[2])\n",
    "                cnt_svd += 1\n",
    "\n",
    "        movies_order_test = copy.deepcopy(movies_order_svd)\n",
    "        overall_average_test = overall_average_svd \n",
    "        cnt_test = cnt_svd\n",
    "        test_rating_to_predict = []\n",
    "\n",
    "        for user in user_to_data_test_temp:\n",
    "            rand_num  = gen.integers(0, len(user))\n",
    "            index = 0\n",
    "            for movie in user:\n",
    "                movies_order_test.add(movie[1])\n",
    "                if(index == rand_num):\n",
    "                    test_rating_to_predict.append([int(movie[0]), int(movie[1])])\n",
    "                    target_rating_test.append(float(movie[2]))\n",
    "                else:\n",
    "                    overall_average_test+=float(movie[2])\n",
    "                    cnt_test += 1\n",
    "                    test_list.append([int(movie[0]), int(movie[1]), float(movie[2])])\n",
    "                index+=1\n",
    "\n",
    "        overall_average_test = overall_average_test/cnt_test\n",
    "\n",
    "        gen.shuffle(test_list)\n",
    "\n",
    "\n",
    "        # Make predictions with the svd method and add the rmse to total_sum:\n",
    "        b1, b2, p, q = svd_iterative(gen, test_list, nof_latent_features, epochs, rt, lr,\n",
    "                                    overall_average_test, len(user_to_data_svd_temp)+len(user_to_data_test_temp), len(movies_order_test))\n",
    "\n",
    "        prediction = [overall_average_test + b1[pair[0]]+b2[pair[1]]\n",
    "                                    +np.dot(p[pair[0]],q[pair[1]]) for pair in test_rating_to_predict]\n",
    "    \n",
    "        total_sum+=mean_squared_error(target_rating_test, prediction, squared = False)\n",
    "\n",
    "    return (np.array([[total_sum]], dtype=\"float32\"))\n",
    "\n",
    "# main:\n",
    "start_time = time.time()\n",
    "\n",
    "cluster = LocalCluster(n_workers=os.cpu_count())\n",
    "client = Client(cluster)\n",
    "\n",
    "mse_sum =0 \n",
    "ITERATIONS = 320\n",
    "\n",
    "# Note: Set the best hyperparameters found in the bayesian optimization process in the cell above.\n",
    "nof_svd_users, nof_test_users, nof_latent_features, epochs, rt, lr = (467, 189, 292, 268, 0.01588566520994121, 0.04980345674001443)\n",
    "\n",
    "\n",
    "SEED_INT = 5\n",
    "outer_gen = Generator(PCG64(SEED_INT))\n",
    "\n",
    "parameters_list = []\n",
    "\n",
    "# Note: \n",
    "# This loop has a high cost because it makes a new choice of users every iteration.\n",
    "# However, if the choice of users was repeated for a number of iterations before switching then there would be more noise.\n",
    "# The more variety of users the less noise. Also, the cost is relatively low compared to the rest of the operations.\n",
    "\n",
    "# Note:\n",
    "# user_to_data_svd_copy and user_to_data_train_copy are converted from list to numpy array of objects, then sampled, and then convert back to a list. \n",
    "# This is to avoid using more than one random type and suppresses the warning about arrays being a ragged nested sequences. \n",
    "\n",
    "for _ in range(ITERATIONS):\n",
    "    parameters_list.append([outer_gen.integers(0,100000),\n",
    "                            list(copy.deepcopy(outer_gen.choice(np.array(user_to_data_svd, dtype='object'), nof_svd_users, replace = False))),\n",
    "                            list(copy.deepcopy(outer_gen.choice(np.array(user_to_data_test, dtype = \"object\"), nof_test_users, replace = False))),\n",
    "                            nof_latent_features, epochs, rt, lr])\n",
    "\n",
    "parameters_arr = np.array(parameters_list, dtype=\"object\")\n",
    "dask_array = da.from_array(parameters_arr, chunks=(int(ITERATIONS/8),7))\n",
    "results = dask_array.map_blocks(rmse_sum, chunks = (1,1), dtype=\"float32\").compute()\n",
    "\n",
    "for row in results:\n",
    "    mse_sum+= row[0]\n",
    "\n",
    "print(\"Average RMSE score:\",mse_sum/ITERATIONS)\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "print(\"Minutes taken:\", (time.time()- start_time)/60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
