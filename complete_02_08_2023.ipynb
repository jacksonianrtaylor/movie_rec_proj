{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    length = len(array[8])\n",
    "    if(array[8][length-4:]==\"<NA>\"):\n",
    "        return False\n",
    "    length = len(array[9])\n",
    "    if(array[9][length-4:]==\"<NA>\"):\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#convert the dataframe into an array and build a dictionary\n",
    "user_to_data = dict()\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "\n",
    "#get all unique user ids\n",
    "list_of_user_ids = []\n",
    "last_id  = -1\n",
    "for item in complete_array:\n",
    "    if(item[0]!= last_id):\n",
    "        list_of_user_ids.append(item[0])\n",
    "        last_id = item[0]\n",
    "\n",
    "\n",
    "index  = 0\n",
    "#this has been tested with 5000, 10000, 20000, 100000\n",
    "nof_users = 20000\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, nof_users):\n",
    "    user_to_data[list_of_user_ids[i]] = []\n",
    "    for j in range(index, len(complete_array)):\n",
    "        if complete_array[j][0] == list_of_user_ids[i]:\n",
    "            #condition is checked for complete_array[j]\n",
    "            if(condition(complete_array[j])):\n",
    "                #this is where data is tranformed\n",
    "                transformed = provide_data(complete_array[j])\n",
    "                user_to_data[list_of_user_ids[i]].append(transformed)         \n",
    "        else:\n",
    "            #ignore if the number of ratings for a user is too small\n",
    "            if (len(user_to_data[list_of_user_ids[i]])<10):\n",
    "                del user_to_data[list_of_user_ids[i]]\n",
    "            index = j+1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for key in user_to_data.keys():\n",
    "        writer.writerows(user_to_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ordered_set import OrderedSet\n",
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#user to data rows \n",
    "user_to_data = dict()\n",
    "user_to_data_train = dict()\n",
    "user_to_data_test = dict()\n",
    "\n",
    "user_id = -1\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_data[row[0]] = [row]\n",
    "    else:\n",
    "        user_to_data[row[0]].append(row)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    user = random.choice(list(user_to_data.keys()))\n",
    "    user_to_data_train[user] = user_to_data[user]\n",
    "    user_to_data.pop(user)\n",
    "\n",
    "for i in range(20):\n",
    "    user = random.choice(list(user_to_data_train.keys()))\n",
    "    user_to_data_test[user] = user_to_data_train[user]\n",
    "    user_to_data_train.pop(user)\n",
    "\n",
    "\n",
    "user_to_data.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "\n",
    "user_to_movie_id_to_corpus_train = dict()\n",
    "movies_in_order = OrderedSet()\n",
    "user_to_movie_id_to_rating = dict()\n",
    "\n",
    "#this is what determines ommited movies\n",
    "user_to_rand_movie_id = dict()\n",
    "\n",
    "# WordNetLemmatizer().lemmatize(token.lower())\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for user in user_to_data_train.keys():\n",
    "    movie_strings = dict()\n",
    "    movie_id_to_rating = dict()\n",
    "    cnt = 0\n",
    "    rand_int = random.randint(0, len(user_to_data_train[user])-1)\n",
    "    for movie_data in user_to_data_train[user]:\n",
    "        if cnt == rand_int:    \n",
    "            user_to_rand_movie_id[user] = movie_data[1]\n",
    "        else:\n",
    "            movies_in_order.add(movie_data[1])\n",
    "        movie_string = \"\"\n",
    "        #avoid the first three data points (user id, movieid, and rating)\n",
    "        #use only the text data\n",
    "        for index in range (3,len(movie_data)):\n",
    "            if(index!= len(movie_data)-1):\n",
    "                movie_string+= movie_data[index]+\" \"\n",
    "            else:\n",
    "                movie_string+= movie_data[index]\n",
    "        cleaned = remove_stopwords(movie_string)\n",
    "        #why recreate the string when you could accept it as a list???\n",
    "        # cleaned = \" \".join([wnl.lemmatize(word) for word in cleaned.split(\" \")])\n",
    "        cleaned = [wnl.lemmatize(word) for word in cleaned.split(\" \")]\n",
    "        # cleaned.sort()\n",
    "        movie_strings[movie_data[1]] = cleaned\n",
    "        movie_id_to_rating[movie_data[1]] = movie_data[2]\n",
    "        cnt+=1\n",
    "    user_to_movie_id_to_corpus_train[user] = movie_strings\n",
    "    user_to_movie_id_to_rating[user] = movie_id_to_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import statistics\n",
    "import math\n",
    "from ordered_set import OrderedSet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#these 2 data strctures all have omited values from user_to_rand_movie_id[user]\n",
    "user_to_word_counts = dict()\n",
    "words_in_order = OrderedSet()\n",
    "\n",
    "#this includes all the movies (even movies not rated by any users)\n",
    "movie_id_to_word_counts = dict()\n",
    "\n",
    "#the ommited movie rating is marked with -2 and the unrated movies to be filled in are marked with -1 \n",
    "user_to_ratings = dict()\n",
    "\n",
    "#this is the user to word counts of the omitted movies\n",
    "user_to_rand_word_counts = dict()\n",
    "\n",
    "# These previous data structures do not have omitted movies for each user:\n",
    "# user_to_movie_id_to_corpus_train = dict()\n",
    "# user_to_movie_id_to_rating = dict()\n",
    "\n",
    "# movies in order omits movies with id that is equal to user_to_rand_movie_id[user] for all users it has a rating for\n",
    "# movies_in_order = OrderedSet()\n",
    "\n",
    "\n",
    "user_to_rating_to_predict = dict()\n",
    "\n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    temp = []\n",
    "    #note: length of user_to_ratings[user] is always equal to len(movies_in_order)\n",
    "    for movie_id in movies_in_order:\n",
    "        if movie_id != user_to_rand_movie_id[user]:\n",
    "            if movie_id in user_to_movie_id_to_rating[user].keys():\n",
    "                temp.append(user_to_movie_id_to_rating[user][movie_id])\n",
    "            else:\n",
    "                #this signifies the ratings to be filled in before preictions from the model take place\n",
    "                temp.append(-1)\n",
    "        else:\n",
    "            #this signifies the ratings to be predicted by the model\n",
    "            temp.append(-2)\n",
    "            user_to_rating_to_predict[user] = user_to_movie_id_to_rating[user][movie_id]\n",
    "    user_to_ratings[user] = temp\n",
    "\n",
    "\n",
    "\n",
    "#note: words_in_order has ommited words from a single movie (user_to_rand_movie_id[user])\n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    for movie_id in user_to_movie_id_to_corpus_train[user].keys():\n",
    "        if movie_id != user_to_rand_movie_id[user]:\n",
    "            for word in user_to_movie_id_to_corpus_train[user][movie_id]:\n",
    "                words_in_order.add(word)\n",
    "\n",
    "            \n",
    "\n",
    "#note: user_to_word_counts and movie_id_to_word_counts have an omitted movies...\n",
    "#by user_to_rand_movie_id[user]\n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    user_to_word_counts[user] = []\n",
    "    for movie_id in user_to_movie_id_to_corpus_train[user].keys():\n",
    "        if movie_id != user_to_rand_movie_id[user]:\n",
    "            temp = []\n",
    "            for word in words_in_order:\n",
    "                temp.append(user_to_movie_id_to_corpus_train[user][movie_id].count(word))\n",
    "            user_to_word_counts[user].append(temp)\n",
    "            if movie_id not in movie_id_to_word_counts.keys():\n",
    "                movie_id_to_word_counts[movie_id] = temp\n",
    "        else:\n",
    "            temp = []\n",
    "            for word in words_in_order:\n",
    "                temp.append(user_to_movie_id_to_corpus_train[user][movie_id].count(word))\n",
    "            user_to_rand_word_counts[user] = temp\n",
    "            if movie_id not in movie_id_to_word_counts.keys():\n",
    "                movie_id_to_word_counts[movie_id] = temp\n",
    "\n",
    "#what if a new user to be tested has new words that are new to words_in_order???\n",
    "#what if a user simply has more words than normal, need to normalize(not this is doen with cossine similairity)\n",
    "#what if a user rated a movie that has not been rated by any users??? (is it omited ???)\n",
    "#do the randomly selected users meed to be omited from the words_in_order and user_to_word_counts\n",
    "#to emulate completely new users???\n",
    "\n",
    "\n",
    "def predict(user, word_counts):\n",
    "    values = []\n",
    "    #note: user_to_word_counts[user] length is equal to the number of movies rated by the user...\n",
    "    #minus one for the user_to_rand_movie_id[user]\n",
    "    for counts in user_to_word_counts[user]:\n",
    "        values.append(counts)\n",
    "    \n",
    "    cosine_sim = cosine_similarity(X = values ,Y = [word_counts])\n",
    "    #not sure if reshape is needed???\n",
    "    cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "    ratings = []\n",
    "    #note: user_to_ratings[user] length is equal to the number of movies\n",
    "    #the length of ratings is the number of rated movies -1 for user_to_rand_movie_id[user]\n",
    "    for rating in user_to_ratings[user]:\n",
    "        if(rating != -1 and rating !=2):\n",
    "            ratings.append(rating)\n",
    "\n",
    "\n",
    "    max = -10\n",
    "    pred_rating = ratings[0]\n",
    "\n",
    "    #note: for accuracy purposes, the user should have a certain number of rated movies\n",
    "    #instead of selecting the most similair movie should clusters be used???\n",
    "    #for now this simple method will suffice\n",
    "    #are the ratings lined up with the cossine similarities???\n",
    "    #yes the order of user_to_ratings is used to make user_to_corpus_list...\n",
    "    #which is used to make lst_of_word_to_nof_occurances\n",
    "\n",
    "    for sim, rat in zip(cosine_sim, ratings):\n",
    "        if sim>max:\n",
    "            max = sim\n",
    "            pred_rating = rat\n",
    "\n",
    "    return pred_rating\n",
    "\n",
    "\n",
    "#fill in the missing ratings for a user\n",
    "def fill_in_ratings(user):\n",
    "    cnt = 0\n",
    "    pred = 0\n",
    "    #note: it is possible that movies in order includes some movies that are not part of user_to_ratings[user]\n",
    "    #due to the randomly omitted movie for each user in user_to_ratings\n",
    "    for rating, movie_id in zip(user_to_ratings[user], list(movies_in_order)):\n",
    "        if rating == -2:\n",
    "            user_to_ratings[user][cnt] = predict(user, movie_id_to_word_counts[movie_id])\n",
    "            pred = user_to_ratings[user][cnt]\n",
    "            break\n",
    "        cnt+=1\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "predicted = []\n",
    "true = []\n",
    "\n",
    "#why are these different lengths???\n",
    "print(len(list(user_to_movie_id_to_corpus_train.keys())))\n",
    "print(len(list(user_to_rating_to_predict.keys())))\n",
    "\n",
    "\n",
    "# for user in user_to_movie_id_to_corpus_train.keys():\n",
    "#     predicted.append(fill_in_ratings(user))\n",
    "#     true.append(user_to_rating_to_predict[user])\n",
    "\n",
    "\n",
    "# print(r2_score(true, predicted))\n",
    "\n",
    "#now for the user comparison logic (need user to list of movie ratings)\n",
    "#fill in ratings that the user hasn't watched with the method above\n",
    "#then cluster the users by their ratings\n",
    "\n",
    "#note: agglomerative clustering might make more sense here since k-means has random init for centroids...\n",
    "#note: to guess a new users rating requires that none of that users ratings have been used to train the model\n",
    "#The data needs to be split into test and train before modeling the algorithm on the train data\n",
    "\n",
    "#Training process:\n",
    "#split data into test and train data\n",
    "#proceed with train data...\n",
    "#cluster movies by the tokens with range for k\n",
    "#cluster users by the ratings with range for k and (fill in ratings for movies a users hasn't watched with some guess)\n",
    "#guess: this can be obtained by clustering the movies that the user has watched...\n",
    "#for each movie the user hasn't watched find the cluster that it belongs to with the highest possible k value\n",
    "#that the user has at least one movie belonging to one of the clusters and then take the average of those movies\n",
    "#this is exactly like a later training step excpet it is applied to all the movies the user watched\n",
    "\n",
    "#for a single randomly chosen movie from each user in the trainging data...\n",
    "\n",
    "#find the cluster the movie belongs to \n",
    "#find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#take the average score of these movies\n",
    "#find the cluster the user belongs to\n",
    "#find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#train an mlp model with both averages and perhaps some extra statistics as features...\n",
    "#using the given movie ratings as actuals\n",
    "\n",
    "\n",
    "#The process of predicting a rating:\n",
    "#1. find the cluster the movie belongs to \n",
    "#2. find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#3. take the average score of these movies\n",
    "#4. find the cluster the user belongs to\n",
    "#5. find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#6. input into the trained mlp model both averages and perhaps some extra statistics\n",
    "#7. make predictions and test against the randomly chosen movies actual ratings\n",
    "\n",
    "\n",
    "#summary:\n",
    "#find cluster for movie -> find movies part of the same clusters that the users rated -> average\n",
    "#question: are the clusters unique to the movies the user has watched or to all movies???\n",
    "#what is the technical difference???\n",
    "#is this the same as finding the most simimlair movie the user rated and copying the rating???\n",
    "\n",
    "#find cluster for user -> find the ratings for the movie by people in the same cluster -> average\n",
    "\n",
    "#other avenues considered:\n",
    "#idea 1:\n",
    "#for the first process, instead of averaging the movies that only the user rated, find other users that are...\n",
    "#like the user in question and find the average for that movie cluster\n",
    "#Problem: it is better to get the users raw opionion rather than generalizing it to some like minded users\n",
    "#there is an extra costly step to this\n",
    "#idea 2: \n",
    "#for the second process, instead of finding the average rating for the movie in the same cluster of users...\n",
    "#also find the average rating of movies that are like the movie in question \n",
    "#Problem, it is better to get the movies rating itself as it would be the most accurate indicator\n",
    "#there is an extra costly step to this\n",
    "\n",
    "\n",
    "\n",
    "#next steps\n",
    "#This took a little under an hour to compute: 57m 20.7 seconds\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
