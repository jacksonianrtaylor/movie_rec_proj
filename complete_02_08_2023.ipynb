{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId    id rating               title  \\\n",
      "8830418      1  2959    4.0      License to Wed   \n",
      "1690638      1  1968    4.0       Fools Rush In   \n",
      "2343877      1  2762    4.5  Young and Innocent   \n",
      "2943547      1   147    4.5       The 400 Blows   \n",
      "8434697      1  1246    5.0        Rocky Balboa   \n",
      "\n",
      "                                                                                                genres  \\\n",
      "8830418                                                                 [{'id': 35, 'name': 'Comedy'}]   \n",
      "1690638  [{'id': 18, 'name': 'Drama'}, {'id': 35, 'name': 'Comedy'}, {'id': 10749, 'name': 'Romance'}]   \n",
      "2343877                                     [{'id': 18, 'name': 'Drama'}, {'id': 80, 'name': 'Crime'}]   \n",
      "2943547                                                                  [{'id': 18, 'name': 'Drama'}]   \n",
      "8434697                                                                  [{'id': 18, 'name': 'Drama'}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                production_companies  \\\n",
      "8830418  [{'name': 'Village Roadshow Pictures', 'id': 79}, {'name': 'Robert Simonds Productions', 'id': 3929}, {'name': 'Warner Bros.', 'id': 6194}, {'name': 'Phoenix Pictures', 'id': 11317}, {'name': 'Underground', 'id': 49326}, {'name': 'Proposal Productions', 'id': 49327}]   \n",
      "1690638                                                                                                                                                                                                                                     [{'name': 'Columbia Pictures', 'id': 5}]   \n",
      "2343877                                                                                                                                                                                                                [{'name': 'Gaumont British Picture Corporation', 'id': 4978}]   \n",
      "2943547                                                                                                                                 [{'name': 'Les Films du Carrosse', 'id': 53}, {'name': 'SÃ©dif Productions', 'id': 10897}, {'name': 'The Criterion Collection', 'id': 10932}]   \n",
      "8434697                                                                                                  [{'name': 'Columbia Pictures', 'id': 5}, {'name': 'Revolution Studios', 'id': 497}, {'name': 'Rogue Marble', 'id': 696}, {'name': 'Metro-Goldwyn-Mayer (MGM)', 'id': 8411}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       keywords  \\\n",
      "8830418                                                                                                                                                                                                                                                                                                                                             [{'id': 1605, 'name': 'new love'}, {'id': 2856, 'name': 'ten commandments'}, {'id': 3582, 'name': 'bride'}, {'id': 3583, 'name': 'bridegroom'}, {'id': 6038, 'name': 'marriage'}, {'id': 6192, 'name': 'relation'}, {'id': 6281, 'name': 'partnership'}, {'id': 6704, 'name': 'civil registry office'}, {'id': 10093, 'name': 'priest'}, {'id': 13027, 'name': 'wedding'}, {'id': 14765, 'name': 'church'}]   \n",
      "1690638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [{'id': 828, 'name': 'waitress'}, {'id': 1463, 'name': 'culture clash'}, {'id': 9799, 'name': 'romantic comedy'}, {'id': 13149, 'name': 'pregnancy'}]   \n",
      "2343877                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [{'id': 769, 'name': 'falsely accused'}, {'id': 1655, 'name': 'country house'}, {'id': 9826, 'name': 'murder'}, {'id': 9937, 'name': 'suspense'}]   \n",
      "2943547                                                                                                                                                                                                                                                                                                                                                                      [{'id': 6930, 'name': 'fondling'}, {'id': 10183, 'name': 'independent film'}, {'id': 155518, 'name': 'nouvelle vague'}, {'id': 170268, 'name': 'skipping school'}, {'id': 170272, 'name': 'mise en scene'}, {'id': 170273, 'name': 'fingerprinting'}, {'id': 170279, 'name': '\\xa0mugshot'}, {'id': 170286, 'name': 'strict teacher'}, {'id': 170293, 'name': 'montmartre paris'}]   \n",
      "8434697  [{'id': 276, 'name': 'philadelphia'}, {'id': 396, 'name': 'transporter'}, {'id': 1721, 'name': 'fight'}, {'id': 2038, 'name': \"love of one's life\"}, {'id': 2416, 'name': 'publicity'}, {'id': 2792, 'name': 'boxer'}, {'id': 2968, 'name': 'grave'}, {'id': 3393, 'name': 'tombstone'}, {'id': 3586, 'name': 'tv station'}, {'id': 4487, 'name': 'boxing match'}, {'id': 4610, 'name': 'comeback'}, {'id': 4613, 'name': 'training'}, {'id': 5167, 'name': 'restaurant owner'}, {'id': 5378, 'name': 'world champion'}, {'id': 5379, 'name': 'challenger'}, {'id': 5380, 'name': 'virtual fight'}, {'id': 6066, 'name': 'defeat'}, {'id': 6067, 'name': 'victory'}, {'id': 10163, 'name': 'cancer'}, {'id': 155464, 'name': 'over-the-hill fighter'}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        cast  \\\n",
      "8830418                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [{'cast_id': 18, 'character': 'Reverend Frank', 'credit_id': '52fe4376c3a36847f8056039', 'gender': 2, 'id': 2157, 'name': 'Robin Williams', 'order': 0, 'profile_path': '/sojtJyIV3lkUeThD7A2oHNm8183.jpg'}, {'cast_id': 19, 'character': 'Sadie Jones', 'credit_id': '52fe4376c3a36847f805603d', 'gender': 1, 'id': 16855, 'name': 'Mandy Moore', 'order': 1, 'profile_path': '/15sDtRpe301tZWrRYV31wjMuFpx.jpg'}, {'cast_id': 20, 'character': 'Ben Murphy', 'credit_id': '52fe4376c3a36847f8056041', 'gender': 2, 'id': 17697, 'name': 'John Krasinski', 'order': 2, 'profile_path': '/nOWwdZURikW22qo6OUSGFCTukgc.jpg'}, {'cast_id': 21, 'character': 'Carlisle', 'credit_id': '52fe4376c3a36847f8056045', 'gender': 2, 'id': 29020, 'name': 'Eric Christian Olsen', 'order': 3, 'profile_path': '/clbouet8o9IJlUd8WILD0lzHAtG.jpg'}, {'cast_id': 22, 'character': 'Lindsey Jones', 'credit_id': '52fe4376c3a36847f8056049', 'gender': 1, 'id': 15286, 'name': 'Christine Taylor', 'order': 4, 'profile_path': '/99OssnGmgGjduXFA7syxjNqt9tQ.jpg'}, {'cast_id': 23, 'character': 'Choir Boy', 'credit_id': '52fe4376c3a36847f805604d', 'gender': 2, 'id': 216, 'name': 'Josh Flitter', 'order': 5, 'profile_path': '/6RCA8tDWBxIVk9N3IqUjJEAzYGv.jpg'}, {'cast_id': 24, 'character': 'Joel', 'credit_id': '52fe4376c3a36847f8056051', 'gender': 2, 'id': 11827, 'name': 'DeRay Davis', 'order': 6, 'profile_path': '/w2JYPRLwXhNCpxpJc2v4UQYyMv8.jpg'}, {'cast_id': 25, 'character': 'Mr. Jones', 'credit_id': '52fe4376c3a36847f8056055', 'gender': 2, 'id': 21368, 'name': 'Peter Strauss', 'order': 7, 'profile_path': '/ufx1trct43k7UcT4DpoIMPZXi5A.jpg'}, {'cast_id': 26, 'character': 'Grandma Jones', 'credit_id': '52fe4376c3a36847f8056059', 'gender': 1, 'id': 6465, 'name': 'Grace Zabriskie', 'order': 8, 'profile_path': '/ibBabuSM1UyPYFFo0wBXhGbqElk.jpg'}, {'cast_id': 27, 'character': 'Mrs. Jones', 'credit_id': '52fe4376c3a36847f805605d', 'gender': 1, 'id': 29021, 'name': 'Roxanne Hart', 'order': 9, 'profile_path': '/yWGMW6HdhUGT2oIcQ4jmnkw7ZAM.jpg'}, {'cast_id': 28, 'character': 'Shelly', 'credit_id': '5586ee469251417f6f0059c8', 'gender': 1, 'id': 125167, 'name': 'Mindy Kaling', 'order': 10, 'profile_path': '/Agpd4tJyZ95hk74RifjnfnJpn9U.jpg'}, {'cast_id': 30, 'character': 'Expectant Father', 'credit_id': '56c3467cc3a36847c5001f66', 'gender': 2, 'id': 1368801, 'name': 'David Quinlan', 'order': 11, 'profile_path': '/2m75rrBhvOTtdUS9jlKW8GOHCBV.jpg'}, {'cast_id': 31, 'character': 'Judith', 'credit_id': '58e26093c3a36872f600dcf2', 'gender': 1, 'id': 113867, 'name': 'Angela Kinsey', 'order': 12, 'profile_path': '/omLdRLdwMLliVeVIualEnWVhm1a.jpg'}]   \n",
      "1690638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [{'cast_id': 2, 'character': 'Alex Whitman', 'credit_id': '52fe4327c3a36847f803e629', 'gender': 2, 'id': 14408, 'name': 'Matthew Perry', 'order': 0, 'profile_path': '/oSKEEDXDNnwWdQ68qfDVD6Q7Pxp.jpg'}, {'cast_id': 3, 'character': 'Isabel Fuentes', 'credit_id': '52fe4327c3a36847f803e62d', 'gender': 1, 'id': 3136, 'name': 'Salma Hayek', 'order': 1, 'profile_path': '/u5mg73xKVqm8oT93HoMmsgQHyoK.jpg'}, {'cast_id': 4, 'character': 'Jeff', 'credit_id': '52fe4327c3a36847f803e631', 'gender': 2, 'id': 4602, 'name': 'Jon Tenney', 'order': 2, 'profile_path': '/fiG1bW6DX1szsRDPIYjfIKPQ0kV.jpg'}, {'cast_id': 5, 'character': 'Lanie', 'credit_id': '52fe4327c3a36847f803e635', 'gender': 1, 'id': 6751, 'name': 'Siobhan Fallon', 'order': 3, 'profile_path': '/wVFa8GiY0xdOLFsvGygy9RMtcBc.jpg'}, {'cast_id': 16, 'character': 'Great Grandma', 'credit_id': '52fe4327c3a36847f803e675', 'gender': 1, 'id': 20360, 'name': 'Angelina Torres', 'order': 4, 'profile_path': None}, {'cast_id': 17, 'character': 'Richard Whitman', 'credit_id': '52fe4327c3a36847f803e679', 'gender': 2, 'id': 20361, 'name': 'John Bennett Perry', 'order': 5, 'profile_path': '/bzFhwuXsdZiOHRtBgz4XVELIFYO.jpg'}, {'cast_id': 18, 'character': 'Nan Whitman', 'credit_id': '52fe4327c3a36847f803e67d', 'gender': 1, 'id': 20362, 'name': 'Jill Clayburgh', 'order': 6, 'profile_path': '/twrfhIvbqHuJ7nXVpehvU6nyi6R.jpg'}, {'cast_id': 19, 'character': 'Cathy Stewart', 'credit_id': '52fe4327c3a36847f803e681', 'gender': 1, 'id': 20363, 'name': 'Suzanne Snyder', 'order': 7, 'profile_path': '/90FrTcjJudpeIYUjUzlO6XAmvnt.jpg'}, {'cast_id': 20, 'character': 'Amalia', 'credit_id': '52fe4327c3a36847f803e685', 'gender': 0, 'id': 13029, 'name': 'Anne Betancourt', 'order': 8, 'profile_path': '/6UU5P4DzjJTSBFztIu1nALT2tk0.jpg'}, {'cast_id': 21, 'character': 'Juan Fuentes', 'credit_id': '52fe4327c3a36847f803e689', 'gender': 2, 'id': 4511, 'name': 'Mark Adair-Rios', 'order': 9, 'profile_path': '/rX4d1e5jlF5P73qynjjUzJslB0c.jpg'}, {'cast_id': 22, 'character': 'Judd Marshall', 'credit_id': '52fe4327c3a36847f803e68d', 'gender': 2, 'id': 4171, 'name': 'Stanley DeSantis', 'order': 10, 'profile_path': '/4cHxkhTd7oklyNkdva9WJp0FLrX.jpg'}, {'cast_id': 23, 'character': 'Antonio Fuentes', 'credit_id': '52fe4327c3a36847f803e691', 'gender': 0, 'id': 4665, 'name': 'Josh Cruze', 'order': 11, 'profile_path': '/v3QrQzH0uGV9pd1dNR5Ue6a74qO.jpg'}, {'cast_id': 24, 'character': 'Petra', 'credit_id': '52fe4327c3a36847f803e695', 'gender': 0, 'id': 4666, 'name': 'Angela Lanza', 'order': 12, 'profile_path': '/zmf6TMWMVCdnuUfpgdnioaICk1L.jpg'}, {'cast_id': 25, 'character': 'Phil', 'credit_id': '52fe4327c3a36847f803e699', 'gender': 2, 'id': 4445, 'name': 'Chris Bauer', 'order': 13, 'profile_path': '/3KYVMaGkWTEDQ0T9lsu85pVbP4T.jpg'}, {'cast_id': 26, 'character': 'Chuy', 'credit_id': '577e438f925141440c000d63', 'gender': 0, 'id': 115874, 'name': 'Carlos GÃ³mez', 'order': 14, 'profile_path': '/nBxwoMv1zrhNXyEjYXbcdmAdmF0.jpg'}]   \n",
      "2343877                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [{'cast_id': 18, 'character': 'Erica Burgoyne', 'credit_id': '52fe436bc3a36847f8052cd5', 'gender': 1, 'id': 27939, 'name': 'Nova Pilbeam', 'order': 0, 'profile_path': '/l6oHJaRYrVxsvoTSmMS5wIXaei5.jpg'}, {'cast_id': 19, 'character': 'Robert Tisdall', 'credit_id': '52fe436bc3a36847f8052cd9', 'gender': 0, 'id': 27940, 'name': 'Derrick De Marney', 'order': 1, 'profile_path': '/7VRZ7K0EZ50haOlbVr7DHZ5550O.jpg'}, {'cast_id': 20, 'character': 'Col. Burgoyne', 'credit_id': '52fe436bc3a36847f8052cdd', 'gender': 2, 'id': 27929, 'name': 'Percy Marmont', 'order': 2, 'profile_path': '/p3DIyvlxx6B0SVIxcDaPUPlEV0U.jpg'}, {'cast_id': 21, 'character': 'Old Will', 'credit_id': '52fe436bc3a36847f8052ce1', 'gender': 2, 'id': 27941, 'name': 'Edward Rigby', 'order': 3, 'profile_path': '/B7GJ0jPtODqZVgVtZHPtvZl2tO.jpg'}, {'cast_id': 22, 'character': 'Ericas Tante Margaret', 'credit_id': '52fe436bc3a36847f8052ce5', 'gender': 1, 'id': 14304, 'name': 'Mary Clare', 'order': 4, 'profile_path': '/lAdEwCGiSUj9CCMPB4L9X4oujLe.jpg'}, {'cast_id': 23, 'character': 'Det. Insp. Kent', 'credit_id': '52fe436bc3a36847f8052ce9', 'gender': 2, 'id': 7383, 'name': 'John Longden', 'order': 5, 'profile_path': '/rsCoUEx2ThNIz12fBR6vPncCICk.jpg'}, {'cast_id': 24, 'character': 'Guy', 'credit_id': '52fe436bc3a36847f8052ced', 'gender': 2, 'id': 27942, 'name': 'George Curzon', 'order': 6, 'profile_path': None}, {'cast_id': 25, 'character': 'Ericas Onkel Basil', 'credit_id': '52fe436bc3a36847f8052cf1', 'gender': 2, 'id': 14303, 'name': 'Basil Radford', 'order': 7, 'profile_path': '/9STo7Tgdutplo78ZtyeINGWkXUk.jpg'}, {'cast_id': 26, 'character': 'Christine Clay', 'credit_id': '52fe436bc3a36847f8052cf5', 'gender': 1, 'id': 27943, 'name': 'Pamela Carme', 'order': 8, 'profile_path': None}, {'cast_id': 27, 'character': 'Detective Sergeant Miller', 'credit_id': '52fe436bc3a36847f8052cf9', 'gender': 2, 'id': 27944, 'name': 'George Merritt', 'order': 9, 'profile_path': None}, {'cast_id': 28, 'character': 'Henry Briggs', 'credit_id': '52fe436bc3a36847f8052cfd', 'gender': 2, 'id': 27945, 'name': 'J.H. Roberts', 'order': 10, 'profile_path': None}, {'cast_id': 29, 'character': \"Truckfahrer bei Tom's Hat\", 'credit_id': '52fe436bc3a36847f8052d01', 'gender': 2, 'id': 27946, 'name': 'Jerry Verno', 'order': 11, 'profile_path': None}, {'cast_id': 30, 'character': 'Police Sergeant Ruddock', 'credit_id': '52fe436bc3a36847f8052d05', 'gender': 2, 'id': 27947, 'name': 'H.F. Maltby', 'order': 12, 'profile_path': None}, {'cast_id': 31, 'character': 'Police Constable', 'credit_id': '52fe436bc3a36847f8052d09', 'gender': 2, 'id': 27948, 'name': 'John Miller', 'order': 13, 'profile_path': None}]   \n",
      "2943547  [{'cast_id': 6, 'character': 'Antoine Doinel', 'credit_id': '52fe421ec3a36847f8005661', 'gender': 2, 'id': 1653, 'name': 'Jean-Pierre LÃ©aud', 'order': 0, 'profile_path': '/dzkPODapVe4CSubEqI9ytTCqnZ7.jpg'}, {'cast_id': 7, 'character': 'Gilberte Doinel', 'credit_id': '52fe421ec3a36847f8005665', 'gender': 1, 'id': 1654, 'name': 'Claire Maurier', 'order': 1, 'profile_path': '/cP1n7zMsMKr77xJeR3CncomxEZ0.jpg'}, {'cast_id': 8, 'character': 'Julien Doinel', 'credit_id': '52fe421ec3a36847f8005669', 'gender': 0, 'id': 1655, 'name': 'Albert RÃ©my', 'order': 2, 'profile_path': '/6b8eyIXAV6oA5eX6ltc3hF7ZB3d.jpg'}, {'cast_id': 10, 'character': 'Mr. Bigey', 'credit_id': '52fe421ec3a36847f8005673', 'gender': 2, 'id': 1658, 'name': 'Georges Flamant', 'order': 3, 'profile_path': '/lQwmtPsFWME63x5M7IRF6g8bLrR.jpg'}, {'cast_id': 11, 'character': 'RenÃ©', 'credit_id': '52fe421ec3a36847f8005677', 'gender': 0, 'id': 1659, 'name': 'Patrick Auffay', 'order': 4, 'profile_path': None}, {'cast_id': 12, 'character': 'Director of the school', 'credit_id': '52fe421ec3a36847f800567b', 'gender': 0, 'id': 1660, 'name': 'Robert Beauvais', 'order': 5, 'profile_path': None}, {'cast_id': 13, 'character': 'Mme Bigey', 'credit_id': '52fe421ec3a36847f800567f', 'gender': 0, 'id': 1661, 'name': 'Yvonne Claudie', 'order': 6, 'profile_path': None}, {'cast_id': 14, 'character': 'English Teacher', 'credit_id': '52fe421ec3a36847f8005683', 'gender': 0, 'id': 1662, 'name': 'Pierre Repp', 'order': 7, 'profile_path': '/1AUhiNGBAR0C6AU9iK1IXBs3QTz.jpg'}, {'cast_id': 17, 'character': 'French Teacher', 'credit_id': '52fe421ec3a36847f8005693', 'gender': 0, 'id': 1656, 'name': 'Guy Decomble', 'order': 8, 'profile_path': '/34iexAuqI1asyFounbSXSCFphen.jpg'}, {'cast_id': 20, 'character': 'Betrand Mauricet', 'credit_id': '52fe421ec3a36847f8005697', 'gender': 0, 'id': 1077237, 'name': 'Daniel Couturier', 'order': 9, 'profile_path': None}, {'cast_id': 21, 'character': 'Child', 'credit_id': '52fe421ec3a36847f800569b', 'gender': 0, 'id': 1077238, 'name': 'FranÃ§ois Nocher', 'order': 10, 'profile_path': None}, {'cast_id': 22, 'character': 'Child', 'credit_id': '52fe421ec3a36847f800569f', 'gender': 2, 'id': 150939, 'name': 'Richard Kanayan', 'order': 11, 'profile_path': '/vCMDk3ifj2vJKZYCISXT3K6DYXF.jpg'}, {'cast_id': 23, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056a3', 'gender': 0, 'id': 1077239, 'name': 'Renaud Fontanarosa', 'order': 12, 'profile_path': None}, {'cast_id': 24, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056a7', 'gender': 0, 'id': 1077240, 'name': 'Michel Girard', 'order': 13, 'profile_path': None}, {'cast_id': 25, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056ab', 'gender': 0, 'id': 71997, 'name': 'Serge Moati', 'order': 14, 'profile_path': '/wccRQKHrX61sH4WlOtM1KBP4qaq.jpg'}, {'cast_id': 26, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056af', 'gender': 0, 'id': 1077241, 'name': 'Bernard Abbou', 'order': 15, 'profile_path': None}, {'cast_id': 27, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056b3', 'gender': 0, 'id': 1077242, 'name': 'Jean-FranÃ§ois Bergouignan', 'order': 16, 'profile_path': None}, {'cast_id': 28, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056b7', 'gender': 0, 'id': 1077243, 'name': 'Michel Lesignor', 'order': 17, 'profile_path': None}, {'cast_id': 31, 'character': 'Man in Street', 'credit_id': '5457f0a1c3a3683993000156', 'gender': 2, 'id': 24299, 'name': 'Jean-Claude Brialy', 'order': 18, 'profile_path': '/g3kkYcAvq90tALMErxmdAIcIXsE.jpg'}, {'cast_id': 32, 'character': 'Woman with Dog', 'credit_id': '5457f0bec3a36839a0000144', 'gender': 1, 'id': 14812, 'name': 'Jeanne Moreau', 'order': 19, 'profile_path': '/uHJnVwCzehEoz0mIlwN7xkymql8.jpg'}, {'cast_id': 33, 'character': 'Man in Funfair', 'credit_id': '5457f0d3c3a368399300015b', 'gender': 2, 'id': 34613, 'name': 'Philippe de Broca', 'order': 20, 'profile_path': '/yrvmXE2SJBX659r2Y7eWwlmwfYd.jpg'}, {'cast_id': 34, 'character': 'Man in Funfair', 'credit_id': '5457f0e5c3a368399d00014c', 'gender': 0, 'id': 1650, 'name': 'FranÃ§ois Truffaut', 'order': 21, 'profile_path': '/apCCV99N3FqB5NsEPqOzetlkprL.jpg'}]   \n",
      "8434697                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [{'cast_id': 24, 'character': 'Rocky Balboa', 'credit_id': '52fe42e9c3a36847f802c61b', 'gender': 2, 'id': 16483, 'name': 'Sylvester Stallone', 'order': 0, 'profile_path': '/gnmwOa46C2TP35N7ARSzboTdx2u.jpg'}, {'cast_id': 25, 'character': 'Paulie', 'credit_id': '52fe42e9c3a36847f802c61f', 'gender': 2, 'id': 4521, 'name': 'Burt Young', 'order': 1, 'profile_path': '/rbSsSkQ72FoGcvwIHUxQWJ92I3W.jpg'}, {'cast_id': 26, 'character': 'Rocky Jr.', 'credit_id': '52fe42e9c3a36847f802c623', 'gender': 2, 'id': 16501, 'name': 'Milo Ventimiglia', 'order': 2, 'profile_path': '/maJeS6bA6ku21rSRceISQtwHL2h.jpg'}, {'cast_id': 27, 'character': 'Marie', 'credit_id': '52fe42e9c3a36847f802c627', 'gender': 1, 'id': 16502, 'name': 'Geraldine Hughes', 'order': 3, 'profile_path': '/bTXux3EJq25Fh2ixbet6MjdG3Fb.jpg'}, {'cast_id': 28, 'character': 'Steps', 'credit_id': '52fe42e9c3a36847f802c62b', 'gender': 2, 'id': 16503, 'name': 'James Francis Kelly III', 'order': 4, 'profile_path': '/iZyTQ2UlwNXrqLqPeNHbofFXubP.jpg'}, {'cast_id': 29, 'character': 'Duke', 'credit_id': '52fe42e9c3a36847f802c62f', 'gender': 2, 'id': 16504, 'name': 'Tony Burton', 'order': 5, 'profile_path': '/ue54hK217thXeQMzN4qUYXLImLd.jpg'}, {'cast_id': 30, 'character': 'L.C.', 'credit_id': '52fe42e9c3a36847f802c633', 'gender': 2, 'id': 16505, 'name': 'A. J. Benza', 'order': 6, 'profile_path': '/5hVinC6C1ZyD7c8EmZFTiEaF7vH.jpg'}, {'cast_id': 31, 'character': 'Adrian', 'credit_id': '52fe42e9c3a36847f802c637', 'gender': 1, 'id': 3094, 'name': 'Talia Shire', 'order': 7, 'profile_path': '/liNfrVB3eZFBOjcUGltISCucews.jpg'}, {'cast_id': 32, 'character': 'Martin', 'credit_id': '52fe42e9c3a36847f802c63b', 'gender': 2, 'id': 16506, 'name': 'Henry G. Sanders', 'order': 8, 'profile_path': '/2SU75g2CAIzGWbgfIlNvKZQhYTZ.jpg'}, {'cast_id': 33, 'character': \"Mason 'The Line' Dixon\", 'credit_id': '52fe42e9c3a36847f802c63f', 'gender': 2, 'id': 16507, 'name': 'Antonio Tarver', 'order': 9, 'profile_path': '/kJEljjHwBvrjoxqcSVntXlejgl1.jpg'}, {'cast_id': 34, 'character': 'Spider Rico', 'credit_id': '52fe42e9c3a36847f802c643', 'gender': 2, 'id': 16508, 'name': 'Pedro Lovell', 'order': 10, 'profile_path': None}, {'cast_id': 35, 'character': 'Isabel', 'credit_id': '52fe42e9c3a36847f802c647', 'gender': 1, 'id': 16509, 'name': 'Ana Gerena', 'order': 11, 'profile_path': None}, {'cast_id': 36, 'character': 'Angie', 'credit_id': '52fe42e9c3a36847f802c64b', 'gender': 1, 'id': 16510, 'name': 'Angela Boyd', 'order': 12, 'profile_path': None}, {'cast_id': 37, 'character': 'Bar Thug', 'credit_id': '52fe42e9c3a36847f802c64f', 'gender': 0, 'id': 16511, 'name': 'Louis Giansante', 'order': 13, 'profile_path': None}, {'cast_id': 38, 'character': \"Lucky's Bartender\", 'credit_id': '52fe42e9c3a36847f802c653', 'gender': 0, 'id': 16512, 'name': 'Maureen Schilling', 'order': 14, 'profile_path': None}, {'cast_id': 40, 'character': 'X-Cell', 'credit_id': '5761db05c3a3682f20000302', 'gender': 2, 'id': 98298, 'name': 'Lahmard J. Tate', 'order': 15, 'profile_path': '/4WcFReePSxyGQJWV5wXGNfY0Y7o.jpg'}]   \n",
      "\n",
      "                                                                               tagline  \\\n",
      "8830418                                   First came love... then came Reverend Frank.   \n",
      "1690638  What if finding the love of your life meant changing the life that you loved?   \n",
      "2343877                                                          A Brilliant Melodrama   \n",
      "2943547                                            Angel faces hell-bent for violence.   \n",
      "8434697                                                  It ain't over 'til it's over.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        overview  \n",
      "8830418                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Newly engaged, Ben and Sadie can't wait to start their life together and live happily ever after. However Sadie's family church's Reverend Frank won't bless their union until they pass his patented, \"foolproof\" marriage prep course consisting of outrageous classes, outlandish homework assignments and some outright invasion of privacy.  \n",
      "1690638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Alex Whitman (Matthew Perry) is a designer from New York City who is sent to Las Vegas to supervise the construction of a nightclub that his firm has been hired to build. Alex is a straight-laced WASP-ish type who, while enjoying a night on the town, meets Isabel Fuentes (Salma Hayek), a free-spirited Mexican-American photographer. Alex and Isabel are overtaken by lust at first sight and end up sp  \n",
      "2343877  Derrick De Marney finds himself in a 39 Steps situation when he is wrongly accused of murder. While a fugitive from the law, De Marney is helped by heroine Nova Pilbeam, who three years earlier had played the adolescent kidnap victim in Hitchcock's The Man Who Knew Too Much. The obligatory \"fish out of water\" scene, in which the principals are briefly slowed down by a banal everyday event, occurs during a child's birthday party. The actual villain, whose identity is never in doubt (Hitchcock made thrillers, not mysteries) is played by George Curzon, who suffers from a twitching eye. Curzon's revelation during an elaborate nightclub sequence is a Hitchcockian tour de force, the sort of virtuoso sequence taken for granted in these days of flexible cameras and computer enhancement, but which in 1937 took a great deal of time, patience and talent to pull off. Released in the US as The Girl Was Young, Young and Innocent was based on a novel by Josephine Tey.  \n",
      "2943547                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     For young Parisian boy Antoine Doinel, life is one difficult situation after another. Surrounded by inconsiderate adults, including his neglectful parents, Antoine spends his days with his best friend, Rene, trying to plan for a better life. When one of their schemes goes awry, Antoine ends up in trouble with the law, leading to even more conflicts with unsympathetic authority figures.  \n",
      "8434697                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              When he loses a highly publicized virtual boxing match to ex-champ Rocky Balboa, reigning heavyweight titleholder, Mason Dixon retaliates by challenging Rocky to a nationally televised, 10-round exhibition bout. To the surprise of his son and friends, Rocky agrees to come out of retirement and face an opponent who's faster, stronger and thirty years his junior.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 3\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "#new:\n",
    "#userIds are in order\n",
    "# every_id = list(unique(list(complete[\"userId\"])))\n",
    "#userIds are out of order\n",
    "# sample_ids  = random.sample(every_id, 1000)\n",
    "# completeNew = pd.DataFrame()\n",
    "#This is a very expensive task...\n",
    "#it is possile to choose a subset of users from here instead of\n",
    "#ibcluing  the entire set of users\n",
    "# for user in sample_ids:\n",
    "#     completeNew = pd.concat([completeNew, complete.loc[complete[\"userId\"] == user]])\n",
    "\n",
    "#new:\n",
    "# complete = complete.sample(frac=1, random_state = seed_int, axis =0)\n",
    "#this is not the desired behavior\n",
    "#the users ids need to show up in a true random order\n",
    "# complete = complete.groupby(by = \"userId\", sort = False, group_keys = True).apply(lambda x: x)\n",
    "#this is omitted since the values should not be sorted by userId just grouped by userId\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "\n",
    "print(complete.head())\n",
    "\n",
    "# f = open(\"test_dicts.txt\", \"w\", encoding=\"utf-8\")\n",
    "# f.write(str(list(complete[\"tagline\"])))\n",
    "# f.write(str(list(complete[\"overview\"])))\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2529\n",
      "29.392778279634648\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 3\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    #this is probably not needed due to the dropNa function used above...\n",
    "    # length = len(array[8])\n",
    "    # if(array[8][length-4:]==\"<NA>\"):\n",
    "    #     return False\n",
    "    # length = len(array[9])\n",
    "    # if(array[9][length-4:]==\"<NA>\"):\n",
    "    #     return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#new method\n",
    "list_of_user_ids = list(complete[\"userId\"].unique())\n",
    "\n",
    "counts = complete['userId'].value_counts()\n",
    "gaps = []\n",
    "for id in list_of_user_ids:\n",
    "    gaps.append(counts[id])\n",
    "\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "#old method...\n",
    "# gaps = []\n",
    "# size = 0\n",
    "# list_of_user_ids = []\n",
    "# last_id  = -1\n",
    "# past_first_it = False\n",
    "\n",
    "\n",
    "# for row in complete_array:\n",
    "#     if(row[0]!= last_id):\n",
    "#         list_of_user_ids.append(row[0])\n",
    "#         last_id = row[0]\n",
    "#         if(past_first_it ==True):\n",
    "#             gaps.append(size)\n",
    "#             size =0 \n",
    "#     size+=1\n",
    "#     past_first_it = True\n",
    "\n",
    "# #there is always a gap for the last iteration\n",
    "# gaps.append(size)\n",
    "\n",
    "\n",
    "\n",
    "index  = 0\n",
    "\n",
    "user_to_data = []\n",
    "#this is the total number of users in the whole dataset\n",
    "total_nof_users = len(list_of_user_ids)\n",
    "#this is the number of desired users before filtering\n",
    "#note: set back to 40000\n",
    "desired_nof_users_before_filter = 40000\n",
    "\n",
    "avg = 0\n",
    "cnt = 0\n",
    "\n",
    "\n",
    "#instead of using this random pass generaion \n",
    "#the users can be picked randomly before hand and replace list_of_user_ids[i]\n",
    "#will index still work???\n",
    "\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, total_nof_users):\n",
    "    #generate a random float to determine a pass for the user\n",
    "    if (random.random()<float(desired_nof_users_before_filter/total_nof_users)):\n",
    "        #user_to_data[list_of_user_ids[i]] = []\n",
    "        user_to_data.append([])\n",
    "        last_index = len(user_to_data) -1\n",
    "        for j in range(index, len(complete_array)):\n",
    "            if complete_array[j][0] == list_of_user_ids[i]:\n",
    "                #condition is checked for complete_array[j]\n",
    "                if(condition(complete_array[j])):\n",
    "                    #this is where data is tranformed\n",
    "                    transformed = provide_data(complete_array[j])\n",
    "                    #is copy needed here???\n",
    "                    user_to_data[last_index].append(transformed)\n",
    "\n",
    "                #last iteration\n",
    "                if (j==len(complete_array)-1):\n",
    "                    if (len(user_to_data[last_index])<50 or len(user_to_data[last_index])>75):\n",
    "                        del user_to_data[last_index]       \n",
    "            else:\n",
    "                avg += len(user_to_data[last_index])\n",
    "                cnt+=1\n",
    "                #this condition can be tweaked for better accuracy\n",
    "                #len(user_to_data[list_of_user_ids[i]])<50 or len(user_to_data[list_of_user_ids[i]])>75\n",
    "                if (len(user_to_data[last_index])<50 or len(user_to_data[last_index])>75):\n",
    "                    del user_to_data[last_index]  \n",
    "                #note: changed from (index = j+1)\n",
    "                index = j\n",
    "                break\n",
    "        #note, problem: provide data is run in right order\n",
    "        #but a user can be deleted and then the order can be messed up    \n",
    "    else:\n",
    "        #every iteration, index starts at first data point of the next user\n",
    "        index += gaps[i]\n",
    "\n",
    "\n",
    "#runtime test, go through user_to_data and re-index the users in list order\n",
    "for i in range(len(user_to_data)):\n",
    "    for j in range(len(user_to_data[i])):\n",
    "        user_to_data[i][j][0] = i\n",
    "\n",
    "\n",
    "\n",
    "#needs to be sure that there are enough users after the condiiton\n",
    "print(len(user_to_data))\n",
    "\n",
    "#average number of ratings per users\n",
    "print(float(avg/cnt))\n",
    "\n",
    "#18 minutes for 2599 users\n",
    "\n",
    "#test with dictionaries:\n",
    "# 71\n",
    "#runtime: 30.679282868525895\n",
    "\n",
    "#test without dictionaries:\n",
    "# 63\n",
    "#runtime: 30.422131147540984\n",
    "\n",
    "#18 minutes for 2611 users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "\n",
    "#question: would renaming the user ids as indexes in their order be helpful???\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for i in range(len(user_to_data)):\n",
    "        writer.writerows(user_to_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#try removing unecessay copying...\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#user to data rows \n",
    "user_to_data = []\n",
    "user_to_data_train = []\n",
    "user_to_data_test = []\n",
    "user_id = -1\n",
    "\n",
    "#note: works when row[0] is also an index\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_data.append([row])\n",
    "    else:\n",
    "        user_to_data[int(row[0])].append(row)\n",
    "\n",
    "\n",
    "#this can be tweaked...\n",
    "for i in range(2500):\n",
    "    index = random.randint(0, len(user_to_data)-1)\n",
    "    user_to_data_train.append(user_to_data[index])\n",
    "    del user_to_data[index]\n",
    "\n",
    "\n",
    "#for test data to be used later...\n",
    "for i in range(500):\n",
    "    index = random.randint(0, len(user_to_data_train)-1)\n",
    "    user_to_data_test.append(user_to_data_train[index])\n",
    "    del user_to_data_train[index]\n",
    "\n",
    "\n",
    "del user_to_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import json\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#the version from numpy is used instead\n",
    "from scipy import linalg\n",
    "from scipy.linalg import sqrtm\n",
    "import math\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 1\n",
    "# random.seed(seed_int)\n",
    "\n",
    "\n",
    "#check if there are variables that can be removed!!!\n",
    "class user_type_vars():\n",
    "    def __init__(self):\n",
    "        #user to movie_id to a list of all the words in that movie (copies allowed)\n",
    "        self.user_to_movie_id_to_corpus = [] \n",
    "\n",
    "        #user to movie_id to its rating\n",
    "        self.user_to_movie_id_to_rating = [] \n",
    "\n",
    "        #for each user, includes a random choice from all the movies the user watched to be the target\n",
    "        self.user_to_target_movie_id = [] \n",
    "\n",
    "        #for each user, the index is the placement of the target movie in the entire set of train movies inorder\n",
    "        self.user_to_target_index_full = [] \n",
    "\n",
    "        #for each user, a set of all the words in all the movies the user watched \n",
    "        self.user_to_words_in_order  = [] \n",
    "\n",
    "        #for each user, the word counts for movies that they watched\n",
    "        self.user_to_word_counts  = [] \n",
    "        self.user_to_word_counts_transformed = []\n",
    "\n",
    "        #The word counts of the target movie for each user\n",
    "        self.user_to_target_word_counts  = [] \n",
    "        self.user_to_target_word_counts_transformed = []\n",
    "\n",
    "        #for each user, includes all the ratings of movies the user has rated with a -1 marked as the target rating\n",
    "        self.user_to_ratings  = []\n",
    "\n",
    "        #for each user, includes ratings for all the movies in the entire train set (missing ratings are filled in)\n",
    "        self.user_to_ratings_full = [] \n",
    "        self.user_to_ratings_full_transform = []\n",
    "\n",
    "        #model features x\n",
    "        self.feature_1 = []\n",
    "        self.feature_2 = []\n",
    "        self.feature_3 = []\n",
    "\n",
    "        #output feature y\n",
    "        self.user_to_target_rating  = [] \n",
    "\n",
    "\n",
    "#need to think about the order these variables are used to improve memory perf...\n",
    "#goal: minimize the window between when a variable was populated and used\n",
    "\n",
    "\n",
    "#user_to_movie_id_to_corpus:\n",
    "#1,2, 3, 4\n",
    "\n",
    "#user_to_movie_id_to_rating:\n",
    "#1, 3, after 5, 6\n",
    "\n",
    "#user_to_target_movie_id:\n",
    "#1, 3, 4, after 5, 6\n",
    "\n",
    "#user_to_target_index_full:\n",
    "#6,7\n",
    "\n",
    "#user_to_words_in_order:\n",
    "#2,4\n",
    "#this seems optimal\n",
    "\n",
    "#user_to_word_counts:\n",
    "#4,5,\n",
    "\n",
    "#user_to_word_counts_transformed:\n",
    "#4,5,\n",
    "\n",
    "#user_to_target_word_counts:\n",
    "#4,5\n",
    "\n",
    "#user_to_target_word_counts_transformed:\n",
    "#4,5\n",
    "\n",
    "#user_to_ratings: (the combination of 4 and 5 streamline this this)\n",
    "#3,5\n",
    "\n",
    "#user_to_ratings_full:\n",
    "#6,7\n",
    "\n",
    "#user_to_ratings_full_transform:\n",
    "#6,7\n",
    "\n",
    "#needed until the end:\n",
    "#self.features...\n",
    "#self.user_to_target_rating\n",
    "\n",
    "\n",
    "#goal: minimize the window between when a vraible was populated and used\n",
    "#or remove intermediate variables \n",
    "\n",
    "\n",
    "# first observation:\n",
    "# these variables are populated and used in sucession\n",
    "# user_to_word_counts:\n",
    "# user_to_word_counts_transformed:\n",
    "# user_to_target_word_counts:\n",
    "# user_to_target_word_counts_transformed:\n",
    "\n",
    "# the sole purpose of 4 and 5 is to created a cossine similairty list\n",
    "# so populate 4 and 5 can be combined\n",
    "\n",
    "\n",
    "#idea:\n",
    "#exchange places of 2 and 3\n",
    "\n",
    "\n",
    "#idea:\n",
    "#after the switch is populate 3 even necessary ???\n",
    "\n",
    "\n",
    "        \n",
    "train_users = user_type_vars()\n",
    "test_users = user_type_vars()\n",
    "\n",
    "\n",
    "#no changes needed for these vars below...\n",
    "movie_id_to_ratings_total = dict() #all ratings for each movie\n",
    "# movies_in_order = OrderedSet() #all movies in order\n",
    "\n",
    "train_movies_in_order = OrderedSet()\n",
    "test_movies_in_order = OrderedSet()\n",
    "\n",
    "movie_id_to_average_rating_train = dict() #average movie rating amoung all users for each movie\n",
    "user_to_average_rating = []\n",
    "\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "#train_users.user_to_movie_id_to_rating\n",
    "#train_users.user_to_target_movie_id\n",
    "#test_users.user_to_movie_id_to_rating\n",
    "#test_users.user_to_target_movie_id\n",
    "\n",
    "\n",
    "def load_feature_1(movies_in_order, user_to_data, movie_id_to_ratings, user_to_movie_id_to_rating, user_to_target_movie_id, user_to_target_rating, feature_1, feature_2):\n",
    "    #how do I integrate pop the other populate functions???\n",
    "    overall_rating_sum = 0\n",
    "    overall_rating_count = 0\n",
    "    for i in range(len(user_to_data)):\n",
    "        movie_id_to_words_temp = dict()\n",
    "        movie_id_to_rating_temp = dict()\n",
    "        cnt = 0\n",
    "        total =0\n",
    "        rand_int = random.randint(0, len(user_to_data[i])-1)\n",
    "        for movie_data in user_to_data[i]:\n",
    "            if cnt == rand_int:    \n",
    "                user_to_target_movie_id.append(movie_data[1])\n",
    "            else:\n",
    "                overall_rating_sum += float(movie_data[2])\n",
    "                overall_rating_count += 1\n",
    "                total += float(movie_data[2])\n",
    "            \n",
    "            if movie_data[1] in movie_id_to_ratings.keys():\n",
    "                movie_id_to_ratings[movie_data[1]].append(float(movie_data[2]))\n",
    "            else:\n",
    "                movie_id_to_ratings[movie_data[1]] = [float(movie_data[2])]\n",
    "\n",
    "            movie_string = \"\"\n",
    "            #avoid the first three data points (user id, movieid, and rating)\n",
    "            #use only the text data\n",
    "\n",
    "            #use of all the movi data...\n",
    "            # for index in range (3,len(movie_data)):\n",
    "            #     if(index!= len(movie_data)-1):\n",
    "            #         movie_string+= movie_data[index]+\" \"\n",
    "            #     else:\n",
    "            #         movie_string+= movie_data[index]\n",
    "\n",
    "            #only use the genres...\n",
    "            movie_string = movie_data[4]\n",
    "\n",
    "            #need to try with and without lematization...\n",
    "            cleaned = remove_stopwords(movie_string)\n",
    "            cleaned = [wnl.lemmatize(word) for word in cleaned.split(\" \")]\n",
    "            cleaned = [word[:-1] for word in cleaned if word.endswith(\".\")] + [word for word in cleaned if not word.endswith(\".\")]\n",
    "            #is copy really needed with this scope ???\n",
    "            movie_id_to_words_temp[movie_data[1]] = cleaned\n",
    "            movie_id_to_rating_temp[movie_data[1]] = float(movie_data[2])\n",
    "            movies_in_order.add(movie_data[1])\n",
    "            cnt+=1\n",
    "        user_to_movie_id_to_rating.append(movie_id_to_rating_temp)\n",
    "        user_to_average_rating.append(float(total/(cnt-1)))\n",
    "\n",
    "        #the current users list of words from all the movies they rated\n",
    "        users_words_in_order = OrderedSet()\n",
    "        for movie_id in movie_id_to_words_temp.keys():\n",
    "            for word in movie_id_to_words_temp[movie_id]:\n",
    "                users_words_in_order.add(word)\n",
    "\n",
    "\n",
    "        word_counts = []\n",
    "        target_word_counts = []\n",
    "\n",
    "        # these are only relevant with user averages scalingas opposed to movie average scaling...\n",
    "        word_counts_transformed = []\n",
    "        target_word_counts_transformed = []\n",
    "\n",
    "        #this will transform into the word count averages for each word for each movie rated by the user\n",
    "        averages = dict()\n",
    "\n",
    "        #for each movie the user watched record the wordcount for each word in the movies the user watched\n",
    "        for movie_id in movie_id_to_words_temp.keys():\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                temp_dict = Counter(movie_id_to_words_temp[movie_id])\n",
    "                temp_list = []\n",
    "                # avg = 0\n",
    "                # cnt =0\n",
    "                for word in users_words_in_order:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                        # avg+=temp_dict[word]\n",
    "                        # cnt+=1\n",
    "                        if word in averages.keys():\n",
    "                            averages[word] += temp_dict[word] \n",
    "                        else:\n",
    "                            averages[word] = temp_dict[word] \n",
    "                    else:\n",
    "                        temp_list.append(0) \n",
    "                        if word in averages.keys():\n",
    "                            averages[word] += 0 \n",
    "                        else:\n",
    "                            averages[word] = 0                        \n",
    "                #option 1:\n",
    "                # avg = float(avg/len(users_words_in_order))\n",
    "                #option 2:\n",
    "                # avg = float(avg/cnt)\n",
    "                word_counts.append(temp_list)\n",
    "                #option 1:\n",
    "                # temp_list_normalized = []\n",
    "                # for item in temp_list:\n",
    "                #     if item == 0:\n",
    "                #         temp_list_normalized.append(0)\n",
    "                #     else:\n",
    "                #         temp_list_normalized.append(item - avg)\n",
    "                # word_counts_transformed.append(temp_list_normalized)\n",
    "                #option 2:\n",
    "                # word_counts_transformed.append([x - avg for x in temp_list])\n",
    "            else:\n",
    "\n",
    "                temp_dict = Counter(movie_id_to_words_temp[movie_id])\n",
    "                temp_list = []\n",
    "                # avg = 0\n",
    "                # cnt =0\n",
    "                for word in users_words_in_order:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                        # avg+=temp_dict[word]\n",
    "                        # cnt +=1\n",
    "                        if word in averages.keys():\n",
    "                            averages[word] += temp_dict[word] \n",
    "                        else:\n",
    "                            averages[word] = temp_dict[word]             \n",
    "                    else:\n",
    "                        temp_list.append(0)\n",
    "\n",
    "                        if word in averages.keys():\n",
    "                            averages[word] += 0 \n",
    "                        else:\n",
    "                            averages[word] = 0\n",
    "                # option 1:\n",
    "                # avg = float(avg/len(users_words_in_order))\n",
    "                # option 2:\n",
    "                # avg = float(avg/cnt)\n",
    "                target_word_counts = temp_list\n",
    "                #option 1:\n",
    "                # target_word_counts_transformed = []\n",
    "                # for item in temp_list:\n",
    "                #     if item == 0:\n",
    "                #         target_word_counts_transformed.append(0)\n",
    "                #     else:\n",
    "                #         target_word_counts_transformed.append(item - avg)\n",
    "                #option 2:\n",
    "                #target_word_counts_transformed = [x - avg for x in temp_list]\n",
    "        \n",
    "\n",
    "        #create actual averages list... \n",
    "        #lambda can be used here...\n",
    "        def scale(x):\n",
    "            return float(x/len(movie_id_to_words_temp))\n",
    "        averages = [scale(averages[key]) for key in averages.keys()]\n",
    "\n",
    "        #option 1:\n",
    "        # complete = word_counts.copy()\n",
    "        # complete.append(target_word_counts)\n",
    "        # movie_averages = np.tile(averages, (len(complete),1))\n",
    "        # complete_array = np.array(complete)\n",
    "        # transformed_list  = list(complete_array - movie_averages)\n",
    "\n",
    "        #option 2:\n",
    "        complete_word_counts = word_counts.copy()\n",
    "        complete_word_counts.append(target_word_counts)\n",
    "        tfidf = TfidfTransformer().fit_transform(complete_word_counts).toarray()\n",
    "        \n",
    "\n",
    "        #for content based reccomendation, may need to focus on words from certain sections to not overwelm the model with useless information!!!\n",
    "\n",
    "        def svd_u(a, n):\n",
    "            U, s, V = np.linalg.svd(a, full_matrices=False)\n",
    "            #adjust with n\n",
    "            s=np.diag(s)\n",
    "            s=s[0:n,0:n]\n",
    "            U=U[:,0:n]\n",
    "            V=V[0:n,:]\n",
    "            #could try returning UsV instead...\n",
    "            #could try truncated svd...\n",
    "            return list(U)\n",
    "        \n",
    "        #option 1:\n",
    "        #transformed_word_counts = svd_u(transformed_list, 5)\n",
    "        #option 2:\n",
    "        #transformed_word_counts = svd_u(tfidf, 5)\n",
    "        #option 3:\n",
    "        #transformed_word_counts = TruncatedSVD(n_components = 20, random_state = seed_int).fit_transform(tfidf)\n",
    "\n",
    "\n",
    "        #populate ratings with the exception of the target rating and also record\n",
    "        #the users target rating\n",
    "        ratings = []\n",
    "        for movie_id in movie_id_to_rating_temp.keys():\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                ratings.append(movie_id_to_rating_temp[movie_id])\n",
    "            else:\n",
    "                #this signifies the ratings to be predicted by the model\n",
    "                user_to_target_rating.append(movie_id_to_rating_temp[movie_id])\n",
    "        \n",
    "        #return the average ratings from movies that are alike the target movie...\n",
    "        def predict(word_counts_transformed, word_counts,\n",
    "            target_word_counts_transformed, target_word_counts, ratings):\n",
    "            item_1 = 0 \n",
    "            item_2 = 0\n",
    "            item_3 = 0\n",
    "\n",
    "            #option 0:\n",
    "            # mdl = KMeans(n_clusters=5, n_init = \"auto\", random_state= seed_int)\n",
    "            # predictions = mdl.fit_predict(list(transformed_word_counts))\n",
    "            # target_cluster = predictions[len(predictions)-1]\n",
    "\n",
    "            # same_cluster = list(filter((lambda item: item == target_cluster), predictions))\n",
    "\n",
    "            # if(len(same_cluster)==1):\n",
    "            #     #there are no other ratings in the target movies cluster...\n",
    "            #     distance = np.inf\n",
    "            #     index =0\n",
    "            #     i = 0 \n",
    "            #     for cord in mdl.cluster_centers_[0:-1]:\n",
    "            #         if math.dist(cord, mdl.cluster_centers_[-1]) < distance:\n",
    "            #             distance = math.dist(cord, mdl.cluster_centers_[-1])\n",
    "            #             index = i\n",
    "            #         i+=1\n",
    "            #     closest_cluster = predictions[index]\n",
    "            #     i =0\n",
    "            #     cnt =0\n",
    "            #     avg = 0\n",
    "            #     for item in predictions[0:-1]:\n",
    "            #         if item == closest_cluster:\n",
    "            #             avg+= ratings[i]\n",
    "            #             cnt+=1\n",
    "            #         i+=1\n",
    "            #     item_3 = float(avg/cnt)      \n",
    "\n",
    "            # else:\n",
    "            #     #there is at least one other movie in the target movies cluster\n",
    "            #     i =0\n",
    "            #     cnt =0\n",
    "            #     avg = 0\n",
    "            #     for item in predictions[0:-1]:\n",
    "            #         if item == target_cluster:\n",
    "            #             avg+= ratings[i]\n",
    "            #             cnt+=1\n",
    "            #         i+=1\n",
    "            #     item_3 = float(avg/cnt)\n",
    "\n",
    "            #option 1: \n",
    "            cosine_sim = cosine_similarity(X = tfidf[0:-1],Y = [tfidf[-1]])\n",
    "            cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            combined = zip(cosine_sim, ratings)\n",
    "            combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "            avg = 0\n",
    "            nof = 5.0\n",
    "            for i in range(int(nof)):\n",
    "                avg += combined[i][1]\n",
    "            item_3 =  float(avg/nof)\n",
    "\n",
    "            #option 2:\n",
    "            avg = 0\n",
    "            for i in range(len(ratings)):\n",
    "                avg += ratings[i]\n",
    "            item_1 = float(avg/len(ratings))\n",
    "\n",
    "            #option 3:\n",
    "            # cosine_sim = cosine_similarity(X = transformed_list[0:-1],Y = [transformed_list[-1]])\n",
    "            # cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            # combined = zip(cosine_sim, ratings)\n",
    "            # combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "            # avg = 0\n",
    "            # nof = 10.0\n",
    "            # for i in range(int(nof)):\n",
    "            #     avg += combined[i][1]\n",
    "            # item_1 = float(avg/nof)\n",
    "\n",
    "            #option 4:\n",
    "            # cosine_sim = cosine_similarity(X = transformed_list[0:-1],Y = [transformed_list[-1]])\n",
    "            # cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            # numerator = 0\n",
    "            # denominator = 0\n",
    "            # note: item_1 is the total average here (found above)...\n",
    "            # item_2 = item_1\n",
    "            # for i in range(len(ratings)):\n",
    "            #     numerator += float(cosine_sim[i]*ratings[i])\n",
    "            #     denominator += cosine_sim[i]\n",
    "            \n",
    "            # if denominator != 0:\n",
    "            #     item_2 = float(numerator/denominator)\n",
    "        \n",
    "            return (item_1, item_2, item_3)\n",
    "        \n",
    "        \n",
    "        items = predict(word_counts_transformed, word_counts,\n",
    "                                    target_word_counts_transformed, target_word_counts, ratings)\n",
    "\n",
    "        # feature_1.append(items[0])\n",
    "        feature_1.append(items[2])\n",
    "        feature_2.append(items[0])\n",
    "            \n",
    "        \n",
    "    return overall_rating_sum, overall_rating_count\n",
    "\n",
    "\n",
    "#this has been replaced with populate feature_3\n",
    "#this is simply an average rating from all users who rated the movie...\n",
    "def load_feature_2(train_user, feature_2):\n",
    "    if(train_user):\n",
    "        #populates feature_2_train\n",
    "        for i in range(len(train_users.user_to_movie_id_to_rating)): \n",
    "            if(len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]])==1):\n",
    "                #this means that there is no other rating besides the first train rating\n",
    "                feature_2.append(overall_average_train)\n",
    "            else:\n",
    "                #omit the rating the user in question made\n",
    "                feature_2.append(float(((movie_id_to_average_rating_train[train_users.user_to_target_movie_id[i]]\n",
    "                                *len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]]))\n",
    "                                -train_users.user_to_movie_id_to_rating[i][train_users.user_to_target_movie_id[i]])\n",
    "                                /(len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]])-1)))\n",
    "    else:   \n",
    "        #populates feature_2_test\n",
    "        for i in range(len(test_users.user_to_movie_id_to_rating)):\n",
    "            if(test_users.user_to_target_movie_id[i] not in movie_id_to_ratings_total.keys()):\n",
    "                #if the movie is not in the train set make a guess\n",
    "                feature_2.append(overall_average_train)\n",
    "            else:\n",
    "                #only use the train data\n",
    "                feature_2.append(movie_id_to_average_rating_train[test_users.user_to_target_movie_id[i]])\n",
    "\n",
    "\n",
    "def pre_svd(user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, \n",
    "               user_to_movie_id_to_rating, user_to_target_movie_id):\n",
    "    for i in range(len(user_to_movie_id_to_rating)):\n",
    "        ratings = []\n",
    "        transformed_ratings = []\n",
    "        index = 0\n",
    "        #note: movies in order includes amoung all users\n",
    "        #this needs to be restricted to only train users\n",
    "        #this ways there will always be movie_id_to_average_rating_train[movie_id]...\n",
    "        #becasue there is always a train users with a rating\n",
    "        #when a test user iterates through this, the same train movies in order will be used\n",
    "        for movie_id in train_movies_in_order:\n",
    "            if movie_id == user_to_target_movie_id[i]:\n",
    "                user_to_target_index_full.append(index)\n",
    "                ratings.append(movie_id_to_average_rating_train[movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(movie_id_to_average_rating_train[movie_id] - movie_id_to_average_rating_train[movie_id])             \n",
    "            elif movie_id in user_to_movie_id_to_rating[i].keys():\n",
    "                ratings.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(user_to_movie_id_to_rating[i][movie_id] - movie_id_to_average_rating_train[movie_id])\n",
    "            else:\n",
    "                ratings.append(movie_id_to_average_rating_train[movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(movie_id_to_average_rating_train[movie_id] - movie_id_to_average_rating_train[movie_id])\n",
    "            index +=1\n",
    "        user_to_ratings_full.append(ratings)\n",
    "        user_to_ratings_full_transform.append(transformed_ratings)\n",
    "\n",
    "\n",
    "#note: before passing to this function the data is normalized about the average movie ratings (not average user ratings)\n",
    "#each user train and test users have a single rating that needs to be trained against in the train case\n",
    "#and predicted in the test case\n",
    "\n",
    "#the svd can be applied to the combined data of the train and test sets\n",
    "#both movies that the user didn't watch and movies that should be guesses are...\n",
    "#transformed to have a value of zero before svd\n",
    "\n",
    "#the movie columns are taken from the train dataset...\n",
    "#senario: suppose a test user has a rating of a movie not part of the train set and it is not the target movie (ignore it)\n",
    "#senario: suppose a test user has a rating of a movie not part of the train set and it is the target movie (guess the rating instead of using svd)\n",
    "\n",
    "#...Once the UsV is created...\n",
    "#take the rating from the new UsV for the user row and movie column for the target movie\n",
    "#other option: cossine similairty on the U ignoring other test users\n",
    "\n",
    "\n",
    "#why isn't this replaced with truncated svd???\n",
    "def svd_full(user_to_ratings_full_transform, n, movie_id_to_average_rating_train):\n",
    "    U, s, V = np.linalg.svd(user_to_ratings_full_transform, full_matrices=False)\n",
    "    \n",
    "    #adjust wiht n\n",
    "    s=np.diag(s)\n",
    "    s=s[0:n,0:n]\n",
    "    U=U[:,0:n]\n",
    "    V=V[0:n,:]\n",
    "\n",
    "    #construct a new array\n",
    "    #why use sqrtm here\n",
    "    # s_root = sqrtm(s)\n",
    "    # Usk = np.dot(U,s_root)\n",
    "    # skV = np.dot(s_root,V)\n",
    "    # UsV = np.dot(Usk, skV)\n",
    "\n",
    "    #new method:\n",
    "    Us = np.dot(U,s)\n",
    "    UsV = np.dot(Us,V)\n",
    "    \n",
    "\n",
    "\n",
    "    #the keys of movie_id_to_ratings is in the same order of movies_in_order and therefore so is movie_id_to_average_rating_train\n",
    "    x = np.tile(list(movie_id_to_average_rating_train.values()), (UsV.shape[0],1))\n",
    "    UsV = UsV + x\n",
    "\n",
    "    #be consistent with data structures...\n",
    "    return list(UsV)\n",
    "\n",
    "\n",
    "\n",
    "overall_rating_sum_train, overall_rating_count_train = load_feature_1(train_movies_in_order, user_to_data_train, movie_id_to_ratings_total, train_users.user_to_movie_id_to_rating, \n",
    "                                                         train_users.user_to_target_movie_id, train_users.user_to_target_rating, train_users.feature_1, train_users.feature_2)\n",
    "\n",
    "#this is used when there are no usable ratings for a movie\n",
    "overall_average_train = float(overall_rating_sum_train/overall_rating_count_train)\n",
    "\n",
    "load_feature_1(test_movies_in_order, user_to_data_test, dict(), test_users.user_to_movie_id_to_rating, \n",
    "               test_users.user_to_target_movie_id,\n",
    "               test_users.user_to_target_rating, test_users.feature_1, test_users.feature_2)\n",
    "\n",
    "\n",
    "#movie_id_to_average_rating_train is used by both the test and train set\n",
    "for movie in movie_id_to_ratings_total.keys():\n",
    "    temp = 0\n",
    "    for rating in movie_id_to_ratings_total[movie]:\n",
    "        temp +=rating\n",
    "    movie_id_to_average_rating_train[movie] = float(temp/len(movie_id_to_ratings_total[movie]))\n",
    "\n",
    "\n",
    "# effectiveness has been surpasssed...\n",
    "# load_feature_2(True, train_users.feature_2)\n",
    "# load_feature_2(False, test_users.feature_2)\n",
    "\n",
    "\n",
    "pre_svd(train_users.user_to_ratings_full_transform, train_users.user_to_ratings_full, train_users.user_to_target_index_full, \n",
    "               train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id)\n",
    "\n",
    "pre_svd(test_users.user_to_ratings_full_transform, test_users.user_to_ratings_full, test_users.user_to_target_index_full, \n",
    "               test_users.user_to_movie_id_to_rating, test_users.user_to_target_movie_id)\n",
    "\n",
    "\n",
    "#combine train users rating and test user ratings into a single list\n",
    "full_transform = train_users.user_to_ratings_full_transform.copy()\n",
    "full_transform.extend(test_users.user_to_ratings_full_transform)\n",
    "\n",
    "\n",
    "#this needs to be tinkered with for optimal performance...\n",
    "svd_out = svd_full(full_transform, 15, movie_id_to_average_rating_train)\n",
    "\n",
    "#this needs to be tinkered with for optimal performance...\n",
    "# svd_out = TruncatedSVD(n_components = 30, random_state = seed_int).fit_transform(full_transform)\n",
    "# x = np.tile(list(movie_id_to_average_rating_train.values()), (svd_out.shape[0],1))\n",
    "# svd_out = svd_out + x \n",
    "\n",
    "\n",
    "for i in range(len(train_users.user_to_ratings_full_transform)):\n",
    "    train_users.feature_3.append(svd_out[i][train_users.user_to_target_index_full[i]])\n",
    "for i in range(len(test_users.user_to_ratings_full_transform)):\n",
    "    test_users.feature_3.append(svd_out[i+len(train_users.user_to_ratings_full_transform)][test_users.user_to_target_index_full[i]])\n",
    "\n",
    "#should target movie ratings for each user be left as zero in full transform???\n",
    "#or only the target rating that is to be guessed and the svd_ot be remade for every \n",
    "#method 1 in use...\n",
    "\n",
    "#now cossine similaity can be used with ratings svd\n",
    "#and svd can implemented on the word counts or tf-idf for each movie \n",
    "\n",
    "#used to see what the text data looks like...\n",
    "# not applicable with dict to list change...\n",
    "# file = open(\"test_dicts_1.txt\", 'w', encoding=\"utf-8\")\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_train))\n",
    "# file.close()\n",
    "\n",
    "# file = open(\"test_dicts_2.txt\", 'w', encoding=\"utf-8\")\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_test))\n",
    "# file.close()\n",
    "\n",
    "del user_to_data_train\n",
    "del user_to_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "#what if populate 1 2 3 and 4 were combined???\n",
    "#this would mean that user_to_movie_id_to_corpus and user_to_words_in_order would be changed to temporary instead\n",
    "#to save memory...\n",
    "\n",
    "\n",
    "# def populate_2(user_to_movie_id_to_corpus, user_to_words_in_order):\n",
    "#     for i in range(len(user_to_movie_id_to_corpus)):\n",
    "#         user_to_words_in_order.append(OrderedSet())\n",
    "#         for movie_id in user_to_movie_id_to_corpus[i].keys():\n",
    "#             for word in user_to_movie_id_to_corpus[i][movie_id]:\n",
    "#                 user_to_words_in_order[i].add(word)\n",
    "\n",
    "# populate_2(train_users.user_to_movie_id_to_corpus,  train_users.user_to_words_in_order)\n",
    "# populate_2(test_users.user_to_movie_id_to_corpus,  test_users.user_to_words_in_order)\n",
    "\n",
    "\n",
    "\n",
    "# def populate_3(user_to_movie_id_to_corpus, user_to_target_movie_id, user_to_movie_id_to_rating,\n",
    "#     user_to_target_rating, user_to_ratings):\n",
    "#     for i in range(len(user_to_movie_id_to_corpus)):\n",
    "#         temp = []\n",
    "#         cnt = 0\n",
    "#         for movie_id in user_to_movie_id_to_corpus[i].keys():\n",
    "#             if movie_id != user_to_target_movie_id[i]:\n",
    "#                 temp.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "#             else:\n",
    "#                 user_to_target_rating.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "#                 temp.append(-1)\n",
    "#             cnt+=1\n",
    "#         user_to_ratings.append(temp)\n",
    "\n",
    "\n",
    "# populate_3(train_users.user_to_movie_id_to_corpus, train_users.user_to_target_movie_id, train_users.user_to_movie_id_to_rating,\n",
    "#             train_users.user_to_target_rating, train_users.user_to_ratings)\n",
    "# populate_3(test_users.user_to_movie_id_to_corpus, test_users.user_to_target_movie_id, test_users.user_to_movie_id_to_rating,\n",
    "#             test_users.user_to_target_rating, test_users.user_to_ratings)\n",
    "\n",
    "\n",
    "\n",
    "# def populate_4(user_to_word_counts_transformed, user_to_target_word_counts_transformed,\n",
    "#                 user_to_movie_id_to_corpus, user_to_word_counts,\n",
    "#                 user_to_target_movie_id, user_to_words_in_order, user_to_target_word_counts):\n",
    "#     for i in range(len(user_to_movie_id_to_corpus)):\n",
    "#         user_to_word_counts.append([])\n",
    "#         user_to_word_counts_transformed.append([])\n",
    "#         for movie_id in user_to_movie_id_to_corpus[i].keys():\n",
    "#             if movie_id != user_to_target_movie_id[i]:\n",
    "#                 temp_dict = Counter(user_to_movie_id_to_corpus[i][movie_id])\n",
    "#                 temp_list = []\n",
    "#                 avg = 0\n",
    "#                 for word in user_to_words_in_order[i]:\n",
    "#                     if word in temp_dict.keys():\n",
    "#                         temp_list.append(temp_dict[word])\n",
    "#                         avg+=temp_dict[word]\n",
    "#                     else:\n",
    "#                         temp_list.append(0)\n",
    "#                 avg = float(avg/len(user_to_words_in_order[i]))\n",
    "#                 user_to_word_counts[i].append(temp_list)\n",
    "#                 user_to_word_counts_transformed[i].append([x - avg for x in temp_list])\n",
    "#             else:\n",
    "\n",
    "#                 temp_dict = Counter(user_to_movie_id_to_corpus[i][movie_id])\n",
    "#                 temp_list = []\n",
    "#                 avg = 0\n",
    "#                 for word in user_to_words_in_order[i]:\n",
    "#                     if word in temp_dict.keys():\n",
    "#                         temp_list.append(temp_dict[word])\n",
    "#                         avg+=temp_dict[word]\n",
    "#                     else:\n",
    "#                         temp_list.append(0)\n",
    "#                 avg = float(avg/len(user_to_words_in_order[i]))\n",
    "#                 user_to_target_word_counts.append(temp_list)\n",
    "#                 user_to_target_word_counts_transformed.append([x - avg for x in temp_list])\n",
    "\n",
    "\n",
    "# populate_4(train_users.user_to_word_counts_transformed, train_users.user_to_target_word_counts_transformed,\n",
    "#            train_users.user_to_movie_id_to_corpus, train_users.user_to_word_counts, train_users.user_to_target_movie_id,\n",
    "#        train_users.user_to_words_in_order, train_users.user_to_target_word_counts)\n",
    "\n",
    "# populate_4(test_users.user_to_word_counts_transformed, test_users.user_to_target_word_counts_transformed,\n",
    "#            test_users.user_to_movie_id_to_corpus, test_users.user_to_word_counts, \n",
    "#        test_users.user_to_target_movie_id, test_users.user_to_words_in_order, test_users.user_to_target_word_counts)\n",
    "\n",
    "# del train_users.user_to_words_in_order\n",
    "# del train_users.user_to_movie_id_to_corpus\n",
    "# del test_users.user_to_words_in_order\n",
    "# del test_users.user_to_movie_id_to_corpus\n",
    "\n",
    "\n",
    "\n",
    "#now for each user, use the user_to_index_full and find the most similair users by omitting that index across the\n",
    "#sim_matrix\n",
    "\n",
    "\n",
    "\n",
    "#collaboritive filtering idea\n",
    "#data structures:\n",
    "#need a user to movies to ratings dictionary (done) (only needed for train data)\n",
    "#need a ordered set of all movies (done) (only needed for train data)\n",
    "\n",
    "#need to transform it into a user to list of all movies with user ratings or otherwise filled in ratings with sutiable average\n",
    "#(the list needs to be in a consistent order across users)\n",
    "#to fill in the averages we need a movie to average rating dictionary\n",
    "#(note: if no other user has rated the movie then fill it in with the overall movie average)\n",
    "#standardize the data row wise (why not columnwise???)\n",
    "#filter the users that have rated the movie to predict\n",
    "#then use cossine similairity on this set to find the most similair user the the chosen user\n",
    "#ignore the movie rating to predict with the cossine similarity function\n",
    "\n",
    "#Note: this is an expensive task and the number of users may have to be truncated before running\n",
    "\n",
    "\n",
    "\n",
    "#ideas: \n",
    "#idea1: \n",
    "#collaborative filtering:\n",
    "#https://towardsdatascience.com/predict-movie-ratings-with-user-based-collaborative-filtering-392304b988af\n",
    "#https://www.geeksforgeeks.org/user-based-collaborative-filtering/#\n",
    "#https://www.youtube.com/watch?v=3ecNC-So0r4&ab_channel=CodeHeroku\n",
    "#idea2: \n",
    "#note: the model can be scored based on how close a predition is to a threshold of .5 \n",
    "\n",
    "#idea 3:\n",
    "#there may be a replacement for cossine similarity:\n",
    "#perhaps it is not properly matching similair people and similair movies \n",
    "#maybe there is a replacement function\n",
    "\n",
    "#idea 4: \n",
    "#there should be more train and test users\n",
    "\n",
    "#idea 5: (basic model)\n",
    "#https://www.kaggle.com/code/muhammadayman/recommendation-system-using-cosine-similarity#Data-Cleaning\n",
    "\n",
    "\n",
    "\n",
    "#10 minutes for 2500 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "# def predict(user, user_to_word_counts_transformed, user_to_target_word_counts_transformed,\n",
    "#             user_to_target_word_counts, user_to_word_counts, user_to_ratings):\n",
    "\n",
    "\n",
    "#     cosine_sim = cosine_similarity(X = user_to_word_counts_transformed[user] ,Y = [user_to_target_word_counts_transformed[user]])\n",
    "    \n",
    "#     cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "#     ratings = [user_to_ratings[user][x] for x in range(len(user_to_ratings[user])) if user_to_ratings[user][x] != -1]\n",
    "\n",
    "\n",
    "#     #use the movie thats are most similair to the movie in question \n",
    "\n",
    "#     #option 1: \n",
    "#     # combined = zip(cosine_sim, ratings)\n",
    "#     # combined = sorted(combined, key=lambda x: x[0], reverse=False)\n",
    "#     # avg = 0\n",
    "#     # nof = 10.0\n",
    "#     # for i in range(int(nof)):\n",
    "#     #     avg += combined[i][1]\n",
    "#     # return float(avg/nof)\n",
    "\n",
    "#     #option 2:\n",
    "#     # avg = 0\n",
    "#     # for i in range(len(ratings)):\n",
    "#     #     avg += ratings[i]\n",
    "#     # return float(avg/len(ratings))\n",
    "\n",
    "#     #option 3: \n",
    "#     combined = zip(cosine_sim, ratings)\n",
    "#     combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "#     avg = 0\n",
    "#     nof = 10.0\n",
    "#     for i in range(int(nof)):\n",
    "#         avg += combined[i][1]\n",
    "#     return float(avg/nof)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def populate_5(user_to_word_counts_transformed, user_to_target_word_counts_transformed,\n",
    "#                user_to_movie_id_to_rating, feature_1,\n",
    "#                 user_to_target_word_counts, user_to_word_counts,\n",
    "#                 user_to_ratings):\n",
    "#     for i in range(len(user_to_movie_id_to_rating)):\n",
    "#         feature_1.append(predict(i, user_to_word_counts_transformed, user_to_target_word_counts_transformed,\n",
    "#                                   user_to_target_word_counts, user_to_word_counts, user_to_ratings))\n",
    "\n",
    "\n",
    "\n",
    "# populate_5(train_users.user_to_word_counts_transformed, train_users.user_to_target_word_counts_transformed,\n",
    "#            train_users.user_to_movie_id_to_rating, train_users.feature_1,\n",
    "#                 train_users.user_to_target_word_counts, train_users.user_to_word_counts,\n",
    "#                 train_users.user_to_ratings)\n",
    "\n",
    "# populate_5(test_users.user_to_word_counts_transformed, test_users.user_to_target_word_counts_transformed,\n",
    "#            test_users.user_to_movie_id_to_rating, test_users.feature_1,\n",
    "#                 test_users.user_to_target_word_counts, test_users.user_to_word_counts,\n",
    "#                 test_users.user_to_ratings)\n",
    "\n",
    "# del train_users.user_to_word_counts\n",
    "# del test_users.user_to_word_counts\n",
    "# del train_users.user_to_word_counts_transformed\n",
    "# del test_users.user_to_word_counts_transformed\n",
    "# del train_users.user_to_target_word_counts\n",
    "# del test_users.user_to_target_word_counts\n",
    "# del train_users.user_to_target_word_counts_transformed\n",
    "# del test_users.user_to_target_word_counts_transformed\n",
    "# del train_users.user_to_ratings\n",
    "# del test_users.user_to_ratings\n",
    "\n",
    "\n",
    "#note: for some reason feature 2 out performs feature 3 as a feature...\n",
    "\n",
    "# for i in range(len(train_users.user_to_movie_id_to_rating)): \n",
    "#     if(len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]])==1):\n",
    "#         train_users.feature_2.append(overall_average_train)\n",
    "#     else:\n",
    "#         train_users.feature_2.append(float(((movie_id_to_average_rating_train[train_users.user_to_target_movie_id[i]]\n",
    "#                         *len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]]))\n",
    "#                         -train_users.user_to_movie_id_to_rating[i][train_users.user_to_target_movie_id[i]])\n",
    "#                         /(len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]])-1)))\n",
    "\n",
    "\n",
    "# for i in range(len(test_users.user_to_movie_id_to_rating)):\n",
    "#     if(test_users.user_to_target_movie_id[i] not in movie_id_to_ratings_total.keys()):\n",
    "#         test_users.feature_2.append(overall_average_train)\n",
    "#     else:\n",
    "#         test_users.feature_2.append(movie_id_to_average_rating_train[test_users.user_to_target_movie_id[i]])\n",
    "\n",
    "\n",
    "\n",
    "# def populate_6(user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, \n",
    "#                user_to_movie_id_to_rating, user_to_target_movie_id):\n",
    "#     for i in range(len(user_to_movie_id_to_rating)):\n",
    "#         ratings = []\n",
    "#         index = 0\n",
    "#         for movie_id in movies_in_order:\n",
    "#             if movie_id == user_to_target_movie_id[i]:\n",
    "#                 user_to_target_index_full.append(index)\n",
    "#             if movie_id in user_to_movie_id_to_rating[i].keys():\n",
    "#                 ratings.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "#             elif movie_id in movie_id_to_average_rating_train.keys():\n",
    "#                 ratings.append(movie_id_to_average_rating_train[movie_id])\n",
    "#             else:   \n",
    "#                 ratings.append(overall_average_train)\n",
    "#             index +=1\n",
    "#         user_to_ratings_full.append(ratings)\n",
    "#         user_to_ratings_full_transform.append([x - user_to_average_rating[i] for x in ratings])\n",
    "\n",
    "\n",
    "# populate_6(train_users.user_to_ratings_full_transform, train_users.user_to_ratings_full, train_users.user_to_target_index_full, train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id)\n",
    "# populate_6(test_users.user_to_ratings_full_transform, test_users.user_to_ratings_full, test_users.user_to_target_index_full, test_users.user_to_movie_id_to_rating, test_users.user_to_target_movie_id)\n",
    "\n",
    "\n",
    "# del train_users.user_to_movie_id_to_rating\n",
    "# del test_users.user_to_movie_id_to_rating\n",
    "# del train_users.user_to_target_movie_id\n",
    "# del test_users.user_to_target_movie_id\n",
    "\n",
    "#why is feature 3 outperformed by feature 2 ???\n",
    "\n",
    "# def populate_7(user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, feature_3):\n",
    "#     for i in range(len(user_to_ratings_full)):\n",
    "#         list_of_list_of_ratings = []\n",
    "#         sample_ratings = []\n",
    "#         for j in range(len(train_users.user_to_ratings_full)):\n",
    "#             if i != j:\n",
    "#                 sample_ratings.append(train_users.user_to_ratings_full[j][user_to_target_index_full[i]])\n",
    "#                 ratings = [train_users.user_to_ratings_full_transform[j][x] \n",
    "#                           for x \n",
    "#                           in range(len(train_users.user_to_ratings_full_transform[j])) \n",
    "#                           if x != user_to_target_index_full[i]]\n",
    "#                 list_of_list_of_ratings.append(ratings)\n",
    "\n",
    "#         user_ratings = [user_to_ratings_full_transform[i][x] \n",
    "#                     for x \n",
    "#                     in range(len(user_to_ratings_full_transform[i])) \n",
    "#                     if x != user_to_target_index_full[i]]\n",
    "\n",
    "\n",
    "#         sim  = cosine_similarity(X = list_of_list_of_ratings, Y = [user_ratings])\n",
    "#         sim = np.reshape(sim,  (len(sim)))\n",
    "\n",
    "#         combined = zip(sim, sample_ratings)\n",
    "#         combined = sorted(combined, key=lambda x: x[0], reverse = True)\n",
    "\n",
    "#         avg = 0\n",
    "#         nof = 10.0\n",
    "#         for k in range(int(nof)):\n",
    "#             avg+= combined[k][1]\n",
    "\n",
    "#         feature_3.append(float(avg/nof))\n",
    "    \n",
    "\n",
    "# populate_7(train_users.user_to_ratings_full_transform, train_users.user_to_ratings_full, train_users.user_to_target_index_full, train_users.feature_3)\n",
    "# populate_7(test_users.user_to_ratings_full_transform, test_users.user_to_ratings_full, test_users.user_to_target_index_full, test_users.feature_3)\n",
    "\n",
    "# del train_users.user_to_ratings_full_transform\n",
    "# del test_users.user_to_ratings_full_transform\n",
    "# del train_users.user_to_ratings_full\n",
    "# del test_users.user_to_ratings_full\n",
    "# del train_users.user_to_target_index_full\n",
    "# del test_users.user_to_target_index_full\n",
    "\n",
    "\n",
    "#adjusted cossine simlairity\n",
    "#https://stackoverflow.com/questions/40716459/choice-between-an-adjusted-cosine-similarity-vs-regular-cosine-similarity\n",
    "#https://github.com/csaluja/JupyterNotebooks-Medium/blob/master/CF%20Recommendation%20System-Examples.ipynb?source=post_page-----ecbffe1c20b1--------------------------------\n",
    "#https://towardsdatascience.com/collaborative-filtering-based-recommendation-systems-exemplified-ecbffe1c20b1\n",
    "\n",
    "\n",
    "#note: in the item-based collabortive filtering the means is implementaed item-wise\n",
    "#this means with user-based collaboritve filtering the mean is implementad user-wise\n",
    "\n",
    "\n",
    "#idea 1:\n",
    "#use diferrent data structures...\n",
    "#numpy arrays...\n",
    "#https://www.geeksforgeeks.org/python-lists-vs-numpy-arrays/#\n",
    "#https://stackoverflow.com/questions/29839350/numpy-append-vs-python-append\n",
    "#https://www.geeksforgeeks.org/python-convert-list-to-python-array/\n",
    "\n",
    "\n",
    "#idea 2:\n",
    "#make use of both feature_2 and feature_3\n",
    "\n",
    "#idea 3: would it be possible to convert many of the main dictionaries to lists or numpy arrays\n",
    "#note: numpy arrays cant have variable number of items for a dimension\n",
    "#but this can be over come by filling missing values\n",
    "#this mean many matrices would be sparse\n",
    "#there is also alot of appending to lists which is better for lists\n",
    "\n",
    "\n",
    "#idea 4: would it be helpful to design my own data structures???\n",
    "\n",
    "#idea 5: would it be helpful to have features like the average 10 closest in similairy and the 10 farthest in similarity???\n",
    "\n",
    "#idea 6: before trying this there could be better data strcuture to use that would speed up the process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04983553 0.44310513]\n",
      "[0.05961145 0.24973145]\n",
      "[0.06146318 0.4152224 ]\n",
      "[0.05025121 0.4183106 ]\n",
      "[0.05854967 0.41252183]\n",
      "[0.04772123 0.4318004 ]\n",
      "[0.05064246 0.40956692]\n",
      "[0.04867467 0.38275281]\n",
      "[0.05609728 0.40301532]\n",
      "[0.0481132  0.41174102]\n",
      "[0.05405305 0.41564646]\n",
      "[0.04972105 0.39293826]\n",
      "[0.04739247 0.42014916]\n",
      "[0.04980514 0.41787758]\n",
      "[0.04651595 0.39980803]\n",
      "[0.05661721 0.40149823]\n",
      "[0.05016928 0.39267967]\n",
      "[0.0477886  0.22264341]\n",
      "[0.05172902 0.42272772]\n",
      "[0.04829984 0.37363225]\n",
      "(0.19957016757465737, 0.17354885234495593)\n",
      "[4.2, 4.2, 4.0, 2.7, 3.7, 3.8, 4.0, 4.0, 3.8, 3.1, 3.2, 3.4, 3.6, 3.6, 3.2, 4.0, 2.9, 4.0, 4.1, 3.2, 2.4, 3.6, 3.6, 3.8, 3.4, 3.2, 2.8, 4.8, 3.9, 4.6, 4.4, 4.2, 2.9, 3.4, 2.1, 3.3, 3.6, 4.6, 3.4, 3.7, 3.7, 3.4, 4.0, 3.0, 4.6, 4.7, 3.4, 2.7, 4.6, 4.4, 3.7, 4.2, 4.2, 3.4, 3.6, 4.6, 3.0, 3.9, 3.7, 1.8, 4.4, 3.8, 4.4, 3.0, 3.0, 3.4, 2.8, 3.9, 4.8, 4.7, 4.4, 3.6, 3.6, 3.2, 3.7, 3.4, 3.0, 4.2, 2.8, 3.6, 4.0, 4.4, 4.0, 3.5, 3.5, 3.8, 3.4, 3.4, 5.0, 4.4, 4.3, 3.0, 3.6, 3.8, 2.8, 2.8, 4.5, 3.9, 3.6, 3.9, 3.6, 3.3, 3.5, 3.8, 3.0, 2.8, 2.9, 3.8, 3.4, 3.6, 4.4, 2.9, 3.4, 3.6, 3.7, 3.6, 4.3, 4.6, 4.7, 3.2, 4.0, 4.0, 3.2, 2.9, 2.6, 3.9, 4.0, 3.4, 2.8, 3.1, 4.6, 3.2, 3.4, 2.3, 3.0, 4.5, 3.2, 3.1, 4.0, 3.8, 3.8, 3.6, 3.4, 3.8, 3.4, 3.8, 3.3, 2.7, 3.9, 4.4, 4.4, 3.2, 3.6, 3.3, 3.1, 3.6, 4.5, 3.4, 3.8, 3.1, 3.5, 4.7, 3.8, 2.8, 3.8, 4.3, 3.7, 3.6, 3.4, 4.4, 4.8, 4.0, 4.0, 3.4, 4.2, 3.8, 3.3, 3.8, 3.6, 4.2, 3.8, 4.2, 3.8, 3.8, 3.4, 4.1, 4.2, 3.2, 2.8, 3.0, 4.6, 3.7, 3.2, 3.3, 4.2, 3.4, 3.3, 3.4, 3.3, 3.4, 3.0, 3.4, 3.8, 3.4, 2.8, 3.1, 4.2, 2.9, 4.2, 3.8, 4.3, 4.6, 3.4, 4.2, 3.0, 2.8, 4.2, 3.8, 3.8, 4.0, 3.0, 3.7, 3.6, 3.6, 2.8, 3.2, 3.2, 2.3, 3.4, 4.0, 4.9, 2.6, 3.6, 3.7, 3.8, 3.5, 1.8, 4.1, 2.7, 3.7, 4.3, 4.3, 2.5, 4.7, 3.2, 4.8, 3.8, 3.8, 3.2, 4.1, 3.1, 4.0, 3.5, 4.7, 3.8, 4.5, 3.4, 3.8, 4.2, 3.4, 3.8, 3.8, 4.0, 3.9, 3.5, 4.2, 4.0, 4.4, 4.0, 4.1, 4.9, 3.4, 4.2, 3.7, 3.6, 4.3, 3.0, 3.4, 3.8, 3.9, 4.0, 3.4, 4.4, 3.9, 3.6, 3.6, 4.4, 3.9, 4.2, 4.0, 3.7, 3.8, 2.6, 3.6, 3.6, 3.8, 3.6, 3.7, 2.8, 4.1, 4.3, 3.6, 3.8, 2.8, 4.1, 4.0, 3.7, 3.8, 2.6, 4.2, 4.0, 3.5, 3.5, 4.0, 3.9, 4.6, 3.4, 3.2, 3.8, 3.2, 3.8, 2.8, 2.6, 3.2, 4.0, 3.5, 3.1, 3.2, 2.9, 3.0, 4.3, 3.7, 4.4, 3.7, 2.6, 4.2, 3.6, 4.0, 3.0, 2.7, 3.2, 3.6, 3.3, 3.1, 3.6, 3.0, 4.2, 4.6, 4.0, 3.6, 3.0, 4.6, 4.2, 2.6, 2.6, 3.4, 3.4, 3.8, 4.8, 4.2, 2.8, 3.1, 2.4, 1.6, 4.3, 3.1, 2.8, 3.5, 3.8, 3.0, 3.4, 3.7, 4.0, 3.8, 3.6, 3.7, 2.2, 4.4, 3.6, 3.9, 3.4, 4.0, 4.2, 3.9, 3.6, 4.2, 3.3, 1.9, 2.4, 3.4, 3.9, 4.2, 3.4, 4.5, 3.0, 3.0, 3.8, 3.2, 4.2, 4.3, 3.0, 3.7, 4.0, 4.0, 3.0, 4.7, 3.0, 3.2, 4.0, 3.4, 4.2, 3.2, 3.0, 4.0, 3.6, 3.7, 3.8, 3.3, 4.0, 3.2, 3.4, 3.4, 4.0, 4.6, 3.8, 4.2, 3.0, 3.4, 3.0, 4.4, 4.6, 3.4, 3.2, 3.4, 3.4, 3.0, 2.8, 2.6, 3.9, 3.6, 3.4, 3.8, 4.3, 3.9, 3.6, 4.0, 2.8, 3.4, 4.0, 4.2, 3.6, 3.6, 3.6, 1.3, 3.4, 3.8, 3.6, 3.0, 3.9, 3.0, 4.2, 4.0, 3.9, 3.4, 2.8, 3.3, 4.1, 4.0, 3.0, 3.2, 4.4, 3.4, 3.9, 4.0, 2.8, 3.8, 3.4, 3.2, 4.0, 3.0, 3.8, 3.4, 4.2, 4.4, 3.6, 3.9, 3.7, 4.0, 3.8, 3.6, 3.7, 3.8, 2.9, 4.4, 3.8, 4.4, 3.3, 3.6, 3.3, 3.0]\n",
      "[2.75, 4.6461538461538465, 3.6481481481481484, 2.9508196721311477, 3.292452830188679, 3.490566037735849, 3.9019607843137254, 3.5172413793103448, 3.890909090909091, 3.314814814814815, 3.1153846153846154, 3.4714285714285715, 3.970149253731343, 3.6666666666666665, 3.2115384615384617, 3.717391304347826, 3.4017857142857144, 3.9863013698630136, 4.320754716981132, 3.0925925925925926, 2.8728813559322033, 4.092592592592593, 3.710144927536232, 3.82, 3.6315789473684212, 3.278688524590164, 2.9423076923076925, 4.517241379310345, 3.816666666666667, 4.3, 3.535211267605634, 4.054545454545455, 2.873134328358209, 3.7045454545454546, 2.425, 3.381818181818182, 3.8098591549295775, 4.140845070422535, 3.806451612903226, 3.8392857142857144, 3.7535211267605635, 3.263157894736842, 3.510204081632653, 3.0294117647058822, 3.796875, 4.270491803278689, 2.7642857142857142, 3.55, 3.96875, 4.042253521126761, 3.790909090909091, 4.09433962264151, 3.7704918032786887, 3.5, 3.3469387755102042, 3.9726027397260273, 3.235294117647059, 3.8035714285714284, 3.8046875, 2.7, 4.035087719298246, 3.7346938775510203, 4.083333333333333, 3.2777777777777777, 3.559322033898305, 3.318840579710145, 3.6065573770491803, 3.767123287671233, 4.526785714285714, 4.2, 4.533898305084746, 3.7058823529411766, 3.3863636363636362, 3.4571428571428573, 4.078431372549019, 3.546875, 3.2313432835820897, 4.029411764705882, 2.5, 3.3934426229508197, 3.443548387096774, 4.049180327868853, 3.8620689655172415, 3.327272727272727, 3.5681818181818183, 4.0588235294117645, 3.8333333333333335, 3.2155172413793105, 4.75, 3.4776119402985075, 3.878787878787879, 3.491228070175439, 3.2054794520547945, 3.5918367346938775, 3.1363636363636362, 3.6470588235294117, 3.7049180327868854, 3.6475409836065573, 3.43859649122807, 3.4864864864864864, 3.9298245614035086, 3.757575757575758, 3.6640625, 3.8114754098360657, 3.56, 3.1818181818181817, 3.3257575757575757, 3.510204081632653, 3.2642857142857142, 3.7333333333333334, 4.066666666666666, 3.1538461538461537, 3.3230769230769233, 3.825, 3.879032258064516, 3.590909090909091, 4.083333333333333, 4.402985074626866, 4.133928571428571, 3.6666666666666665, 4.022388059701493, 4.275862068965517, 3.330508474576271, 2.857142857142857, 2.7169811320754715, 3.3846153846153846, 3.576923076923077, 3.4909090909090907, 3.436619718309859, 3.481818181818182, 4.304347826086956, 3.140845070422535, 3.473684210526316, 2.6610169491525424, 3.3076923076923075, 4.343283582089552, 3.7205882352941178, 3.3671875, 4.01, 3.508771929824561, 3.515625, 3.6610169491525424, 3.8333333333333335, 3.795918367346939, 3.2661290322580645, 3.7083333333333335, 3.3545454545454545, 2.9930555555555554, 3.618181818181818, 4.068965517241379, 4.2075471698113205, 3.721311475409836, 3.5955882352941178, 3.349056603773585, 3.44, 3.75, 3.75, 3.3389830508474576, 3.7, 2.6610169491525424, 3.92, 4.351851851851852, 3.8360655737704916, 3.3636363636363638, 3.411764705882353, 3.652173913043478, 3.951388888888889, 3.5205479452054793, 3.5753424657534247, 4.111111111111111, 4.166666666666667, 4.046153846153846, 3.6818181818181817, 3.1194029850746268, 3.778688524590164, 3.963768115942029, 3.4714285714285715, 3.303030303030303, 3.801470588235294, 4.035714285714286, 3.564516129032258, 4.453125, 3.3469387755102042, 3.6296296296296298, 3.3013698630136985, 4.072727272727272, 3.7413793103448274, 3.63265306122449, 3.625, 3.5538461538461537, 4.3061224489795915, 3.75, 3.2857142857142856, 3.4642857142857144, 3.9193548387096775, 3.5510204081632653, 3.5, 3.4925373134328357, 4.115942028985507, 3.66, 2.8333333333333335, 3.033333333333333, 4.078125, 3.8275862068965516, 3.4794520547945207, 3.22, 4.176470588235294, 2.982456140350877, 3.7580645161290325, 3.764705882352941, 4.2, 3.8947368421052633, 3.146153846153846, 4.345454545454546, 3.090909090909091, 2.9375, 4.074074074074074, 3.876923076923077, 3.9454545454545453, 4.031746031746032, 3.2419354838709675, 3.4682539682539684, 3.6944444444444446, 3.7037037037037037, 3.377049180327869, 3.550724637681159, 2.7846153846153845, 3.274193548387097, 3.736111111111111, 3.7183098591549295, 3.6475409836065573, 3.018181818181818, 3.4672131147540983, 4.016393442622951, 3.6140350877192984, 3.576923076923077, 2.780701754385965, 3.792452830188679, 3.225, 3.2384615384615385, 4.392857142857143, 3.990566037735849, 3.3518518518518516, 4.052631578947368, 3.1864406779661016, 3.923076923076923, 3.507936507936508, 3.5517241379310347, 3.453125, 3.8983050847457625, 3.7941176470588234, 3.9056603773584904, 3.65, 4.532258064516129, 3.7884615384615383, 4.254716981132075, 3.48, 3.642857142857143, 3.753846153846154, 3.6, 4.015625, 3.875, 3.652173913043478, 3.462686567164179, 4.065573770491803, 3.5483870967741935, 4.038461538461538, 4.28, 3.8653846153846154, 3.6384615384615384, 4.071428571428571, 3.442857142857143, 4.169811320754717, 3.3529411764705883, 3.6384615384615384, 3.1666666666666665, 3.75, 3.240740740740741, 3.870967741935484, 4.076923076923077, 3.185185185185185, 3.2142857142857144, 4.175438596491228, 3.2982456140350878, 3.6122448979591835, 3.52, 4.2745098039215685, 3.7735849056603774, 4.438356164383562, 3.609375, 3.142857142857143, 3.5901639344262297, 3.5277777777777777, 3.445945945945946, 3.8174603174603177, 3.4210526315789473, 3.42, 3.608695652173913, 3.435483870967742, 3.9, 3.9615384615384617, 3.5849056603773586, 3.5652173913043477, 3.042372881355932, 4.36986301369863, 3.7755102040816326, 3.6027397260273974, 3.230769230769231, 3.56, 3.942622950819672, 3.7358490566037736, 3.205357142857143, 3.4827586206896552, 4.159420289855072, 4.21, 4.009090909090909, 3.8035714285714284, 3.5789473684210527, 3.204081632653061, 3.5652173913043477, 3.8676470588235294, 3.0945945945945947, 3.2714285714285714, 3.1691176470588234, 4.032786885245901, 3.6517857142857144, 3.0873015873015874, 3.4722222222222223, 3.4444444444444446, 2.857142857142857, 4.246153846153846, 3.9661016949152543, 4.204081632653061, 3.2966101694915255, 3.0, 4.068965517241379, 3.836734693877551, 3.8076923076923075, 3.3181818181818183, 3.017857142857143, 3.081081081081081, 3.5344827586206895, 3.605769230769231, 3.4444444444444446, 3.52, 3.6129032258064515, 3.8333333333333335, 4.091836734693878, 3.547945205479452, 3.7580645161290325, 2.8333333333333335, 4.086206896551724, 4.378787878787879, 3.1095890410958904, 3.389705882352941, 3.5522388059701493, 3.73972602739726, 4.333333333333333, 3.9057971014492754, 3.683333333333333, 3.433333333333333, 3.9363636363636365, 2.727272727272727, 3.2642857142857142, 4.018867924528302, 2.830188679245283, 3.1454545454545455, 4.081818181818182, 4.220338983050848, 2.9047619047619047, 3.727272727272727, 3.608108108108108, 3.945945945945946, 3.9215686274509802, 3.076923076923077, 3.8835616438356166, 2.9152542372881354, 3.2758620689655173, 4.186440677966102, 4.138888888888889, 3.4210526315789473, 4.5, 3.830357142857143, 3.82, 3.4915254237288136, 3.6301369863013697, 3.0166666666666666, 2.009433962264151, 2.3979591836734695, 3.4210526315789473, 4.175925925925926, 3.5833333333333335, 3.3703703703703702, 3.962686567164179, 3.0961538461538463, 3.1176470588235294, 3.875, 3.4655172413793105, 3.754385964912281, 3.982456140350877, 3.138888888888889, 4.2727272727272725, 3.94, 3.8979591836734695, 3.2857142857142856, 3.808333333333333, 3.6481481481481484, 3.1538461538461537, 4.0, 3.6, 3.8333333333333335, 3.71875, 2.8771929824561404, 3.588235294117647, 3.9523809523809526, 4.0673076923076925, 3.96, 3.26027397260274, 3.696078431372549, 3.033333333333333, 3.5538461538461537, 3.0, 4.098360655737705, 4.159420289855072, 3.6691176470588234, 4.113207547169812, 3.2459016393442623, 3.526315789473684, 3.0153846153846153, 3.8365384615384617, 4.445205479452055, 3.269230769230769, 3.6166666666666667, 3.203125, 3.467741935483871, 3.0338983050847457, 3.5428571428571427, 3.0754716981132075, 3.7767857142857144, 3.935185185185185, 3.4057971014492754, 4.0, 3.984375, 3.857142857142857, 3.3653846153846154, 3.6779661016949152, 3.0833333333333335, 3.925925925925926, 3.5510204081632653, 3.4210526315789473, 4.090163934426229, 3.27027027027027, 3.30188679245283, 1.7727272727272727, 3.1470588235294117, 3.9315068493150687, 3.410958904109589, 3.2685185185185186, 3.7448979591836733, 3.8620689655172415, 3.5384615384615383, 3.967741935483871, 3.7941176470588234, 3.880281690140845, 3.1982758620689653, 3.3219178082191783, 4.1938775510204085, 3.443548387096774, 2.8392857142857144, 3.0793650793650795, 4.196078431372549, 3.7681159420289854, 3.8365384615384617, 3.6031746031746033, 2.8833333333333333, 3.7704918032786887, 3.3615384615384616, 3.5671641791044775, 3.4237288135593222, 3.2941176470588234, 2.8620689655172415, 3.316666666666667, 4.266666666666667, 3.9444444444444446, 3.3454545454545452, 4.007142857142857, 3.8852459016393444, 3.5416666666666665, 4.111111111111111, 3.8173076923076925, 3.63265306122449, 3.68, 3.0714285714285716, 3.857142857142857, 3.5510204081632653, 3.5942028985507246, 3.2758620689655173, 3.6180555555555554, 3.361111111111111, 3.0925925925925926]\n",
      "[3.8872473692436462, 3.937579331446078, 3.895663860493965, 3.179567848392258, 3.421775134961931, 3.808966507752627, 2.5187642013210607, 3.7487200051129257, 4.14680368524482, 3.4630537957078595, 2.8679981173807367, 3.5236877491356853, 3.9106161719812707, 3.554989270469364, 3.1125487523771014, 3.608677610367269, 3.387921545793045, 4.120070953090483, 3.371974359198175, 3.151185215269758, 3.5677316119466713, 4.358234113427891, 3.7136605156152096, 2.8642098217539207, 3.216177521052635, 4.093346958391726, 3.5129418535817942, 4.1984950291982805, 3.549172515352859, 3.1861326419434253, 3.7422019936170865, 3.7949335833145974, 3.068432879901309, 3.637986552900759, 3.8410207153767795, 3.8434989459833555, 3.97558664218405, 4.701234256808749, 3.713122607673019, 3.7887509619698068, 3.988369715690229, 3.4823093583545863, 4.2435357455644995, 3.236842940838634, 4.344605676052141, 3.9528747411731504, 3.7552159352446752, 3.3467419059910224, 4.05295819985605, 2.7489844774620185, 4.056891786696024, 2.9237719373804962, 3.3994453430384786, 3.9449937829069466, 2.833426029644235, 3.987436477855567, 4.164436177230302, 3.887317739844323, 4.454677524263022, 2.8353654746694477, 4.432691796889958, 3.622590916043535, 3.8858774512783945, 3.29366234711138, 3.2306620496564493, 2.5011416583711608, 4.392520827521002, 2.784373459613422, 3.9388589865211814, 4.0592503039337915, 4.072303423462042, 3.2413521513349615, 3.04199920487497, 3.208174047318463, 3.951626962561215, 3.4346880442233245, 3.0269019922717466, 3.765548299249875, 2.823434863812703, 3.241477833478209, 3.8291370827339146, 3.158120208646948, 2.2721958340139774, 3.083179089615138, 3.410769112596483, 3.9658949761349587, 3.310696585556004, 3.0065481097659994, 2.53075259313048, 4.010222956916199, 3.646667134301929, 3.7846016052243336, 3.379380207883795, 3.578934861417335, 3.6084149754519204, 4.2858174059065774, 3.558337544008117, 2.8271149480782194, 3.3743220800829614, 4.340001220234763, 3.815737468331876, 4.017879200177465, 4.255134722608563, 3.5412951894670806, 3.880827245264339, 3.6879566560066146, 4.4070118086984165, 4.159217509673969, 3.078612506925874, 4.14006695380773, 4.417825986657215, 3.9090953938934767, 3.4952213544215955, 3.845865870593218, 2.8110909012450422, 3.717513029440149, 3.700621199016107, 3.82286382228056, 3.4745469944825103, 3.824073916387537, 3.9632019472835456, 2.650550703456492, 3.235460527934169, 3.3526150677611453, 3.7101871775349107, 3.4724941065984254, 3.3576075949936395, 3.5790707103229438, 3.125760974949518, 1.9224820970176286, 3.8694013240197744, 3.9001851978389883, 3.581310211045259, 4.199877311512059, 3.9633061565720586, 3.983287989401868, 3.4864130688883694, 3.8089523342305527, 3.54052961035376, 3.4262216774092042, 3.227791017312803, 3.454197458417749, 4.252805484857526, 3.607082191572785, 3.551180340477875, 4.136195059704076, 3.8917945259929247, 2.7844266215266593, 4.169480726009475, 3.6092770003519017, 3.9787446986252175, 4.01747746770331, 3.614076249239982, 3.542688919122316, 3.303965305126652, 3.9417586229578694, 3.835676716517244, 4.291969071741074, 4.00612226426774, 3.7314123050510464, 3.957176360335105, 4.48579457539107, 3.893333601773087, 3.8637453666610195, 3.7736597486811756, 3.7503211503328853, 3.9351415119084945, 3.853813683092458, 4.0286695732944775, 3.695437607157865, 3.918218728249611, 5.0, 4.109028802745322, 3.2987481176779836, 3.5388409641545286, 3.453201753295624, 3.991544042842295, 3.7211598716448018, 2.1420615818194855, 3.941896524229577, 4.50288676720902, 3.5425465596787538, 3.2005604051706427, 3.8681870509708927, 3.901518609690615, 3.4262757068558165, 3.853022808139663, 2.543832438908496, 3.324039996786061, 4.5303187657722885, 4.289598486019577, 4.0161817047082025, 3.4198289283559165, 3.800293212245122, 4.02615678011843, 3.7077962505004396, 4.151311691232372, 3.054173352898353, 3.8568575863717447, 3.820088759188878, 3.4828771302101535, 3.175244015825557, 3.3133363397672064, 3.3040195852915293, 3.210141098288645, 4.125138979030772, 4.330292858105422, 3.14171301651392, 4.034225867675668, 4.4675871010133115, 3.9579452569497464, 3.6893095087058647, 3.3651686482290613, 4.225118389829428, 3.5175623681683224, 3.5098982198349393, 4.014796933621055, 4.223865730642176, 3.613341863323683, 3.9306548235515795, 3.894619852352858, 3.3127339049643907, 4.137958535704813, 3.4866022842450968, 3.18725156476235, 2.7378643562686182, 3.1209667188650343, 3.3318005243577806, 3.279182850832523, 3.991298211552193, 3.9448476098682046, 3.4969440204788587, 3.036722232258937, 3.395192806318263, 3.2848162075956484, 3.5311208137899546, 3.7494228840206723, 4.4990298916939695, 3.5276700124349207, 3.7695444856931926, 3.9119500254130046, 3.564526476030401, 3.5083739443297115, 3.2509193510065186, 3.729986059880144, 2.789977853478745, 2.909178553829351, 3.7353803435479564, 3.861581487903872, 3.3569872991225025, 3.935548332441297, 3.686891144444846, 3.58245729651337, 4.5389643492801035, 3.828425926838619, 4.246258817442051, 3.9954482956903554, 3.8523157443768516, 3.9006821362421604, 3.5522599459917075, 4.007102166530126, 4.316416997883576, 3.934725831596503, 3.9532255034104433, 3.5050017262224133, 2.9982995039736777, 3.703858089948976, 4.194710458591415, 3.627617815571822, 3.192277551729729, 3.4819390715303067, 3.8627698451349253, 3.794206472452843, 3.5675005828602537, 3.090431717484242, 3.5368019399093997, 4.6187454987555325, 3.659702941045711, 3.9512809182089885, 2.8278594145261398, 2.9983152925416507, 2.9650953886688045, 4.034549198210686, 3.206825612997276, 3.574227007909253, 3.759434735038147, 3.5415259841898745, 3.8723041897646002, 2.8118146731147107, 3.4724582113412543, 3.6685591056352247, 3.4358062567509347, 2.8592976932736835, 3.606550507252246, 3.4348155916634444, 3.6752456624750174, 4.000900925599838, 3.811229941680529, 3.668055256252701, 3.6552686331401385, 3.7029880324337356, 3.0140608329032506, 3.937720740481832, 4.013906911761251, 2.9094816347834005, 4.341008594816939, 4.293344623935421, 3.6736443562418697, 2.750378429907256, 3.8894471993334405, 4.441737038663863, 3.8087004728620584, 2.856244438802545, 3.8534615465234783, 3.137211795420728, 4.151715435828541, 4.240633496831613, 3.9246972090495107, 3.3849467636020933, 3.987881313853906, 4.005425043842247, 3.813463158586162, 3.229406560631939, 3.7662383050279797, 3.5699021030754157, 4.019187895782601, 3.398250624465632, 4.166969498613235, 4.833088711644356, 3.255251981839718, 3.340175486444913, 4.291287665328371, 4.2583127321725325, 3.3097589963299896, 3.3812115394832363, 3.6001149305865128, 3.5009896099801696, 3.767665882730075, 3.1945678936922413, 3.3999188312759343, 3.438478023453409, 3.5023807939466516, 4.052778202364112, 3.031561238755828, 2.798860394972447, 4.245802421995799, 3.5873688378854904, 3.9212406308825587, 4.165554555218474, 3.5723556041459554, 3.2988132733605364, 4.515712775550157, 4.268835550380616, 3.543886641353088, 3.873591239290765, 2.777755301793729, 3.6561058103614217, 4.212923424796455, 4.06782235010605, 3.0495770638088198, 4.125757898497765, 2.5684455779011595, 3.530803592985945, 3.681513374084797, 4.132514980387124, 3.627749481726452, 3.450751727818145, 3.8536701205910107, 3.441515070973102, 3.4815974530992744, 3.55591802012897, 3.7588627315496153, 3.430520710380496, 3.780614528421131, 3.5101553613890353, 3.8189256688146296, 3.8417902595891698, 3.751593130813473, 4.524120767843995, 4.000338900972621, 3.2022849441529817, 3.5450241430864042, 3.9199087165447244, 3.637447502305089, 3.242936445706291, 3.501805133238125, 3.2748748485621832, 3.095925679610334, 4.055655530632716, 3.2161318015997407, 4.052848731105761, 3.910677257510549, 3.681734777243685, 2.4418028037220156, 4.120234921221747, 3.5638471310540774, 3.7363594518820613, 3.9531298699126913, 3.698594326380901, 3.8108039127582685, 3.3683977522517212, 2.8134369246268625, 3.756479433638221, 3.123326262320284, 3.2440972413156355, 3.6472783674810842, 3.0594997977461813, 3.5727981918696488, 4.327706685520707, 2.576303262428408, 3.8695084484862905, 2.975485590745559, 2.8827007014293313, 2.760225711326471, 3.736079994556345, 3.9587603552119033, 3.8769130118516615, 3.1492139743870036, 3.570480580701759, 3.3139876453777775, 2.8643995407065246, 3.597023929718156, 5.0, 3.7776138758519617, 3.452938508108375, 3.5977241352621907, 3.9910640364224648, 3.8557542653375547, 4.2578989960526865, 3.894514827823547, 3.9126538091544285, 3.725220278219747, 3.7943741058455314, 3.997379168936366, 4.106652646671071, 3.5811275241532043, 3.8158255461776664, 3.2560716651353414, 3.5141200780489927, 3.828441149271708, 3.403070076292464, 3.2778865392955128, 4.915205957894904, 3.50525268087533, 3.828727180414672, 3.2594099645185377, 2.504307736193009, 4.021530296408251, 3.8523815984946497, 3.6194262566176483, 3.50217715096617, 3.820504835983982, 3.3060063930866948, 3.071178673790151, 3.665653399930074, 4.189798771093055, 3.9943375004921413, 3.5701536776271587, 4.210125970105092, 4.059994434046606, 3.534742364637214, 4.293147634375841, 3.191297546250071, 3.509978009315507, 3.7099760925866727, 3.6968463819099036, 4.106296808260921, 4.399898008073698, 2.968062732703964, 3.691658357260565, 3.551865234255029, 3.925763077190869, 3.691293100141941, 3.805567491218304, 2.744888004654641, 3.1441895538481632, 3.618009529168598, 3.50500723449116, 3.058104181584129, 3.2314717460096793, 3.6259786729516694, 3.893794492129712, 4.228371673677157, 2.768663761199803, 3.6432294250452366, 3.8363676922095804, 3.9366942164291334, 3.8586439989597014, 3.0558099512567485, 3.835936611256497, 3.3748088853569786, 3.790097366613331, 2.7894402381288264, 3.739210095900022, 3.811553111440086, 3.165009465326257, 3.648124139919731, 3.60621226891727, 3.670660859709994, 4.13511948456684]\n",
      "[5.0, 4.0, 5.0, 3.0, 3.5, 1.0, 1.0, 4.0, 3.0, 3.5, 3.0, 4.0, 4.0, 3.0, 2.0, 3.0, 4.0, 5.0, 4.0, 4.0, 3.0, 5.0, 4.5, 4.0, 3.0, 3.0, 2.0, 5.0, 4.5, 4.0, 4.0, 4.0, 3.5, 3.5, 0.5, 3.0, 3.5, 5.0, 4.0, 3.5, 5.0, 3.0, 5.0, 4.0, 5.0, 5.0, 3.5, 4.0, 5.0, 1.0, 3.5, 5.0, 3.5, 5.0, 3.0, 3.0, 3.0, 2.5, 3.5, 1.0, 5.0, 5.0, 4.0, 4.0, 1.0, 2.0, 5.0, 2.5, 4.5, 5.0, 4.5, 2.0, 3.5, 3.0, 4.0, 3.0, 2.0, 4.0, 3.0, 4.0, 3.0, 2.0, 4.0, 2.0, 2.5, 5.0, 3.0, 3.5, 5.0, 4.0, 5.0, 5.0, 3.0, 2.0, 3.0, 4.0, 5.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 5.0, 1.0, 4.5, 4.0, 3.0, 4.0, 5.0, 4.0, 4.0, 4.5, 3.0, 3.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 3.0, 3.0, 4.0, 3.5, 3.0, 4.0, 3.0, 4.0, 4.0, 4.0, 3.0, 3.0, 4.0, 5.0, 3.0, 4.0, 4.0, 5.0, 3.0, 3.0, 0.5, 4.0, 2.5, 3.5, 4.0, 2.0, 4.5, 4.0, 3.0, 3.0, 4.0, 3.5, 3.0, 4.0, 5.0, 3.0, 4.0, 2.5, 3.5, 3.5, 4.0, 5.0, 3.0, 0.5, 4.5, 3.0, 4.0, 5.0, 5.0, 2.0, 4.0, 5.0, 3.5, 3.5, 3.5, 4.0, 3.0, 5.0, 5.0, 5.0, 3.0, 4.0, 4.0, 3.0, 2.0, 3.0, 1.0, 5.0, 5.0, 4.0, 3.0, 3.5, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 2.5, 4.0, 3.0, 4.0, 3.0, 4.5, 5.0, 2.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.5, 3.0, 5.0, 5.0, 3.0, 4.0, 4.0, 2.5, 5.0, 5.0, 3.0, 3.0, 2.0, 4.5, 3.0, 4.0, 4.5, 4.0, 3.0, 3.0, 5.0, 3.5, 1.0, 3.5, 2.5, 2.5, 5.0, 3.0, 3.5, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 2.5, 5.0, 3.0, 3.0, 4.5, 3.0, 4.5, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 2.5, 3.0, 3.0, 5.0, 5.0, 4.0, 2.5, 4.5, 2.0, 4.0, 4.0, 2.0, 2.5, 5.0, 3.0, 5.0, 4.0, 4.0, 1.5, 5.0, 3.0, 3.0, 3.0, 4.0, 4.5, 4.0, 4.0, 4.0, 3.0, 3.0, 2.5, 4.0, 4.0, 3.5, 3.5, 4.0, 4.0, 4.0, 3.0, 4.0, 5.0, 5.0, 2.5, 4.0, 3.0, 3.0, 5.0, 4.0, 4.0, 2.0, 4.0, 4.5, 2.0, 5.0, 4.0, 2.0, 4.0, 4.0, 0.5, 2.0, 3.0, 4.0, 5.0, 3.0, 5.0, 5.0, 2.0, 4.0, 4.5, 5.0, 3.5, 2.0, 4.5, 4.0, 4.0, 3.5, 3.5, 2.0, 4.0, 4.0, 1.5, 1.0, 4.0, 5.0, 3.5, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 3.5, 3.0, 4.0, 5.0, 5.0, 3.0, 5.0, 0.5, 3.0, 3.0, 4.0, 3.0, 3.0, 5.0, 5.0, 3.0, 3.0, 3.5, 3.5, 5.0, 3.0, 3.5, 2.0, 2.0, 2.0, 4.5, 3.0, 3.0, 2.0, 5.0, 4.0, 3.0, 2.5, 1.5, 3.0, 1.5, 5.0, 4.0, 4.0, 1.0, 4.0, 3.0, 4.0, 3.0, 3.0, 4.0, 2.5, 2.0, 3.0, 5.0, 3.0, 3.5, 3.0, 3.0, 4.0, 3.0, 4.5, 5.0, 2.0, 4.0, 3.0, 4.5, 4.0, 2.5, 3.0, 4.5, 2.0, 3.0, 4.0, 5.0, 4.0, 4.0, 2.0, 1.5, 3.0, 5.0, 4.0, 4.0, 4.0, 3.0, 5.0, 3.5, 2.0, 3.0, 3.5, 5.0, 3.0, 2.0, 4.5, 2.5, 3.0, 4.5, 3.0, 5.0, 3.0, 2.0, 4.5, 4.5, 2.0, 5.0, 1.5, 5.0, 4.0, 4.0, 4.5, 5.0, 4.0, 3.0, 3.5, 4.0, 3.5, 3.5, 5.0, 3.5, 1.0, 4.0, 3.0, 4.0, 3.0, 3.5, 1.0, 4.0, 2.5, 3.0, 3.0, 5.0, 5.0, 3.0, 5.0, 4.0, 1.5, 3.5, 3.0, 4.0, 2.0, 4.5, 3.5, 5.0, 3.0, 3.0, 4.5, 3.0, 4.0, 2.0, 3.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#alternative to simply rerunning alaysis: run the entire code starting where random choice is implemented...\n",
    "\n",
    "#earliest random choice is the test and train users\n",
    "#this becomes averaged out with more users\n",
    "#the next random choice is the random target movie for each user to be trained and tested against\n",
    "#this can also be averged out with more users...\n",
    "#lastly the there is randomness used in the model\n",
    "#random state can be used to keep runs consistent\n",
    "#but a number of random runs can be combined to see the real difference between including...\n",
    "#different hyper parameters\n",
    "\n",
    "\n",
    "#hyper paramaters: only the layers and the features are a hyper parameters at the moment\n",
    "#def test_hyper_parameters():\n",
    "\n",
    "# runs analysis a number of times and average the returned\n",
    "# def average_results():\n",
    "\n",
    "#returns the two average scores with and without rounding\n",
    "# def analysis():\n",
    "\n",
    "\n",
    "\n",
    "def test_parameters(layers, train_input_features, test_input_features):\n",
    "    train_inputs = [list(pair) for pair in train_input_features]\n",
    "    test_inputs = [list(pair) for pair in test_input_features]\n",
    "\n",
    "    return average_results(20, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "\n",
    "def average_results(nof_runs, layers, train_inputs, text_inputs):\n",
    "    no_rounding = 0\n",
    "    rounding = 0\n",
    "    for _ in range(nof_runs):\n",
    "        #issue here\n",
    "        pair = analysis(layers, train_inputs, text_inputs)\n",
    "        no_rounding+=pair[0]\n",
    "        rounding+=pair[1]\n",
    "    return float(no_rounding/nof_runs), float(rounding/nof_runs)\n",
    "\n",
    "\n",
    "def analysis(layers, train_inputs, test_inputs):\n",
    "    #build and train model\n",
    "    reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    reg.fit(train_inputs, train_users.user_to_target_rating)\n",
    "\n",
    "    #show importance of different inputs features...\n",
    "    results = permutation_importance(reg, train_inputs, train_users.user_to_target_rating)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "\n",
    "    predictions = reg.predict(test_inputs)\n",
    "\n",
    "    #testing if needed...\n",
    "    # predictions = np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "    #test with and without roundings...\n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        # rounded_predictions.append(float(round(item[0]*2)/2.0))\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "print(test_parameters((10,10,10), \n",
    "    zip(train_users.feature_1, train_users.feature_3),\n",
    "      zip(test_users.feature_1, test_users.feature_3)))\n",
    "    #   zip(train_users.feature_1, train_users.feature_2, train_users.feature_3),\n",
    "    #   zip(test_users.feature_1,test_users.feature_2, test_users.feature_3)))\n",
    "\n",
    "print(test_users.feature_1)\n",
    "print(test_users.feature_2)\n",
    "print(test_users.feature_3)\n",
    "print(test_users.user_to_target_rating)\n",
    "\n",
    "\n",
    "\n",
    "#tf-idf\n",
    "#full user averages used for feature 1\n",
    "#svd: fill in predictions instead of movie averages for feature 3 (20 unknown qualities)\n",
    "#(10,10,10)\n",
    "#(0.2387375681566831, 0.20240329910057456)\n",
    "#(0.23792365230470303, 0.20926585590351995)\n",
    "\n",
    "\n",
    "#weighted user average for feature 1\n",
    "#svd: fill in predictions instead of movie averages for feature 3 (20 unknown qualities)\n",
    "#(10,10,10)\n",
    "#(0.23579683490923925, 0.20897142277336064) (this is an anomaly!!!)\n",
    "\n",
    "\n",
    "\n",
    "#last rating to test scaling: \n",
    "#with scaling: (0.17838025048385192, 0.15463718590779604)\n",
    "#without scaling: (0.17508111714976957, 0.15035658116932518)\n",
    "#there seems to be little difference...\n",
    "\n",
    "\n",
    "#test with 10 feature svd and tf-idf with full weighted averages:\n",
    "#(0.18026933343349852, 0.15475042941939582)\n",
    "\n",
    "#test with 5 feature svd and tf-idf with full weighted averages:\n",
    "#(0.21509991618052599, 0.18274422548685634)\n",
    "\n",
    "#test with 2 feature svd and tf-idf with full weighted averages:\n",
    "#(0.22870341583000076, 0.19434036107467173)\n",
    "\n",
    "\n",
    "#test with 5 most simlair items rather than a combination of all items:\n",
    "#test with 2 feature svd and tf-idf:\n",
    "#(0.19767271072185125, 0.18709277633228713)\n",
    "\n",
    "#test with 10 most simlair items rather than a combination of all items:\n",
    "#test with 5 feature svd and tf-idf:\n",
    "#(0.20407756846055794, 0.18016227342238192)\n",
    "\n",
    "\n",
    "#k = 5 no n...\n",
    "#(0.21586848684777182, 0.18883672641092336)\n",
    "\n",
    "#k = 10 no n...\n",
    "#(0.20119183939254853, 0.1870248302253273)\n",
    "\n",
    "#truncated svd and k-means...\n",
    "#k = 10, n = 10\n",
    "#(0.20470831678489237, 0.18000373250614218)\n",
    "\n",
    "#k = 10, n = 5\n",
    "#(0.20917323341221783, 0.1879987244250852)\n",
    "\n",
    "#k = 20, n = 5\n",
    "#(0.20545178723917532, 0.17810124151126625)\n",
    "\n",
    "\n",
    "\n",
    "#feature_1 only:\n",
    "#(0.05944569580128489, 0.0314961913942177)\n",
    "#(0.0641214856712294, 0.03765663842524462)\n",
    "\n",
    "#feature_2 only:\n",
    "#(0.12905299125573932, 0.1101324858490906)\n",
    "#(0.12775797535090633, 0.10859237409133393)\n",
    "\n",
    "#feature_3 only:\n",
    "#(0.15219339823019565, 0.12784377106329303)\n",
    "#(0.1531055397624798, 0.13210172709944393)\n",
    "\n",
    "#feature 2 and 3:\n",
    "#(0.2314230100193003, 0.20514379208128877)\n",
    "#(0.23081202509540993, 0.20532498169984836)\n",
    "\n",
    "\n",
    "\n",
    "#ideas:\n",
    "#what if stopwords were included or the words have more or less filtering???\n",
    "#for content based reccomendation, may need to focus on words from certain sections to not overwelm the model with useless information!!!\n",
    "#k-means clusting can be effected by the random initialization...\n",
    "#this is a reason that there is substantial variation in perfromance\n",
    "#note: there are some predictions over 5 stars???\n",
    "#idea: what about only focusing on movies with a specific number of ratings???\n",
    "#what if there are more ratings per user???\n",
    "#what if boolena values were used for content baed items instead of counts???\n",
    "\n",
    "#even even even more (done):\n",
    "#https://www.kaggle.com/code/cast42/simple-svd-movie-recommender\n",
    "\n",
    "#even even more svd:\n",
    "#https://analyticsindiamag.com/singular-value-decomposition-svd-application-recommender-system/\n",
    "\n",
    "#even more svd:\n",
    "#https://towardsdatascience.com/beginners-guide-to-creating-an-svd-recommender-system-1fd7326d1f65\n",
    "\n",
    "#more svd:\n",
    "#https://machinelearningmastery.com/using-singular-value-decomposition-to-build-a-recommender-system/\n",
    "\n",
    "#even more content base recomendations:\n",
    "#https://towardsdatascience.com/the-4-recommendation-engines-that-can-predict-your-movie-tastes-109dc4e10c52\n",
    "\n",
    "#more content based reccomendation:\n",
    "#https://medium.com/web-mining-is688-spring-2021/content-based-movie-recommendation-system-72f122641eab#:~:text=Content%20Based%20Recommendation%20System%3A%20It,a%20show%20similar%20to%20it.\n",
    "\n",
    "#content based reccomendation:\n",
    "#https://medium.com/geekculture/creating-content-based-movie-recommender-with-python-7f7d1b739c63\n",
    "\n",
    "#methods used to guess ratings\n",
    "#https://web.eecs.umich.edu/~cscott/past_courses/eecs545f11/projects/AsendorfMcgaffinPressSchwartz.pdf\n",
    "\n",
    "#svd...\n",
    "#https://www.youtube.com/watch?v=8wLKuscyO9I&ab_channel=SundogEducationwithFrankKane\n",
    "\n",
    "#pca...\n",
    "#https://www.youtube.com/watch?v=fkf4IBRSeEc&ab_channel=SteveBrunton\n",
    "\n",
    "#eigenvalues...\n",
    "#https://www.youtube.com/watch?v=OA6CkChbe0Q&ab_channel=AndrewMisseldine\n",
    "\n",
    "#more pca:\n",
    "#https://www.youtube.com/watch?v=TJdH6rPA-TI&ab_channel=Computerphile\n",
    "\n",
    "#svd lecture:\n",
    "#https://www.youtube.com/watch?v=rFemvJgXY7E&ab_channel=Tech4Trends\n",
    "\n",
    "#svd with tf-idf\n",
    "#https://www.kaggle.com/code/parnianmalekian/svd-and-its-application-in-tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is for notes and models with scaling and no scaling...\n",
    "\n",
    "\n",
    "#analysis without feature scaling...\n",
    "#feature scaling:\n",
    "#https://analyticsindiamag.com/why-data-scaling-is-important-in-machine-learning-how-to-effectively-do-it/#:~:text=Scaling%20the%20target%20value%20is,learn%20and%20understand%20the%20problem.&text=Scaling%20of%20the%20data%20comes,algorithms%20in%20the%20data%20set.\n",
    "#https://towardsdatascience.com/collaborative-filtering-based-recommendation-systems-exemplified-ecbffe1c20b1\n",
    "\n",
    "\n",
    "# with scaling start:\n",
    "\n",
    "# def test_parameters(layers, train_input_features, test_input_features):\n",
    "#     train_inputs = [list(pair) for pair in train_input_features]\n",
    "#     test_inputs = [list(pair) for pair in test_input_features]\n",
    "\n",
    "#     return average_results(20, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "# def average_results(nof_runs, layers, train_inputs, text_inputs):\n",
    "#     no_rounding = 0\n",
    "#     rounding = 0\n",
    "#     for _ in range(nof_runs):\n",
    "#         #issue here\n",
    "#         pair = analysis(layers, train_inputs, text_inputs)\n",
    "#         no_rounding+=pair[0]\n",
    "#         rounding+=pair[1]\n",
    "#     return float(no_rounding/nof_runs), float(rounding/nof_runs)\n",
    "\n",
    "\n",
    "# def analysis(layers, train_inputs, test_inputs):\n",
    "#     #scale input features\n",
    "#     #scalar 1:\n",
    "#     input_scalar = StandardScaler()\n",
    "#     train_inputs_scaled = input_scalar.fit_transform(train_inputs)\n",
    "\n",
    "#     #scale target values\n",
    "#     #scalar 2:\n",
    "#     target_scalar = StandardScaler()\n",
    "#     true_rating_train_scaled = target_scalar.fit_transform(np.array(train_users.user_to_target_rating).reshape(-1, 1))\n",
    "#     true_rating_train_scaled = np.reshape(true_rating_train_scaled, len(true_rating_train_scaled))\n",
    "\n",
    "#     #build and train model\n",
    "#     reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "#     reg.fit(train_inputs_scaled, true_rating_train_scaled)\n",
    "\n",
    "#     #show importance of different inputs features...\n",
    "#     results = permutation_importance(reg, train_inputs_scaled,true_rating_train_scaled)\n",
    "#     print(results[\"importances_mean\"])\n",
    "\n",
    "#     #scale inputs features\n",
    "#     #scalar 3: \n",
    "#     input_scalar = StandardScaler()\n",
    "#     test_inputs_scaled = input_scalar.fit_transform(test_inputs)\n",
    "\n",
    "#     #predict the scaled verison of ouptuts\n",
    "#     scaled_predictions = reg.predict(test_inputs_scaled)\n",
    "\n",
    "#     #change back into regular outputs to be tested with target scalar above...\n",
    "#     #not sure if this is correct???\n",
    "#     #would it be better to use a new scalar object???\n",
    "#     #need to know the mean and the std dev of the orginal predictions\n",
    "#     #scalar 4:\n",
    "#     predictions = target_scalar.inverse_transform(np.array(scaled_predictions).reshape(-1, 1))\n",
    "\n",
    "#     #test with and without roundings...\n",
    "#     rounded_predictions = []\n",
    "#     for item in predictions:\n",
    "#         rounded_predictions.append(float(round(item[0]*2)/2.0))\n",
    "\n",
    "#     #evaluation metric 1\n",
    "#     return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "#         r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#     #evaluation metric 2\n",
    "#     # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "#     #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "# print(test_parameters((10,10,10), \n",
    "#       zip(train_users.feature_1, train_users.feature_3),\n",
    "#       zip(test_users.feature_1, test_users.feature_3)))\n",
    "\n",
    "\n",
    "# print(test_users.feature_1)\n",
    "# print(test_users.feature_3)\n",
    "# print(test_users.user_to_target_rating)\n",
    "\n",
    "# ...with scaling done\n",
    "\n",
    "\n",
    "#without scaling:\n",
    "\n",
    "# def test_parameters(layers, train_input_features, test_input_features):\n",
    "#     train_inputs = [list(pair) for pair in train_input_features]\n",
    "#     test_inputs = [list(pair) for pair in test_input_features]\n",
    "\n",
    "#     return average_results(20, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "\n",
    "# def average_results(nof_runs, layers, train_inputs, text_inputs):\n",
    "#     no_rounding = 0\n",
    "#     rounding = 0\n",
    "#     for _ in range(nof_runs):\n",
    "#         #issue here\n",
    "#         pair = analysis(layers, train_inputs, text_inputs)\n",
    "#         no_rounding+=pair[0]\n",
    "#         rounding+=pair[1]\n",
    "#     return float(no_rounding/nof_runs), float(rounding/nof_runs)\n",
    "\n",
    "\n",
    "# def analysis(layers, train_inputs, test_inputs):\n",
    "#     #build and train model\n",
    "#     reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "#     reg.fit(train_inputs, train_users.user_to_target_rating)\n",
    "\n",
    "#     #show importance of different inputs features...\n",
    "#     results = permutation_importance(reg, train_inputs, train_users.user_to_target_rating)\n",
    "#     print(results[\"importances_mean\"])\n",
    "\n",
    "\n",
    "#     predictions = reg.predict(test_inputs)\n",
    "\n",
    "#     #testing if needed...\n",
    "#     # predictions = np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "#     #test with and without roundings...\n",
    "#     rounded_predictions = []\n",
    "#     for item in predictions:\n",
    "#         # rounded_predictions.append(float(round(item[0]*2)/2.0))\n",
    "#         rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "#     #evaluation metric 1:\n",
    "#     return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "#         r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#     #evaluation metric 2:\n",
    "#     # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "#     #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "# print(test_parameters((10,10,10), \n",
    "#       zip(train_users.feature_1, train_users.feature_3),\n",
    "#       zip(test_users.feature_1, test_users.feature_3)))\n",
    "\n",
    "# print(test_users.feature_1)\n",
    "# print(test_users.feature_3)\n",
    "# print(test_users.user_to_target_rating)\n",
    "\n",
    "#done with scaling...\n",
    "\n",
    "\n",
    "#now for the user comparison option (need user to list of movie ratings)\n",
    "#fill in ratings that the user hasn't watched with the method above\n",
    "#then cluster the users by their ratings\n",
    "\n",
    "#note: agglomerative clustering might make more sense here since k-means has random init for centroids...\n",
    "#note: to guess a new users rating requires that none of that users ratings have been used to train the model\n",
    "#The data needs to be split into test and train before modeling the algorithm on the train data\n",
    "\n",
    "#Training process:\n",
    "#split data into test and train data\n",
    "#proceed with train data...\n",
    "#cluster movies by the tokens with range for k\n",
    "#cluster users by the ratings with range for k and (fill in ratings for movies a users hasn't watched with some guess)\n",
    "#guess: this can be obtained by clustering the movies that the user has watched...\n",
    "#for each movie the user hasn't watched find the cluster that it belongs to with the highest possible k value\n",
    "#that the user has at least one movie belonging to one of the clusters and then take the average of those movies\n",
    "#this is exactly like a later training step excpet it is applied to all the movies the user watched\n",
    "\n",
    "#for a single randomly chosen movie from each user in the trainging data...\n",
    "\n",
    "#find the cluster the movie belongs to \n",
    "#find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#take the average score of these movies\n",
    "#find the cluster the user belongs to\n",
    "#find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#train an mlp model with both averages and perhaps some extra statistics as features...\n",
    "#using the given movie ratings as actuals\n",
    "\n",
    "\n",
    "#The process of predicting a rating:\n",
    "#1. find the cluster the movie belongs to \n",
    "#2. find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#3. take the average score of these movies\n",
    "#4. find the cluster the user belongs to\n",
    "#5. find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#6. input into the trained mlp model both averages and perhaps some extra statistics\n",
    "#7. make predictions and test against the randomly chosen movies actual ratings\n",
    "\n",
    "\n",
    "#summary:\n",
    "#find cluster for movie -> find movies part of the same clusters that the users rated -> average\n",
    "#question: are the clusters unique to the movies the user has watched or to all movies???\n",
    "#what is the technical difference???\n",
    "#is this the same as finding the most simimlair movie the user rated and copying the rating???\n",
    "\n",
    "#find cluster for user -> find the ratings for the movie by people in the same cluster -> average\n",
    "\n",
    "#other avenues considered:\n",
    "#idea 1:\n",
    "#for the first process, instead of averaging the movies that only the user rated, find other users that are...\n",
    "#like the user in question and find the average for that movie cluster\n",
    "#Problem: it is better to get the users raw opionion rather than generalizing it to some like minded users\n",
    "#there is an extra costly step to this\n",
    "#idea 2: \n",
    "#for the second process, instead of finding the average rating for the movie in the same cluster of users...\n",
    "#also find the average rating of movies that are like the movie in question \n",
    "#Problem, it is better to get the movies rating itself as it would be the most accurate indicator\n",
    "#there is an extra costly step to this\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test with overall averges:\n",
    "#note: users have between 50 and 75 ratings each\n",
    "#note: there are 250 users who are taken into account \n",
    "#note: seed int is one for above cells (cells part of creating the csv file)\n",
    "\n",
    "#with seed int == 3, 4 taking overall averages: 0.09565753948597455\n",
    "#with seed_int == 1, 2 taking overall averages: 0.070404868516315\n",
    "#with seed_int == 2, 5 taking overall averages: 0.11310085954932936\n",
    "#with seed_int == 4, 6 taking overall averages: 0.07125374341347135\n",
    "#with seed_int == 5, 4 taking overall averages: 0.17736444913943628\n",
    "#compute time: 11 minutes\n",
    "\n",
    "\n",
    "#test with users related movies:\n",
    "#is there a magic proportion of movies to average???\n",
    "#note: this is taking around the same time as the above tests meaning \n",
    "#there could be more users to include in analysis with little increase in runtime\n",
    "#k fold cross validation could be effective\n",
    "#https://www.youtube.com/watch?v=TIgfjmp-4BA&ab_channel=Udacity\n",
    "\n",
    "#effect of choosing a random seed...\n",
    "#https://towardsdatascience.com/how-to-use-random-seeds-effectively-54a4cd855a79\n",
    "\n",
    "#try tinkering with the number of similair movies to average\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
