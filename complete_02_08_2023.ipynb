{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 3\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "#new:\n",
    "#userIds are in order\n",
    "# every_id = list(unique(list(complete[\"userId\"])))\n",
    "#userIds are out of order\n",
    "# sample_ids  = random.sample(every_id, 1000)\n",
    "# completeNew = pd.DataFrame()\n",
    "#This is a very expensive task...\n",
    "#it is possile to choose a subset of users from here instead of\n",
    "#ibcluing  the entire set of users\n",
    "# for user in sample_ids:\n",
    "#     completeNew = pd.concat([completeNew, complete.loc[complete[\"userId\"] == user]])\n",
    "\n",
    "#new:\n",
    "# complete = complete.sample(frac=1, random_state = seed_int, axis =0)\n",
    "#this is not the desired behavior\n",
    "#the users ids need to show up in a true random order\n",
    "# complete = complete.groupby(by = \"userId\", sort = False, group_keys = True).apply(lambda x: x)\n",
    "#this is omitted since the values should not be sorted by userId just grouped by userId\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "\n",
    "# print(complete.head())\n",
    "\n",
    "# f = open(\"test_dicts.txt\", \"w\", encoding=\"utf-8\")\n",
    "# f.write(str(list(complete[\"tagline\"])))\n",
    "# f.write(str(list(complete[\"overview\"])))\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272\n",
      "29.929668552950687\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 3\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    #this is probably not needed due to the dropNa function used above...\n",
    "    # length = len(array[8])\n",
    "    # if(array[8][length-4:]==\"<NA>\"):\n",
    "    #     return False\n",
    "    # length = len(array[9])\n",
    "    # if(array[9][length-4:]==\"<NA>\"):\n",
    "    #     return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#convert the dataframe into an array and build a dictionary\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "\n",
    "gaps = []\n",
    "size = 0\n",
    "list_of_user_ids = []\n",
    "last_id  = -1\n",
    "past_first_it = False\n",
    "\n",
    "\n",
    "#there seems to be a technical problem with gaps...\n",
    "for row in complete_array:\n",
    "    #need to omit the first iteration for gaps\n",
    "    if(row[0]!= last_id):\n",
    "        list_of_user_ids.append(row[0])\n",
    "        last_id = row[0]\n",
    "        if(past_first_it ==True):\n",
    "            gaps.append(size)\n",
    "            size =0 \n",
    "    size+=1\n",
    "    past_first_it = True\n",
    "\n",
    "#there is always a gap for the last iteration\n",
    "gaps.append(size)\n",
    "\n",
    "\n",
    "index  = 0\n",
    "user_to_data = dict()\n",
    "#this is the total number of users in the whole dataset\n",
    "total_nof_users = 261306\n",
    "#this is the number of desired users before filtering\n",
    "selected_nof_users_before_filter = 20000\n",
    "\n",
    "avg =0\n",
    "cnt =0\n",
    "\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, total_nof_users):\n",
    "    #generate a random float to determine a pass for the user\n",
    "    if (random.random()<float(selected_nof_users_before_filter/total_nof_users)):\n",
    "        user_to_data[list_of_user_ids[i]] = []\n",
    "        for j in range(index, len(complete_array)):\n",
    "            if complete_array[j][0] == list_of_user_ids[i]:\n",
    "                #condition is checked for complete_array[j]\n",
    "                if(condition(complete_array[j])):\n",
    "                    #this is where data is tranformed\n",
    "                    transformed = provide_data(complete_array[j])\n",
    "                    user_to_data[list_of_user_ids[i]].append(transformed)         \n",
    "            else:\n",
    "                avg += len(user_to_data[list_of_user_ids[i]])\n",
    "                cnt+=1\n",
    "                #this condition can be tweaked for better accuracy\n",
    "                #len(user_to_data[list_of_user_ids[i]])<50 or len(user_to_data[list_of_user_ids[i]])>75\n",
    "                if (len(user_to_data[list_of_user_ids[i]])<50 or len(user_to_data[list_of_user_ids[i]])>75):\n",
    "                    del user_to_data[list_of_user_ids[i]]\n",
    "                #note: changed from (index = j+1)\n",
    "                index = j\n",
    "                break\n",
    "    else:\n",
    "        #every iteration, index starts at first data point of the next user\n",
    "        index += gaps[i]\n",
    "\n",
    "#test for the last iteration not done in loop\n",
    "if list_of_user_ids[total_nof_users-1] in user_to_data.keys():\n",
    "    if (len(user_to_data[list_of_user_ids[i]])<50 or len(user_to_data[list_of_user_ids[i]])>75):\n",
    "        del user_to_data[list_of_user_ids[total_nof_users-1]]\n",
    "\n",
    "\n",
    "#needs to be sure that there are enough users after the condiiton\n",
    "print(len(list(user_to_data.keys())))\n",
    "\n",
    "\n",
    "#average number of ratings per user\n",
    "print(float(avg/cnt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for key in user_to_data.keys():\n",
    "        writer.writerows(user_to_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 1\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#user to data rows \n",
    "user_to_data = dict()\n",
    "user_to_data_train = dict()\n",
    "user_to_data_test = dict()\n",
    "\n",
    "user_id = -1\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_data[row[0]] = [row]\n",
    "    else:\n",
    "        user_to_data[row[0]].append(row)\n",
    "\n",
    "#this can be tweaked...\n",
    "for i in range(150):\n",
    "    user = random.choice(list(user_to_data.keys()))\n",
    "    user_to_data_train[user] = user_to_data[user]\n",
    "    user_to_data.pop(user)\n",
    "\n",
    "\n",
    "#for test data to be used later...\n",
    "for i in range(50):\n",
    "    user = random.choice(list(user_to_data_train.keys()))\n",
    "    user_to_data_test[user] = user_to_data_train[user]\n",
    "    user_to_data_train.pop(user)\n",
    "\n",
    "\n",
    "user_to_data.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import json\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 1\n",
    "# random.seed(seed_int)\n",
    "\n",
    "\n",
    "user_to_movie_id_to_corpus_train = dict() #includes omitted movies\n",
    "user_to_movie_id_to_rating_train = dict() #includes omitted movies\n",
    "movie_id_to_ratings_train = dict() #includes omitted movies, only needed for train set\n",
    "overall_sum_train = 0 #does not include omitted movies, only needed for train set\n",
    "overall_counts_train = 0 #does not include omitted movies, only needed for train set\n",
    "user_to_rand_movie_id_train  = dict() #determines ommited movies\n",
    "movies_in_order = OrderedSet()\n",
    "\n",
    "\n",
    "user_to_movie_id_to_corpus_test = dict() #includes omitted movies\n",
    "user_to_movie_id_to_rating_test = dict() #includes omitted movies\n",
    "user_to_rand_movie_id_test  = dict() #determines ommited movies\n",
    "\n",
    "\n",
    "#not an input to function\n",
    "movie_id_to_average_rating_train = dict()\n",
    "\n",
    "# WordNetLemmatizer().lemmatize(token.lower())\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def populate(user_to_data, user_to_movie_id_to_corpus, user_to_movie_id_to_rating,\n",
    "            movie_id_to_ratings, overall_sum, overall_counts, user_to_rand_movie_id):\n",
    "    for user in user_to_data.keys():\n",
    "        movie_strings_temp = dict()\n",
    "        movie_id_to_rating_temp = dict()\n",
    "        cnt = 0\n",
    "        #does this cover all of 0-len(user_to_data_train[user])-1 ???\n",
    "        rand_int = random.randint(0, len(user_to_data[user])-1)\n",
    "        for movie_data in user_to_data[user]:\n",
    "            if cnt == rand_int:    \n",
    "                user_to_rand_movie_id[user] = movie_data[1]\n",
    "            else:\n",
    "                overall_sum += float(movie_data[2])\n",
    "                overall_counts += 1\n",
    "            movies_in_order.add(movie_data[1])\n",
    "\n",
    "            #this also includes omitted ratings (the rating to be predicted by the model)\n",
    "            if movie_data[1] in movie_id_to_ratings.keys():\n",
    "                movie_id_to_ratings[movie_data[1]].append(float(movie_data[2]))\n",
    "            else:\n",
    "                movie_id_to_ratings[movie_data[1]] = [float(movie_data[2])]\n",
    "\n",
    "            # movies_in_order.add(movie_data[1])\n",
    "            movie_string = \"\"\n",
    "            #avoid the first three data points (user id, movieid, and rating)\n",
    "            #use only the text data\n",
    "            for index in range (3,len(movie_data)):\n",
    "                if(index!= len(movie_data)-1):\n",
    "                    movie_string+= movie_data[index]+\" \"\n",
    "                else:\n",
    "                    movie_string+= movie_data[index]\n",
    "            cleaned = remove_stopwords(movie_string)\n",
    "            #why recreate the string when you could accept it as a list???\n",
    "            #lematize and form a list of words\n",
    "            cleaned = [wnl.lemmatize(word) for word in cleaned.split(\" \")]\n",
    "            #This is incorrect since it only keeps strings that are at the end of a sentence\n",
    "            #[word[:-1] for word in cleaned if word.endswith(\".\")]\n",
    "            cleaned = [word[:-1] for word in cleaned if word.endswith(\".\")] + [word for word in cleaned if not word.endswith(\".\")]\n",
    "            movie_strings_temp[movie_data[1]] = cleaned\n",
    "            movie_id_to_rating_temp[movie_data[1]] = float(movie_data[2])\n",
    "            cnt+=1\n",
    "        user_to_movie_id_to_corpus[user] = movie_strings_temp\n",
    "        user_to_movie_id_to_rating[user] = movie_id_to_rating_temp\n",
    "    return overall_sum, overall_counts\n",
    "\n",
    "\n",
    "\n",
    "overall_sum_train, overall_counts_train = populate(user_to_data_train, user_to_movie_id_to_corpus_train, \n",
    "                                                   user_to_movie_id_to_rating_train,movie_id_to_ratings_train,\n",
    "                                                   overall_sum_train, overall_counts_train, user_to_rand_movie_id_train)\n",
    "\n",
    "overall_average_train = float(overall_sum_train/overall_counts_train)\n",
    "\n",
    "populate(user_to_data_test, user_to_movie_id_to_corpus_test, user_to_movie_id_to_rating_test,\n",
    "            dict(), 0, 0, user_to_rand_movie_id_test)\n",
    "\n",
    "\n",
    "#this is only needed for the train set since it is reused with the test set\n",
    "for movie in movie_id_to_ratings_train.keys():\n",
    "    temp = 0\n",
    "    for rating in movie_id_to_ratings_train[movie]:\n",
    "        temp +=rating\n",
    "    movie_id_to_average_rating_train[movie] = float(temp/len(movie_id_to_ratings_train[movie]))\n",
    "\n",
    "\n",
    "#testing different encodings and seeing what the text data looks like...\n",
    "file = open(\"test_dicts.txt\", 'w', encoding=\"utf-8\")\n",
    "file.write(json.dumps(user_to_movie_id_to_corpus_train))\n",
    "file.close()\n",
    "\n",
    "file = open(\"test_dicts2.txt\", 'w', encoding=\"utf-8\")\n",
    "file.write(json.dumps(user_to_movie_id_to_corpus_test))\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "#these 2 data structures all have omited values from user_to_rand_movie_id[user]\n",
    "user_to_word_counts_train = dict()\n",
    "user_to_words_in_order_train = dict()\n",
    "#the ommited movie rating is marked with -1\n",
    "user_to_ratings_train = dict()\n",
    "#this is the user to word counts of the omitted movies with user_to_rand_movie_id[user]\n",
    "user_to_rand_word_counts_train = dict()\n",
    "#this store the actual ratings of movies to be predicted by the model\n",
    "user_to_rating_to_predict_train = dict()\n",
    "#this stores the index of the rating to be predicted for each user\n",
    "user_to_rand_index_train = dict()\n",
    "\n",
    "#these 2 data structures all have omited values from user_to_rand_movie_id[user]\n",
    "user_to_word_counts_test = dict()\n",
    "user_to_words_in_order_test = dict()\n",
    "#the ommited movie rating is marked with -1\n",
    "user_to_ratings_test = dict()\n",
    "#this is the user to word counts of the omitted movies with user_to_rand_movie_id[user]\n",
    "user_to_rand_word_counts_test = dict()\n",
    "#this store the actual ratings of movies to be predicted by the model\n",
    "user_to_rating_to_predict_test = dict()\n",
    "#this stores the index of the rating to be predicted for each user\n",
    "user_to_rand_index_test = dict()\n",
    "\n",
    "\n",
    "\n",
    "def step_1(user_to_movie_id_to_corpus, user_to_rand_movie_id, user_to_movie_id_to_rating,\n",
    "           user_to_rand_index, user_to_rating_to_predict, user_to_ratings):\n",
    "    for user in user_to_movie_id_to_corpus.keys():\n",
    "        temp = []\n",
    "        cnt = 0\n",
    "        for movie_id in user_to_movie_id_to_corpus[user]:\n",
    "            if movie_id != user_to_rand_movie_id[user]:\n",
    "                temp.append(user_to_movie_id_to_rating[user][movie_id])\n",
    "            else:\n",
    "                #this signifies the ratings to be predicted by the model\n",
    "                user_to_rand_index[user] = cnt\n",
    "                user_to_rating_to_predict[user] = user_to_movie_id_to_rating[user][movie_id]\n",
    "                temp.append(-1)\n",
    "            cnt+=1\n",
    "        user_to_ratings[user] = temp\n",
    "\n",
    "\n",
    "step_1(user_to_movie_id_to_corpus_train, user_to_rand_movie_id_train, user_to_movie_id_to_rating_train,\n",
    "           user_to_rand_index_train, user_to_rating_to_predict_train, user_to_ratings_train)\n",
    "step_1(user_to_movie_id_to_corpus_test, user_to_rand_movie_id_test, user_to_movie_id_to_rating_test,\n",
    "           user_to_rand_index_test, user_to_rating_to_predict_test, user_to_ratings_test)\n",
    "\n",
    "\n",
    "#need to think about this part...\n",
    "#cossine similairty is relative to a user\n",
    "#if two movies do not share a word then the cossine similarity is unchanged\n",
    "#This means that extra words abscent in both movies do no change the outcome of the cossine similairty\n",
    "#which means its ok to keep them in\n",
    "\n",
    "def step_2(user_to_movie_id_to_corpus, user_to_words_in_order):\n",
    "    for user in user_to_movie_id_to_corpus.keys():\n",
    "        user_to_words_in_order[user] = OrderedSet()\n",
    "        for movie_id in user_to_movie_id_to_corpus[user].keys():\n",
    "            for word in user_to_movie_id_to_corpus[user][movie_id]:\n",
    "                user_to_words_in_order[user].add(word)\n",
    "\n",
    "step_2(user_to_movie_id_to_corpus_train,  user_to_words_in_order_train)\n",
    "step_2(user_to_movie_id_to_corpus_test,  user_to_words_in_order_test)\n",
    "\n",
    "\n",
    "#note: user to word counts needs to omit movies with no rating\n",
    "\n",
    "def step_3(user_to_movie_id_to_corpus, user_to_word_counts, user_to_rand_movie_id, user_to_words_in_order, user_to_rand_word_counts):\n",
    "    for user in user_to_movie_id_to_corpus.keys():\n",
    "        user_to_word_counts[user] = []\n",
    "        for movie_id in user_to_movie_id_to_corpus[user].keys():\n",
    "            if movie_id != user_to_rand_movie_id[user]:\n",
    " \n",
    "                #idea: instead of using words in order use a set of words that are local to the user\n",
    "                #this cuts time iterating over words_in_order\n",
    "                #and cuts space by having templist be shorter\n",
    "                #note: it also adds a bit of space by needing a list of words for each user\n",
    "\n",
    "\n",
    "                temp_dict = Counter(user_to_movie_id_to_corpus[user][movie_id])\n",
    "                temp_list = []\n",
    "                for word in user_to_words_in_order[user]:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                    else:\n",
    "                        temp_list.append(0)               \n",
    "                user_to_word_counts[user].append(temp_list)\n",
    "            else:\n",
    "\n",
    "                temp_dict = Counter(user_to_movie_id_to_corpus[user][movie_id])\n",
    "                temp_list = []\n",
    "                for word in user_to_words_in_order[user]:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                    else:\n",
    "                        temp_list.append(0)\n",
    "                user_to_rand_word_counts[user] = temp_list\n",
    "\n",
    "step_3(user_to_movie_id_to_corpus_train, user_to_word_counts_train, user_to_rand_movie_id_train,\n",
    "       user_to_words_in_order_train, user_to_rand_word_counts_train)\n",
    "\n",
    "step_3(user_to_movie_id_to_corpus_test, user_to_word_counts_test, \n",
    "       user_to_rand_movie_id_test, user_to_words_in_order_test, user_to_rand_word_counts_test)\n",
    "\n",
    "del user_to_words_in_order_train\n",
    "del user_to_words_in_order_test\n",
    "del user_to_movie_id_to_corpus_train\n",
    "del user_to_movie_id_to_corpus_test\n",
    "\n",
    "def predict(user, word_counts, user_to_word_counts, user_to_ratings):\n",
    "\n",
    "    cosine_sim = cosine_similarity(X = user_to_word_counts[user] ,Y = [word_counts])\n",
    "    #not sure if reshape is needed???\n",
    "    #should test with and without\n",
    "    cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "    ratings = []\n",
    "\n",
    "    for rating in user_to_ratings[user]:\n",
    "        if rating != -1:\n",
    "            ratings.append(rating)\n",
    "\n",
    "    combined = zip(cosine_sim, ratings)\n",
    "    combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "    avg = 0\n",
    "    for i in range(0, 10):\n",
    "        avg += combined[i][1]\n",
    "\n",
    "    return float(avg/10.0)\n",
    "    # for i in range(len(combined)):\n",
    "    #     avg += combined[i][1]\n",
    "\n",
    "    # return float(avg/len(combined))\n",
    "\n",
    "\n",
    "#fill in the missing ratings for a user\n",
    "def fill_in_rating(user, user_to_ratings, user_to_rand_index,\n",
    "                     user_to_rand_word_counts, user_to_word_counts):\n",
    "    user_to_ratings[user][user_to_rand_index[user]] = predict(user, user_to_rand_word_counts[user],\n",
    "                                                        user_to_word_counts, user_to_ratings)\n",
    "    return user_to_ratings[user][user_to_rand_index[user]]\n",
    "\n",
    "feature_1_train = []\n",
    "feature_2_train = []\n",
    "true_rating_train = []\n",
    "\n",
    "feature_1_test = []\n",
    "feature_2_test = []\n",
    "true_rating_test = []\n",
    "\n",
    "\n",
    "for user in user_to_movie_id_to_rating_train.keys():\n",
    "    feature_1_train.append(fill_in_rating(user, user_to_ratings_train, user_to_rand_index_train,\n",
    "                     user_to_rand_word_counts_train, user_to_word_counts_train))\n",
    "    true_rating_train.append(user_to_rating_to_predict_train[user])\n",
    "\n",
    "\n",
    "for user in user_to_movie_id_to_rating_test.keys():\n",
    "    feature_1_test.append(fill_in_rating(user, user_to_ratings_test, user_to_rand_index_test,\n",
    "                     user_to_rand_word_counts_test, user_to_word_counts_test))\n",
    "    true_rating_test.append(user_to_rating_to_predict_test[user])\n",
    "\n",
    "\n",
    "for user in user_to_movie_id_to_rating_train.keys(): \n",
    "    if(len(movie_id_to_ratings_train[user_to_rand_movie_id_train[user]])==1):\n",
    "        feature_2_train.append(overall_average_train)\n",
    "    else:\n",
    "        feature_2_train.append(float(((movie_id_to_average_rating_train[user_to_rand_movie_id_train[user]]\n",
    "                        *len(movie_id_to_ratings_train[user_to_rand_movie_id_train[user]]))\n",
    "                        -user_to_movie_id_to_rating_train[user][user_to_rand_movie_id_train[user]])\n",
    "                        /(len(movie_id_to_ratings_train[user_to_rand_movie_id_train[user]])-1)))\n",
    "\n",
    "\n",
    "for user in user_to_movie_id_to_rating_test.keys():\n",
    "    if(user_to_rand_movie_id_test[user] not in movie_id_to_ratings_train.keys()):\n",
    "        feature_2_test.append(overall_average_train)\n",
    "    else:\n",
    "        feature_2_test.append(movie_id_to_average_rating_train[user_to_rand_movie_id_test[user]])\n",
    "\n",
    "\n",
    "#new...\n",
    "\n",
    "user_to_movie_id_to_ratings_full = dict()\n",
    "user_to_rand_index_full = dict()\n",
    "\n",
    "#repeat this for train and test step\n",
    "#replacements:\n",
    "#outerloop: user_to_movie_id_to_rating_train to user_to_movie_id_to_rating_test\n",
    "#need to put into a function\n",
    "\n",
    "for user in user_to_movie_id_to_rating_train.keys():\n",
    "    movie_id_to_ratings_full = dict()\n",
    "    index = 0\n",
    "    for movie_id in movies_in_order:\n",
    "        if movie_id == user_to_rand_movie_id_train[user]:\n",
    "            user_to_rand_index_full[user] = index\n",
    "        if movie_id in user_to_movie_id_to_rating_train[user].keys():\n",
    "            movie_id_to_ratings_full[movie_id] = user_to_movie_id_to_rating_train[user][movie_id]\n",
    "        elif movie_id in movie_id_to_average_rating_train.keys():\n",
    "            movie_id_to_ratings_full[movie_id] = movie_id_to_average_rating_train[movie_id]\n",
    "        else:   \n",
    "            movie_id_to_ratings_full[movie_id] = overall_average_train\n",
    "        index +=1\n",
    "    user_to_movie_id_to_ratings_full[user] = movie_id_to_ratings_full\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this is just a list version of user_to_movie_id_to_ratings_full...\n",
    "user_to_ratings_full = dict()\n",
    "for user in user_to_movie_id_to_rating_train.keys():\n",
    "    row = []\n",
    "    for movie in user_to_movie_id_to_ratings_full[user].keys():\n",
    "        row.append(user_to_movie_id_to_ratings_full[user][movie])\n",
    "    user_to_ratings_full[user] = row\n",
    "\n",
    "\n",
    "feature_3_train = []\n",
    "\n",
    "#see cosine simialirity...\n",
    "#what if cosine similartiy was used with only an x value???\n",
    "for user1 in user_to_ratings_full.keys():\n",
    "    big_list = []\n",
    "    hidden_ratings = []\n",
    "    for user2 in user_to_ratings_full.keys():\n",
    "        if user1 != user2:\n",
    "            ratings = user_to_ratings_full[user2].copy()\n",
    "            hidden_ratings.append(ratings[user_to_rand_index_full[user1]])\n",
    "            del ratings[user_to_rand_index_full[user1]]\n",
    "            big_list.append(ratings)\n",
    "    rating = user_to_ratings_full[user1].copy()\n",
    "    del rating[user_to_rand_index_full[user1]]\n",
    "    sim  = cosine_similarity(X = big_list ,Y = [rating])\n",
    "    sim = np.reshape(sim,  (len(sim)))\n",
    "\n",
    "    combined = zip(sim, hidden_ratings)\n",
    "    combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    avg =0\n",
    "    nof = 10.0\n",
    "    for i in range(int(nof)):\n",
    "        avg+= combined[i][1]\n",
    "\n",
    "    feature_3_train.append(float(avg/nof))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now for each user, use the user_to_index_full and find the most similair users by omitting that index across the\n",
    "#sim_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#collaboritive filtering idea\n",
    "#data structures:\n",
    "#need a user to movies to ratings dictionary (done) (only needed for train data)\n",
    "#need a ordered set of all movies (done) (only needed for train data)\n",
    "\n",
    "#need to transform it into a user to list of all movies with user ratings or otherwise filled in ratings with sutiable average\n",
    "#(the list needs to be in a consistent order across users)\n",
    "#to fill in the averages we need a movie to average rating dictionary\n",
    "#(note: if no other user has rated the movie then fill it in with the overall movie average)\n",
    "#standardize the data row wise (why not columnwise???)\n",
    "#filter the users that have rated the movie to predict\n",
    "#then use cossine similairity on this set to find the most similair user the the chosen user\n",
    "#ignore the movie rating to predict with the cossine similarity function\n",
    "\n",
    "#Note: this is an expensive task and the number of users may have to be truncated before running\n",
    "\n",
    "\n",
    "\n",
    "#ideas: \n",
    "#idea1: \n",
    "#collaborative filtering:\n",
    "#https://towardsdatascience.com/predict-movie-ratings-with-user-based-collaborative-filtering-392304b988af\n",
    "#https://www.geeksforgeeks.org/user-based-collaborative-filtering/#\n",
    "#https://www.youtube.com/watch?v=3ecNC-So0r4&ab_channel=CodeHeroku\n",
    "#idea2: \n",
    "#note: the model should be scored based on how close a predition is to a threshold of .5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38879705 0.28910693]\n",
      "-0.06382124560607538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "# seed_int = 3\n",
    "# random.seed(seed_int)\n",
    "\n",
    "train_inputs = []\n",
    "\n",
    "for feature_1, feature_2 in zip(feature_1_train, feature_2_train):\n",
    "    train_inputs.append([feature_1, feature_2])\n",
    "\n",
    "\n",
    "train_inputs = scalar.fit_transform(train_inputs)\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes = (10,10,10), max_iter = 10000)\n",
    "reg.fit(train_inputs, true_rating_train)\n",
    "\n",
    "\n",
    "results = permutation_importance(reg, train_inputs,true_rating_train)\n",
    "print(results[\"importances_mean\"])\n",
    "\n",
    "\n",
    "test_inputs = []\n",
    "\n",
    "for feature_1, feature_2 in zip(feature_1_test, feature_2_test):\n",
    "    test_inputs.append([feature_1, feature_2])\n",
    "\n",
    "test_inputs = scalar.fit_transform(test_inputs)\n",
    "\n",
    "predictions = reg.predict(test_inputs)\n",
    "print(r2_score(true_rating_test, predictions))\n",
    "\n",
    "\n",
    "#grouping similair movies\n",
    "#users: 400 train 100 test\n",
    "#cluster: 10\n",
    "#hidden layers: (20,15,10)\n",
    "#number of ratings: greater than 50 and less than 75\n",
    "#upper seed_int = none\n",
    "#lower seed_int = none\n",
    "#score: 0.19422281654929585\n",
    "\n",
    "#grouping similair movies\n",
    "#users: 400 train 100 test\n",
    "#cluster: 20\n",
    "#hidden layers: (20,15,10)\n",
    "#number of ratings: greater than 50 and less than 75\n",
    "#upper seed_int = none\n",
    "#lower seed_int = none\n",
    "#score: 0.2168881681272362\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is for observations and process descriptions\n",
    "\n",
    "\n",
    "#How will the model work to predict ratings in the test data???\n",
    "#for each test user a movie is randomly selected\n",
    "\n",
    "#The prediction will be made with the word counts for the ratings by the user but using the\n",
    "#the words_in_order of the train data for faster runtime (may change later)\n",
    "\n",
    "#besides an average for a number of simailair movies the user has watched, the easy\n",
    "#parameter is average rating since it only considers ratings from train data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# commented out for testing\n",
    "# predicted = []\n",
    "# for user in user_to_movie_id_to_corpus_train.keys(): \n",
    "#     if(len(movie_id_to_ratings[user_to_rand_movie_id[user]])==1):\n",
    "#         predicted.append(overall_average)\n",
    "#     else:\n",
    "#         predicted.append(float(((movie_id_to_average_rating[user_to_rand_movie_id[user]]\n",
    "#                         *len(movie_id_to_ratings[user_to_rand_movie_id[user]]))\n",
    "#                         -user_to_movie_id_to_rating[user][user_to_rand_movie_id[user]])\n",
    "#                         /(len(movie_id_to_ratings[user_to_rand_movie_id[user]])-1)))\n",
    "        \n",
    "# nof_overall_avg = 0\n",
    "# for item in predicted:\n",
    "#     if item == overall_average:\n",
    "#         nof_overall_avg += 1\n",
    "# print(nof_overall_avg)\n",
    "# print(r2_score(true, predicted))\n",
    "\n",
    "\n",
    "#now for the user comparison logic (need user to list of movie ratings)\n",
    "#fill in ratings that the user hasn't watched with the method above\n",
    "#then cluster the users by their ratings\n",
    "\n",
    "#note: agglomerative clustering might make more sense here since k-means has random init for centroids...\n",
    "#note: to guess a new users rating requires that none of that users ratings have been used to train the model\n",
    "#The data needs to be split into test and train before modeling the algorithm on the train data\n",
    "\n",
    "#Training process:\n",
    "#split data into test and train data\n",
    "#proceed with train data...\n",
    "#cluster movies by the tokens with range for k\n",
    "#cluster users by the ratings with range for k and (fill in ratings for movies a users hasn't watched with some guess)\n",
    "#guess: this can be obtained by clustering the movies that the user has watched...\n",
    "#for each movie the user hasn't watched find the cluster that it belongs to with the highest possible k value\n",
    "#that the user has at least one movie belonging to one of the clusters and then take the average of those movies\n",
    "#this is exactly like a later training step excpet it is applied to all the movies the user watched\n",
    "\n",
    "#for a single randomly chosen movie from each user in the trainging data...\n",
    "\n",
    "#find the cluster the movie belongs to \n",
    "#find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#take the average score of these movies\n",
    "#find the cluster the user belongs to\n",
    "#find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#train an mlp model with both averages and perhaps some extra statistics as features...\n",
    "#using the given movie ratings as actuals\n",
    "\n",
    "\n",
    "#The process of predicting a rating:\n",
    "#1. find the cluster the movie belongs to \n",
    "#2. find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#3. take the average score of these movies\n",
    "#4. find the cluster the user belongs to\n",
    "#5. find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#6. input into the trained mlp model both averages and perhaps some extra statistics\n",
    "#7. make predictions and test against the randomly chosen movies actual ratings\n",
    "\n",
    "\n",
    "#summary:\n",
    "#find cluster for movie -> find movies part of the same clusters that the users rated -> average\n",
    "#question: are the clusters unique to the movies the user has watched or to all movies???\n",
    "#what is the technical difference???\n",
    "#is this the same as finding the most simimlair movie the user rated and copying the rating???\n",
    "\n",
    "#find cluster for user -> find the ratings for the movie by people in the same cluster -> average\n",
    "\n",
    "#other avenues considered:\n",
    "#idea 1:\n",
    "#for the first process, instead of averaging the movies that only the user rated, find other users that are...\n",
    "#like the user in question and find the average for that movie cluster\n",
    "#Problem: it is better to get the users raw opionion rather than generalizing it to some like minded users\n",
    "#there is an extra costly step to this\n",
    "#idea 2: \n",
    "#for the second process, instead of finding the average rating for the movie in the same cluster of users...\n",
    "#also find the average rating of movies that are like the movie in question \n",
    "#Problem, it is better to get the movies rating itself as it would be the most accurate indicator\n",
    "#there is an extra costly step to this\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test with overall averges:\n",
    "#note: users have between 50 and 75 ratings each\n",
    "#note: there are 250 users who are taken into account \n",
    "#note: seed int is one for above cells (cells part of creating the csv file)\n",
    "\n",
    "#with seed int == 3, 4 taking overall averages: 0.09565753948597455\n",
    "#with seed_int == 1, 2 taking overall averages: 0.070404868516315\n",
    "#with seed_int == 2, 5 taking overall averages: 0.11310085954932936\n",
    "#with seed_int == 4, 6 taking overall averages: 0.07125374341347135\n",
    "#with seed_int == 5, 4 taking overall averages: 0.17736444913943628\n",
    "#compute time: 11 minutes\n",
    "\n",
    "\n",
    "#test with users related movies:\n",
    "#is there a magic proportion of movies to average???\n",
    "#note: this is taking around the same time as the above tests meaning \n",
    "#there could be more users to include in analysis with little increase in runtime\n",
    "#k fold cross validation could be effective\n",
    "#https://www.youtube.com/watch?v=TIgfjmp-4BA&ab_channel=Udacity\n",
    "\n",
    "#effect of choosing a random seed...\n",
    "#https://towardsdatascience.com/how-to-use-random-seeds-effectively-54a4cd855a79\n",
    "\n",
    "#try tinkering with the number of similair movies to average\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
