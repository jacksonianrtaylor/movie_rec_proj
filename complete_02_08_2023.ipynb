{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    length = len(array[8])\n",
    "    if(array[8][length-4:]==\"<NA>\"):\n",
    "        return False\n",
    "    length = len(array[9])\n",
    "    if(array[9][length-4:]==\"<NA>\"):\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#convert the dataframe into an array and build a dictionary\n",
    "user_to_data = dict()\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "\n",
    "#get all unique user ids\n",
    "list_of_user_ids = []\n",
    "last_id  = -1\n",
    "for item in complete_array:\n",
    "    if(item[0]!= last_id):\n",
    "        list_of_user_ids.append(item[0])\n",
    "        last_id = item[0]\n",
    "\n",
    "\n",
    "index  = 0\n",
    "#this has been tested with 5000, 10000, 20000, 100000\n",
    "nof_users = 20000\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, nof_users):\n",
    "    user_to_data[list_of_user_ids[i]] = []\n",
    "    for j in range(index, len(complete_array)):\n",
    "        if complete_array[j][0] == list_of_user_ids[i]:\n",
    "            #condition is checked for complete_array[j]\n",
    "            if(condition(complete_array[j])):\n",
    "                #this is where data is tranformed\n",
    "                transformed = provide_data(complete_array[j])\n",
    "                user_to_data[list_of_user_ids[i]].append(transformed)         \n",
    "        else:\n",
    "            #ignore if the number of ratings for a user is too small\n",
    "            if (len(user_to_data[list_of_user_ids[i]])<10):\n",
    "                del user_to_data[list_of_user_ids[i]]\n",
    "            index = j+1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for key in user_to_data.keys():\n",
    "        writer.writerows(user_to_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ordered_set import OrderedSet\n",
    "\n",
    "#movie id to list of its ratings by all users\n",
    "movie_to_ratings = dict()\n",
    "\n",
    "#user id to the ratings of movies by the user\n",
    "#actually includes whole data row from constucted data...\n",
    "user_to_ratings = dict()\n",
    "\n",
    "#The list created by the constructed data csv is in order by user id\n",
    "#This code populates movie_to_ratings and user_to_ratings\n",
    "user_id = -1\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_ratings[row[0]] = [row]\n",
    "    else:\n",
    "        user_to_ratings[row[0]].append(row)\n",
    "\n",
    "    if(row[1] in movie_to_ratings.keys()):\n",
    "        movie_to_ratings[row[1]].append(row[2])\n",
    "    else:\n",
    "        movie_to_ratings[row[1]] = [row[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#dictionary of user id to a list of strings of combined textual features for each movie rated by the user\n",
    "#the strings do not include ratings or movie id\n",
    "\n",
    "#could this instead be user to movie id to corpus\n",
    "user_to_movie_id_to_corpus = dict()\n",
    "\n",
    "\n",
    "# WordNetLemmatizer().lemmatize(token.lower())\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for user in user_to_ratings.keys():\n",
    "    movie_strings = dict()\n",
    "    for movie_data in user_to_ratings[user]:\n",
    "        movie_string = \"\"\n",
    "        #avoid the first three data points (user id, movieid, and rating)\n",
    "        #use only the text data\n",
    "        for index in range (3,len(movie_data)):\n",
    "            if(index!= len(movie_data)-1):\n",
    "                movie_string+= movie_data[index]+\" \"\n",
    "            else:\n",
    "                movie_string+= movie_data[index]\n",
    "        cleaned = remove_stopwords(movie_string)\n",
    "        #why recreate the string when you could accept it as a list???\n",
    "        cleaned = \" \".join([wnl.lemmatize(word) for word in cleaned.split(\" \")])\n",
    "        movie_strings[movie_data[1]] = cleaned\n",
    "    user_to_movie_id_to_corpus[user] = movie_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m temp \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m     93\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words_in_order:\n\u001b[1;32m---> 94\u001b[0m     temp[word] \u001b[39m=\u001b[39m user_to_movie_id_to_corpus_train[user][movie_id]\u001b[39m.\u001b[39mcount(word)\n\u001b[0;32m     95\u001b[0m user_to_movie_id_to_words_to_counts[user][movie_id] \u001b[39m=\u001b[39m temp\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m movie_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m movie_id_to_words_to_count\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import statistics\n",
    "import math\n",
    "from ordered_set import OrderedSet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "\n",
    "#get average rating for a single movie amoung all users who rated it\n",
    "def get_avg_movie_rating(movie_id):\n",
    "    ret =0 \n",
    "    cnt = 0\n",
    "    for item in movie_to_ratings[movie_id]:\n",
    "        ret+= float(item)\n",
    "        cnt+=1\n",
    "    return float(ret/cnt)\n",
    "\n",
    "\n",
    "#get all the movie ratings from a single user\n",
    "def get_user_ratings(user_id):\n",
    "    ret = []\n",
    "    for item in user_to_ratings[user_id]:\n",
    "        ret.append(float(item[2]))\n",
    "    return ret\n",
    "\n",
    "def get_user_movie_id_to_rating(user_id):\n",
    "    ret = dict()\n",
    "    for item in user_to_ratings[user_id]:\n",
    "        ret[int(item[1])] = float(item[2])\n",
    "    return ret\n",
    "\n",
    "\n",
    "#dictionary of user to list of ratings for every movie in train set, even movies not rated by the user...\n",
    "#they will be marked with a rating of -1\n",
    "#the empty ratings will be filled with a similair process to below (taking the rating of the most simlair user)\n",
    "#this must be done before clustering\n",
    "\n",
    "user_to_movie_id_to_rating = dict()\n",
    "user_to_rand_movie_id = dict()\n",
    "user_to_rand_words_to_counts = dict()\n",
    "user_to_movie_id_to_words_to_counts = dict()\n",
    "user_to_movie_id_to_corpus_test = dict()\n",
    "movie_id_to_words_to_count = dict()\n",
    "words_in_order = OrderedSet()\n",
    "#how does copy work here???\n",
    "user_to_movie_id_to_corpus_train = user_to_movie_id_to_corpus.copy()\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    user = random.choice(list(user_to_movie_id_to_corpus_train.keys()))\n",
    "    user_to_movie_id_to_corpus_test[user] = user_to_movie_id_to_corpus_train[user]\n",
    "    user_to_movie_id_to_corpus_train.pop(user)\n",
    "\n",
    "\n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    #copy needed???\n",
    "    movie_id_to_rating = get_user_movie_id_to_rating(user)\n",
    "    user_to_rand_movie_id[user] = random.choice(list(user_to_movie_id_to_corpus_train[user].keys()))\n",
    "    temp = dict()\n",
    "    for movie_id in movie_id_to_rating.keys():\n",
    "        if movie_id != user_to_rand_movie_id[user]:\n",
    "            if movie_id in movie_id_to_rating.keys():\n",
    "                temp[movie_id] = movie_id_to_rating[movie_id]\n",
    "            else:\n",
    "                temp[movie_id] = -1\n",
    "    user_to_movie_id_to_rating[user] = temp\n",
    "\n",
    "\n",
    "#note: the words should be stored as a list and sorted before this and the next process\n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    for movie_id in user_to_movie_id_to_corpus_train[user].keys():\n",
    "        if movie_id != user_to_rand_movie_id[user]:\n",
    "            for word in user_to_movie_id_to_corpus_train[user][movie_id].split(\" \"):\n",
    "                words_in_order.add(word)\n",
    "\n",
    "            \n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    user_to_movie_id_to_words_to_counts[user] = dict()\n",
    "    for movie_id in user_to_movie_id_to_corpus_train[user].keys():\n",
    "        if movie_id != user_to_rand_movie_id[user]:\n",
    "            temp = dict()\n",
    "            for word in words_in_order:\n",
    "                temp[word] = user_to_movie_id_to_corpus_train[user][movie_id].count(word)\n",
    "            user_to_movie_id_to_words_to_counts[user][movie_id] = temp\n",
    "            if movie_id not in movie_id_to_words_to_count.keys():\n",
    "                movie_id_to_words_to_count[movie_id] = temp\n",
    "        else:\n",
    "            temp = dict()\n",
    "            for word in words_in_order:\n",
    "                temp[word] = user_to_movie_id_to_corpus_train[user][movie_id].count(word)\n",
    "            user_to_rand_words_to_counts[user] = temp\n",
    "\n",
    "\n",
    "#what if a new user to be tested has new words that are new to words_in_order???\n",
    "#what if a user simply has more words than normal, need to normalize(not this is doen with cossine similairity)\n",
    "#what if a user rated a movie that has not been rated by any users??? (is it omited ???)\n",
    "#do the randomly selected users meed to be omited from the words_in_order and user_to_list_of_words_to_counts\n",
    "#to emulate completely new users???\n",
    "\n",
    "\n",
    "def predict(user, words_to_counts):\n",
    "    #what kind of copy is needed???\n",
    "    #copy.deepcopy???\n",
    "    temp_movie_id_to_words_to_counts = user_to_movie_id_to_words_to_counts[user].copy()\n",
    "\n",
    "    values = []\n",
    "    for movie_id in temp_movie_id_to_words_to_counts.keys():\n",
    "        values.append(temp_movie_id_to_words_to_counts[movie_id].values())\n",
    "    \n",
    "    cosine_sim = cosine_similarity(X = values ,Y = [words_to_counts.values()])\n",
    "    #not sure if reshape is needed???\n",
    "    cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "    ratings = []\n",
    "    for movie_id in user_to_movie_id_to_rating[user].keys():\n",
    "        ratings.append(user_to_movie_id_to_rating[user][movie_id])\n",
    "\n",
    "\n",
    "    max = -10\n",
    "    pred_rating = ratings[0]\n",
    "\n",
    "    #note: for accuracy purposes, the user should have a certain number of rated movies\n",
    "    #instead of selecting the most similair movie should clusters be used???\n",
    "    #for now this simple method will suffice\n",
    "    #are the ratings lined up with the cossine similarities???\n",
    "    #yes the order of user_to_ratings is used to make user_to_corpus_list...\n",
    "    #which is used to make lst_of_word_to_nof_occurances\n",
    "\n",
    "    for sim, rat in zip(cosine_sim, ratings):\n",
    "        if sim>max:\n",
    "            max = sim\n",
    "            pred_rating = rat\n",
    "\n",
    "    return pred_rating\n",
    "\n",
    "\n",
    "#fill in the missing ratings for a user\n",
    "def fill_in_ratings(user):\n",
    "    for movie_id in user_to_movie_id_to_rating[user].keys():\n",
    "        if user_to_movie_id_to_rating[user][movie_id] == -1:\n",
    "            user_to_movie_id_to_rating[user][movie_id] = predict(user, movie_id_to_words_to_count[movie_id])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now for the user comparison logic (need user to list of movie ratings)\n",
    "#fill in ratings that the user hasn't watched with the method above\n",
    "#then cluster the users by their ratings\n",
    "\n",
    "#note: agglomerative clustering might make more sense here since k-means has random init for centroids...\n",
    "#note: to guess a new users rating requires that none of that users ratings have been used to train the model\n",
    "#The data needs to be split into test and train before modeling the algorithm on the train data\n",
    "\n",
    "#Training process:\n",
    "#split data into test and train data\n",
    "#proceed with train data...\n",
    "#cluster movies by the tokens with range for k\n",
    "#cluster users by the ratings with range for k and (fill in ratings for movies a users hasn't watched with some guess)\n",
    "#guess: this can be obtained by clustering the movies that the user has watched...\n",
    "#for each movie the user hasn't watched find the cluster that it belongs to with the highest possible k value\n",
    "#that the user has at least one movie belonging to one of the clusters and then take the average of those movies\n",
    "#this is exactly like a later training step excpet it is applied to all the movies the user watched\n",
    "\n",
    "#for a single randomly chosen movie from each user in the trainging data...\n",
    "\n",
    "#find the cluster the movie belongs to \n",
    "#find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#take the average score of these movies\n",
    "#find the cluster the user belongs to\n",
    "#find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#train an mlp model with both averages and perhaps some extra statistics as features...\n",
    "#using the given movie ratings as actuals\n",
    "\n",
    "\n",
    "#The process of predicting a rating:\n",
    "#1. find the cluster the movie belongs to \n",
    "#2. find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#3. take the average score of these movies\n",
    "#4. find the cluster the user belongs to\n",
    "#5. find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#6. input into the trained mlp model both averages and perhaps some extra statistics\n",
    "#7. make predictions and test against the randomly chosen movies actual ratings\n",
    "\n",
    "\n",
    "#summary:\n",
    "#find cluster for movie -> find movies part of the same clusters that the users rated -> average\n",
    "#question: are the clusters unique to the movies the user has watched or to all movies???\n",
    "#what is the technical difference???\n",
    "#is this the same as finding the most simimlair movie the user rated and copying the rating???\n",
    "\n",
    "#find cluster for user -> find the ratings for the movie by people in the same cluster -> average\n",
    "\n",
    "#other avenues considered:\n",
    "#idea 1:\n",
    "#for the first process, instead of averaging the movies that only the user rated, find other users that are...\n",
    "#like the user in question and find the average for that movie cluster\n",
    "#Problem: it is better to get the users raw opionion rather than generalizing it to some like minded users\n",
    "#there is an extra costly step to this\n",
    "#idea 2: \n",
    "#for the second process, instead of finding the average rating for the movie in the same cluster of users...\n",
    "#also find the average rating of movies that are like the movie in question \n",
    "#Problem, it is better to get the movies rating itself as it would be the most accurate indicator\n",
    "#there is an extra costly step to this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
