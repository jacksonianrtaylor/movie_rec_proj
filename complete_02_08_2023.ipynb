{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_11480\\1973637973.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  movies_full= movies_full.drop(movies_full[movies_full['genres'][len(movies_full['genres']) -2:] == \"[]\"].index)\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32md:\\repos\\movie_rec_proj\\complete_02_08_2023.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m movies_full \u001b[39m=\u001b[39m movies_full\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#this is strcitly for testing purposes\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m movies_full\u001b[39m=\u001b[39m movies_full\u001b[39m.\u001b[39mdrop(movies_full[movies_full[\u001b[39m'\u001b[39;49m\u001b[39mgenres\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39mlen\u001b[39;49m(movies_full[\u001b[39m'\u001b[39;49m\u001b[39mgenres\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m:] \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m[]\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mindex)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m drop_indices \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(movies_full)):\n",
      "File \u001b[1;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3798\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3796\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3797\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3798\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_bool_array(key)\n\u001b[0;32m   3800\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m is_single_key \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3851\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3846\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mItem wrong length \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(key)\u001b[39m}\u001b[39;00m\u001b[39m instead of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3847\u001b[0m     )\n\u001b[0;32m   3849\u001b[0m \u001b[39m# check_bool_indexer will throw exception if Series key cannot\u001b[39;00m\n\u001b[0;32m   3850\u001b[0m \u001b[39m# be reindexed to match DataFrame rows\u001b[39;00m\n\u001b[1;32m-> 3851\u001b[0m key \u001b[39m=\u001b[39m check_bool_indexer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex, key)\n\u001b[0;32m   3852\u001b[0m indexer \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]\n\u001b[0;32m   3853\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:2552\u001b[0m, in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2550\u001b[0m indexer \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_indexer_for(index)\n\u001b[0;32m   2551\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39min\u001b[39;00m indexer:\n\u001b[1;32m-> 2552\u001b[0m     \u001b[39mraise\u001b[39;00m IndexingError(\n\u001b[0;32m   2553\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnalignable boolean Series provided as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2554\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mindexer (index of the boolean Series and of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2555\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe indexed object do not match).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2556\u001b[0m     )\n\u001b[0;32m   2558\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   2560\u001b[0m \u001b[39m# fall through for boolean\u001b[39;00m\n",
      "\u001b[1;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#Data source:\n",
    "#https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#condition is checked for: \"genres\",\"production_companies\",\"keywords\", \"cast\"\n",
    "#from movies_full: \"genres\", \"production_companies\"\n",
    "#from keywords: keywords\n",
    "#from credits: cast\n",
    "\n",
    "movies_full = pd.read_csv('large_source_data/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "movies_full = movies_full.dropna()\n",
    "movies_full = movies_full.reset_index()\n",
    "\n",
    "#this is strcitly for testing purposes\n",
    "# currently does not work\n",
    "# movies_full= movies_full.drop(movies_full[movies_full['genres'][len(movies_full['genres']) -2:] == \"[]\"].index)\n",
    "\n",
    "\n",
    "drop_indices = []\n",
    "for i in range(len(movies_full)):\n",
    "    len_1 = len(movies_full.iloc[i].loc[\"genres\"])                   \n",
    "    if(movies_full.iloc[i].loc[\"genres\"][len_1 -2:] == \"[]\"):\n",
    "        drop_indices.append(i)\n",
    "        continue\n",
    "    len_2 = len(movies_full.iloc[i].loc[\"production_companies\"])\n",
    "    if(movies_full.iloc[i].loc[\"production_companies\"][len_2 -2:] == \"[]\"):\n",
    "        drop_indices.append(i)    \n",
    "        continue   \n",
    "\n",
    "movies_full = movies_full.drop(labels=drop_indices, axis = 0)\n",
    "movies_full = movies_full.reset_index(names = \"index_1\")\n",
    "\n",
    "\n",
    "ratings = pd.read_csv('large_source_data/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "ratings.dropna()\n",
    "ratings = ratings.reset_index(names = \"index_2\")\n",
    "\n",
    "\n",
    "keywords = pd.read_csv('large_source_data/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "keywords.dropna()\n",
    "keywords = keywords.reset_index()\n",
    "\n",
    "drop_indices = []\n",
    "for i in range(len(keywords)):\n",
    "    len_1 = len(keywords.iloc[i].loc[\"keywords\"])                   \n",
    "    if(keywords.iloc[i].loc[\"keywords\"][len_1 -2:] == \"[]\"):\n",
    "        drop_indices.append(i)\n",
    "\n",
    "keywords = keywords.drop(labels=drop_indices, axis = 0)\n",
    "keywords = keywords.reset_index(names = \"index_3\")\n",
    "\n",
    "\n",
    "credits = pd.read_csv(\"large_source_data/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "credits.dropna()\n",
    "credits = credits.reset_index()\n",
    "\n",
    "drop_indices = []\n",
    "for i in range(len(credits)):\n",
    "    len_1 = len(credits.iloc[i].loc[\"cast\"])                   \n",
    "    if(credits.iloc[i].loc[\"cast\"][len_1 -2:] == \"[]\"):\n",
    "        drop_indices.append(i)\n",
    "\n",
    "credits = credits.drop(labels=drop_indices, axis = 0)\n",
    "credits = credits.reset_index(names = \"index_4\")\n",
    "\n",
    "\n",
    "#default is inner: this only keeps movies that have the id existing in both dataframes...\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "#this may be an unessesary step...\n",
    "# complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "\n",
    "print(complete.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users: 260788\n",
      "Number of faults: 0\n",
      "Number of users left after filtering: 29185\n",
      "Average number of ratings for the filtered user: 5.065821483638856\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "# seed for consistent results across runtime\n",
    "# not needed if the previous cell is run\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "#a method simlair to this is used in the previous cell to above unecessary expense\n",
    "def condition(array):\n",
    "    #is this necessary with the dropNa function\n",
    "    #answer: yes\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    \n",
    "    #this is not needed due to the dropNa function used above...\n",
    "    # length = len(array[8])\n",
    "    # if(array[8][length-4:]==\"<NA>\"):\n",
    "    #     return False\n",
    "    # length = len(array[9])\n",
    "    # if(array[9][length-4:]==\"<NA>\"):\n",
    "    #     return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "#transform dataframe data fram into numpy array\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "#a list of the unique user ids\n",
    "list_of_user_ids = list(complete[\"userId\"].unique())\n",
    "\n",
    "#a dictionary of user ids to the number of ratings with that id\n",
    "counts = complete['userId'].value_counts()\n",
    "\n",
    "#gaps is a list of the number of movies each user rated where a user corresponds with the index\n",
    "gaps = [counts[id] for id in list_of_user_ids]\n",
    "\n",
    "index  = 0\n",
    "\n",
    "#this is how the filtered data is formated\n",
    "user_to_data = []\n",
    "\n",
    "#this is the total number of users in the whole dataset\n",
    "#total number of users: 261306, 260788 with filtering...\n",
    "total_nof_users = len(list_of_user_ids)\n",
    "print(\"Total number of users:\", total_nof_users)\n",
    "\n",
    "#this is the number of desired users before filtering...\n",
    "#it controls the frequencey of any given user being tested \n",
    "desired_nof_users_before_filter = 61306\n",
    "\n",
    "#this is the minimum number of ratings a user must have to be tested\n",
    "#this can be altered to fully test more realistic senarios\n",
    "#for instance: what if test users dont have 100 ratings???\n",
    "min_number_of_users = 100\n",
    "\n",
    "#this is collected for insite\n",
    "avg = 0.0\n",
    "cnt = 0.0\n",
    "\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, total_nof_users):\n",
    "    #generate a random float to determine a pass for the user\n",
    "    if (random.random()<float(desired_nof_users_before_filter/total_nof_users)):\n",
    "        # another example condition:\n",
    "        # if(gaps[i] >= 50 and gaps[i]<=75):\n",
    "        # if(gaps[i] >= min_number_of_users):\n",
    "        if(gaps[i] <= 10):\n",
    "            user_to_data.append([])\n",
    "            last_index = len(user_to_data) -1\n",
    "            for j in range(index, len(complete_array)):\n",
    "                if complete_array[j][0] == list_of_user_ids[i]:\n",
    "                    #condition is checked for complete_array[j] to move onto the \"append data\" step\n",
    "                    # if(condition(complete_array[j])):\n",
    "                        #this is where the data is transformed...\n",
    "                    transformed = provide_data(complete_array[j])\n",
    "                    user_to_data[last_index].append(transformed)    \n",
    "                else:\n",
    "                    avg += len(user_to_data[last_index])\n",
    "                    cnt+=1\n",
    "                    index = j\n",
    "                    break\n",
    "        else:\n",
    "            index += gaps[i]             \n",
    "    else:\n",
    "        index += gaps[i]\n",
    "\n",
    "\n",
    "#this is the number of users that dont have any viable ratings for movies\n",
    "nof_faults =0\n",
    "\n",
    "#Go through user_to_data and re-index the users in list order\n",
    "#this is for simplicity and readability \n",
    "#also checks for and remove users with no viable movies based on the condition check in the loop above \n",
    "#(note: this is unlikely to happen with a minimum number of user rating of 100)\n",
    "index = 0\n",
    "for i in range(len(user_to_data)):\n",
    "    if len(user_to_data[index]) == 0:\n",
    "        del user_to_data[index]\n",
    "        nof_faults+=1\n",
    "        index -= 1\n",
    "    else:\n",
    "        for j in range(len(user_to_data[index])):\n",
    "            user_to_data[index][j][0] = index\n",
    "    index+=1\n",
    "\n",
    "\n",
    "print(\"Number of faults:\", nof_faults)\n",
    "\n",
    "#How many users pass the conditions in the loop\n",
    "print(\"Number of users left after filtering:\", len(user_to_data))\n",
    "\n",
    "#average number of ratings per users\n",
    "print(\"Average number of ratings for the filtered user:\", float(avg/cnt))\n",
    "\n",
    "\n",
    "#note: another method to complete the above is to remove movies from the data set that dont have all the required data before completing the if(condition(complete_array[j]))\n",
    "#step for every user\n",
    "#^ completed\n",
    "\n",
    "#another way of the above is to remove all users that don't have enough ratings before running the main loop\n",
    "#then we knwo that the remainign users are inthe proportion of (desired_nof_users_before_filter/total_nof_users)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "\n",
    "#question: would renaming the user ids as indexes in their order be helpful???\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"constructed_data/constructed_data.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for i in range(len(user_to_data)):\n",
    "        writer.writerows(user_to_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructed_data.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructed_data/constructed_data.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "#used with every random function except for the last cell where a certain number of models are tested and accumulated with identiacal test and train data\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#user to data rows \n",
    "user_to_data = []\n",
    "user_to_data_train = []\n",
    "user_to_data_test = []\n",
    "user_id = -1\n",
    "\n",
    "#note: works when row[0] is also an index\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_data.append([row])\n",
    "    else:\n",
    "        user_to_data[int(row[0])].append(row)\n",
    "\n",
    "\n",
    "#these both can be increased for consistency as long as there is enough data\n",
    "#with the current configuration there are 4204 users\n",
    "#this can be increased by increasing the desired_nof_users_before_filter parameter above\n",
    "for i in range(4000):\n",
    "    index = random.randint(0, len(user_to_data)-1)\n",
    "    user_to_data_train.append(user_to_data[index])\n",
    "    del user_to_data[index]\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    index = random.randint(0, len(user_to_data_train)-1)\n",
    "    user_to_data_test.append(user_to_data_train[index])\n",
    "    del user_to_data_train[index]\n",
    "\n",
    "\n",
    "del user_to_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.130624693639974, 3.0286685310067956, 3.573263428822902, 3.3463300049429403, 2.991701485136822]\n",
      "[4.0, 2.5, 4.0, 4.0, 3.5]\n",
      "[3.4069034044135935, 3.5264684110064706, 3.0600993899167994, 2.8262482790149606, 2.716596377538777]\n",
      "[1.0, 3.5, 2.5, 3.0, 4.5]\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import json\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#the linalg is used from numpy instea of scipy\n",
    "import numpy as np\n",
    "#the version from numpy is used instead\n",
    "from scipy import linalg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.linalg import sqrtm\n",
    "import math\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "class user_type_vars():\n",
    "    def __init__(self):\n",
    "        #for each user of the user type, a dictionary of movie_id to the movies rating for each movie the user watched\n",
    "        self.user_to_movie_id_to_rating = [] \n",
    "\n",
    "        #for each user, a random choice of movie_id from all the movies the user watched to represent the target movie\n",
    "        self.user_to_target_movie_id = [] \n",
    "\n",
    "        #for each user, this is the index of the users target movie in the order of movies_in_order\n",
    "        #(train_users only)\n",
    "        self.user_to_target_index_full = [] \n",
    "\n",
    "        #for each user, includes ratings for all the movies in the entire train set \n",
    "        #missing ratings and target movie ratings are set to that movies average rating\n",
    "        #(train_users only)\n",
    "        self.user_to_ratings_full = [] \n",
    "\n",
    "        #for each user, includes ratings for all the movies in the entire train set\n",
    "        #the movies mean rating is subtracted from each rating\n",
    "        #missing ratings and target movie ratings are set to zero\n",
    "        #(train_users only)\n",
    "        self.user_to_ratings_full_transform = []\n",
    "\n",
    "        #for every movie watched by the user_type, a list of ratings\n",
    "        self.movie_id_to_ratings = dict()\n",
    "\n",
    "        #this is a set of every unique target movie for the user_type\n",
    "        self.target_movies = set()\n",
    "\n",
    "        #all the movies in order of the movies ratings for each user of the user type\n",
    "        self.movies_in_order = OrderedSet()\n",
    "\n",
    "        #model input features x\n",
    "        self.feature_1 = []\n",
    "        self.feature_2 = []\n",
    "        self.feature_3 = []\n",
    "\n",
    "        #model output feature y\n",
    "        self.user_to_target_rating  = [] \n",
    "\n",
    "\n",
    "#for most of the variables above a train and test version is used\n",
    "train_users = user_type_vars()\n",
    "test_users = user_type_vars()\n",
    "\n",
    "\n",
    "#This is the users average rating not including the chosen target movie\n",
    "#this is for all train users in order followed by the test users in order\n",
    "#(this not being used currently)\n",
    "user_to_average_rating = []\n",
    "\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "def load_feature_1_and_2(target_movies, movies_in_order, user_to_data, movie_id_to_ratings, user_to_movie_id_to_rating, user_to_target_movie_id, user_to_target_rating, feature_1, feature_2):\n",
    "   \n",
    "    #these are used to calculate the overall train rating\n",
    "    #this is used to fill in rating for movies that are only target movies (they dont have ratings)\n",
    "    overall_rating_sum = 0\n",
    "    overall_rating_count = 0\n",
    "\n",
    "    for i in range(len(user_to_data)):\n",
    "        movie_id_to_words = dict()\n",
    "        movie_id_to_rating = dict()\n",
    "        cnt = 0\n",
    "        total =0\n",
    "        rand_int = random.randint(0, len(user_to_data[i])-1)\n",
    "        for movie_data in user_to_data[i]:\n",
    "            if cnt == rand_int:    \n",
    "                target_movies.add(movie_data[1])\n",
    "                user_to_target_movie_id.append(movie_data[1])\n",
    "            else:\n",
    "                overall_rating_sum += float(movie_data[2])\n",
    "                overall_rating_count += 1\n",
    "                total += float(movie_data[2])\n",
    "\n",
    "                #this only runs when the movie is not the target movie because\n",
    "                #the target movies are thought to be the movies whose rating is to be predicted...\n",
    "                #not ratings that are already on record\n",
    "                if movie_data[1] in movie_id_to_ratings.keys():\n",
    "                    movie_id_to_ratings[movie_data[1]].append(float(movie_data[2]))\n",
    "                else:\n",
    "                    movie_id_to_ratings[movie_data[1]] = [float(movie_data[2])]\n",
    "\n",
    "            movie_string = \"\"\n",
    "\n",
    "            #use this to apply all the text data and combine in to a single list of words (repeats allowed):\n",
    "            # for index in range (3,len(movie_data)):\n",
    "            #     if(index!= len(movie_data)-1):\n",
    "            #         movie_string+= movie_data[index]+\" \"\n",
    "            #     else:\n",
    "            #         movie_string+= movie_data[index]\n",
    "\n",
    "\n",
    "            #all of the text columns and a few combinations of certain text columns were tested but they were not helpful in...\n",
    "            #increasing model perfromance (see below)\n",
    "\n",
    "\n",
    "            #Use this truncated code to only include the genre column strings:\n",
    "            movie_string = movie_data[4]\n",
    "\n",
    "            #lematization and conversion to lists\n",
    "            cleaned = remove_stopwords(movie_string)\n",
    "            cleaned = [wnl.lemmatize(word) for word in cleaned.split(\" \")]\n",
    "            cleaned = [word[:-1] for word in cleaned if word.endswith(\".\")] + [word for word in cleaned if not word.endswith(\".\")]\n",
    "\n",
    "            movie_id_to_words[movie_data[1]] = cleaned\n",
    "            movie_id_to_rating[movie_data[1]] = float(movie_data[2])\n",
    "            movies_in_order.add(movie_data[1])\n",
    "            cnt+=1\n",
    "\n",
    "        user_to_movie_id_to_rating.append(movie_id_to_rating)\n",
    "        user_to_average_rating.append(float(total/(cnt-1)))\n",
    "\n",
    "        #the current users list of words from all the movies they rated\n",
    "        users_words_in_order = OrderedSet()\n",
    "        for movie_id in movie_id_to_words.keys():\n",
    "            for word in movie_id_to_words[movie_id]:\n",
    "                users_words_in_order.add(word)\n",
    "\n",
    "\n",
    "        word_counts = [] #list of word counts for the users_words_in_order for each movie (excluding target)\n",
    "        target_word_counts = [] #word counts for the users_words_in_order for the target movie\n",
    "\n",
    "        #these are the scaled versions of variables directly above\n",
    "        #these are only relevant with user averages scalings opposed to movie average scaling...\n",
    "        #note: scaling also happens automatically below\n",
    "        word_counts_transformed = []\n",
    "        target_word_counts_transformed = []\n",
    "\n",
    "        #word count sums for each word in users_words_in_order for each user\n",
    "        sums = dict()\n",
    "\n",
    "        #for each movie the user watched record the wordcount for each word in users_words_in_order\n",
    "        for movie_id in movie_id_to_words.keys():\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "                temp_list = []\n",
    "                # sum = 0\n",
    "                for word in users_words_in_order:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                        # sum+=temp_dict[word]\n",
    "                        if word in sums.keys():\n",
    "                            sums[word] += temp_dict[word] \n",
    "                        else:\n",
    "                            sums[word] = temp_dict[word] \n",
    "                    else:\n",
    "                        temp_list.append(0) \n",
    "                        if word not in sums.keys():\n",
    "                            sums[word] = 0  \n",
    "\n",
    "                word_counts.append(temp_list)  \n",
    "\n",
    "                # append to word_counts_transformed:\n",
    "                # avg = float(sum/len(users_words_in_order))\n",
    "                # word_counts_transformed.append([x - avg for x in temp_list])\n",
    "            else:\n",
    "\n",
    "                temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "                temp_list = []\n",
    "                # sum = 0\n",
    "                for word in users_words_in_order:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                        # sum+=temp_dict[word]\n",
    "                        if word in sums.keys():\n",
    "                            sums[word] += temp_dict[word] \n",
    "                        else:\n",
    "                            sums[word] = temp_dict[word]             \n",
    "                    else:\n",
    "                        temp_list.append(0) \n",
    "                        if word not in sums.keys():\n",
    "                            sums[word] = 0 \n",
    "\n",
    "                target_word_counts = temp_list\n",
    "\n",
    "                # set target_word_counts_transformed:\n",
    "                # avg = float(sum/len(users_words_in_order))\n",
    "                # target_word_counts_transformed = [x - avg for x in temp_list]\n",
    "        \n",
    "\n",
    "        complete_word_counts = word_counts.copy()\n",
    "        complete_word_counts.append(target_word_counts)\n",
    "        transformed_word_counts = TfidfTransformer().fit_transform(complete_word_counts).toarray()\n",
    "\n",
    "\n",
    "        #populate ratings with the exception of the target rating \n",
    "        #also record the users target movie rating \n",
    "        ratings = []\n",
    "        for movie_id in movie_id_to_rating.keys():\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                ratings.append(movie_id_to_rating[movie_id])\n",
    "            else:\n",
    "                #this signifies the ratings to be predicted by the model\n",
    "                user_to_target_rating.append(movie_id_to_rating[movie_id])\n",
    "        \n",
    "\n",
    "        #potential functions of predict:\n",
    "        #return the average ratings from movies that are a like the target movie with cosine similairity\n",
    "        #unweighted average of all of the users movies\n",
    "        #weighted average of all the users movies (weights are based on cossine similarity)\n",
    "        def predict():\n",
    "            item_1 = 0 \n",
    "            item_2 = 0\n",
    "\n",
    "            # option 1: \n",
    "            # cosine_sim = linear_kernel(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "            #or\n",
    "            #cosine_sim = cosine_similarity(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "            # cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            # combined = zip(cosine_sim, ratings)\n",
    "            # combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "            # avg = 0\n",
    "            # nof = 10.0\n",
    "            # for i in range(int(nof)):\n",
    "            #     avg += combined[i][1]\n",
    "            # item_2 =  float(avg/nof)\n",
    "\n",
    "            #option 2:\n",
    "            #note: item 1 is a higher performing feature than any of the other methods in the function\n",
    "            sum = 0\n",
    "            for i in range(len(ratings)):\n",
    "                sum += ratings[i]\n",
    "            item_1 = float(sum/len(ratings))\n",
    "\n",
    "            #option 3:\n",
    "            #when the svd function is used:\n",
    "            # cosine_sim = cosine_similarity(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "\n",
    "            #when the svd function is not used:\n",
    "            cosine_sim = linear_kernel(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "            cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            numerator = 0\n",
    "            denominator = 0\n",
    "            item_2 = item_1\n",
    "            for i in range(len(ratings)):\n",
    "                numerator += float(cosine_sim[i]*ratings[i])\n",
    "                denominator += cosine_sim[i]\n",
    "            \n",
    "            if denominator != 0:\n",
    "                item_2 = float(numerator/denominator)\n",
    "        \n",
    "            return (item_1, item_2)\n",
    "        \n",
    "        \n",
    "        items = predict()\n",
    "\n",
    "        feature_1.append(items[0])\n",
    "        feature_2.append(items[1])\n",
    "            \n",
    "        \n",
    "    return float(overall_rating_sum/overall_rating_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pre_svd(movie_id_to_average_rating, movies_in_order, user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, \n",
    "               user_to_movie_id_to_rating, user_to_target_movie_id):\n",
    "    for i in range(len(user_to_movie_id_to_rating)):\n",
    "        ratings = []\n",
    "        transformed_ratings = []\n",
    "        index = 0\n",
    "\n",
    "\n",
    "        #what if there is no movie_id == user_to_target_movie_id[i]\n",
    "        #this can happen when a test users target movie is not in the train_users.movies_in_order...\n",
    "\n",
    "        #solution:\n",
    "\n",
    "        #this could be run once with only train_movies\n",
    "        #and then used to populate the train svd\n",
    "        #and then extract the prediction to train the model\n",
    "\n",
    "\n",
    "        #then again with all movies train_movies + test_movies\n",
    "        #then used to populate the full svd\n",
    "        #and then extract the prediction to test model\n",
    "\n",
    "\n",
    "        #note: movie_id_to_average_rating_train shouold onyl be used for the train run of this function\n",
    "        #for the test version of the this movie_id_to_average_rating_full should be used\n",
    "\n",
    "        for movie_id in movies_in_order:\n",
    "            if movie_id == user_to_target_movie_id[i]:\n",
    "                user_to_target_index_full.append(index)\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(movie_id_to_average_rating[movie_id] - movie_id_to_average_rating[movie_id]) \n",
    "\n",
    "            #note: It should not matter that user_to_movie_id_to_rating includes movie id equal to user_to_target_movie_id[i] since the above condition will flag before this condition\n",
    "            elif movie_id in user_to_movie_id_to_rating[i].keys():\n",
    "                ratings.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(user_to_movie_id_to_rating[i][movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            else:\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(movie_id_to_average_rating[movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            index +=1\n",
    "        user_to_ratings_full.append(ratings)\n",
    "        user_to_ratings_full_transform.append(transformed_ratings)\n",
    "\n",
    "\n",
    "#note: before passing to this function the data is normalized about the average movie ratings (not average user ratings)\n",
    "#each user train and test users have a single rating that needs to be trained against in the train case\n",
    "#and predicted in the test case\n",
    "\n",
    "#the svd can be applied to the combined data of the train and test sets\n",
    "#both movies that the user didn't watch and movies that should be guesses are...\n",
    "#transformed to have a value of zero before svd\n",
    "\n",
    "#the movie columns are taken from the train dataset...\n",
    "#senario: suppose a test user has a rating of a movie not part of the train set and it is not the target movie (ignore it)\n",
    "#senario: suppose a test user has a rating of a movie not part of the train set and it is the target movie (guess the rating instead of using svd)\n",
    "\n",
    "#...Once the UsV is created...\n",
    "#take the rating from the new UsV for the user row and movie column for the target movie\n",
    "#other option: cossine similairty on the U ignoring other test users\n",
    "\n",
    "\n",
    "def svd_full(user_to_ratings_full_transform, n, movie_id_to_average_rating):\n",
    "    #is this the source the random variation???\n",
    "    U, s, V = np.linalg.svd(user_to_ratings_full_transform, full_matrices=False)\n",
    "    \n",
    "    #simplify ratings to n features\n",
    "    s=np.diag(s)\n",
    "    s=s[0:n,0:n]\n",
    "    U=U[:,0:n]\n",
    "    V=V[0:n,:]\n",
    "\n",
    "    #reconstrcut to a new array\n",
    "    Us = np.dot(U,s)\n",
    "    UsV = np.dot(Us,V)\n",
    "    \n",
    "\n",
    "    #the keys of movie_id_to_ratings is in the same order of movies_in_order and therefore so is movie_id_to_average_rating_train\n",
    "    x = np.tile(list(movie_id_to_average_rating.values()), (UsV.shape[0],1))\n",
    "\n",
    "    #this tranforms the UsV row by row into the original rating scale (1-5)\n",
    "    UsV = UsV + x\n",
    "\n",
    "    #be consistent with data structures...\n",
    "    return list(UsV)\n",
    "\n",
    "\n",
    "\n",
    "overall_average_train = load_feature_1_and_2(train_users.target_movies, train_users.movies_in_order, user_to_data_train, train_users.movie_id_to_ratings, train_users.user_to_movie_id_to_rating, \n",
    "                                                         train_users.user_to_target_movie_id, train_users.user_to_target_rating, train_users.feature_1, train_users.feature_2)\n",
    "\n",
    "\n",
    "load_feature_1_and_2(test_users.target_movies, test_users.movies_in_order, user_to_data_test, test_users.movie_id_to_ratings, test_users.user_to_movie_id_to_rating, \n",
    "               test_users.user_to_target_movie_id,\n",
    "               test_users.user_to_target_rating, test_users.feature_1, test_users.feature_2)\n",
    "\n",
    "\n",
    "\n",
    "#Unlike the other feature loading functions it only makes sense to run this once since...\n",
    "#there is significantly difference processes for train and test data\n",
    "def load_feature_3():\n",
    "\n",
    "    movie_id_to_average_rating_train = dict()\n",
    "    movie_id_to_average_rating_full = dict()\n",
    "\n",
    "    #is all_movies_in_order still in order???\n",
    "    all_movies_in_order = train_users.movies_in_order|test_users.movies_in_order\n",
    "\n",
    "\n",
    "    #this is used to populate movie_id_to_average_rating_train and movie_id_to_average_rating_full...\n",
    "    #without skippig the movies are target movies  and not in (movies not in test_users.movie_id_to_ratings or train_users.movie_id_to_ratings)\n",
    "    for movie in all_movies_in_order:\n",
    "        temp = 0\n",
    "        if(movie in train_users.movie_id_to_ratings and movie in test_users.movie_id_to_ratings):\n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_train[movie] = float(temp/len(train_users.movie_id_to_ratings[movie])) \n",
    "\n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_full[movie] = float(temp/(len(train_users.movie_id_to_ratings[movie])+len(test_users.movie_id_to_ratings[movie])))  \n",
    "\n",
    "        elif(movie in train_users.movie_id_to_ratings):\n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_train[movie] = float(temp/len(train_users.movie_id_to_ratings[movie]))\n",
    "            movie_id_to_average_rating_full[movie] = movie_id_to_average_rating_train[movie]\n",
    "\n",
    "        elif(movie in test_users.movie_id_to_ratings):\n",
    "            #is the movie a target movie in the train set that isn't in train_users.movies_id_to_ratings???         \n",
    "            if(movie in train_users.target_movies):\n",
    "                movie_id_to_average_rating_train[movie] = overall_average_train\n",
    "\n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_full[movie] = float(temp/len(test_users.movie_id_to_ratings[movie]))\n",
    "        else:\n",
    "            #is the movie a target movie in the train set that isn't in train_users.movie_id_to_ratings???\n",
    "            #is the movie a target movie in the test set that isn't in test_users.movie_id_to_ratings???\n",
    "            if(movie in train_users.target_movies):\n",
    "                movie_id_to_average_rating_train[movie] = overall_average_train\n",
    "                movie_id_to_average_rating_full[movie] = overall_average_train\n",
    "            else:\n",
    "                movie_id_to_average_rating_full[movie] = overall_average_train\n",
    "   \n",
    "\n",
    "    #for all users in train and then test order\n",
    "    full_user_to_ratings_full_transform = []\n",
    "    full_user_to_ratings_full = []\n",
    "    full_user_to_target_index_full = []\n",
    "\n",
    "\n",
    "    #this makes a comprehensive list of the train data followed by the test users data\n",
    "    full_user_to_movie_id_to_rating  = train_users.user_to_movie_id_to_rating + test_users.user_to_movie_id_to_rating\n",
    "    full_user_to_target_movie_id = train_users.user_to_target_movie_id + test_users.user_to_target_movie_id\n",
    "\n",
    "\n",
    "    #This is used to scale the ratings and store in train_users.user_to_ratings_full_transform and full_user_to_ratings_full_transform\n",
    "    #This will transform the target movie ratings and unrated movies to zero\n",
    "\n",
    "    #run once with only train data to train model\n",
    "    #run again with train and test data to evaluate model...\n",
    "\n",
    "    pre_svd(movie_id_to_average_rating_train, train_users.movies_in_order, train_users.user_to_ratings_full_transform, train_users.user_to_ratings_full, train_users.user_to_target_index_full, \n",
    "                train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id)\n",
    "\n",
    "    pre_svd(movie_id_to_average_rating_full, all_movies_in_order, full_user_to_ratings_full_transform, full_user_to_ratings_full, full_user_to_target_index_full, \n",
    "                full_user_to_movie_id_to_rating, full_user_to_target_movie_id)\n",
    "\n",
    "\n",
    "    #In practice, there is a train and a test set, the train set is what the database has on record\n",
    "    #the test data will usually be data that hasn't been seen before that can include any number of test users\n",
    "\n",
    "    #When train_users.user_to_ratings_full_transform is used as the input of the svd function, \n",
    "    #svd_out_train is used to produce predictions used to train the model\n",
    "\n",
    "    #When full_user_to_ratings_full_transform is used as the input of the svd function,\n",
    "    #svd_out_full is used to produce predictions used to test the model\n",
    "    \n",
    "\n",
    "    #n = 20 proved to be close to the highest performing constant for the above configuration\n",
    "    svd_out_train = svd_full(train_users.user_to_ratings_full_transform, 20, movie_id_to_average_rating_train)\n",
    "    svd_out_full = svd_full(full_user_to_ratings_full_transform, 20, movie_id_to_average_rating_full)\n",
    "\n",
    "    #here the smaller svd provides predictions used to train the mlp model\n",
    "    for i in range(len(train_users.user_to_ratings_full_transform)):\n",
    "        train_users.feature_3.append(svd_out_train[i][train_users.user_to_target_index_full[i]])\n",
    "\n",
    "    #here the larger svd provides predictions used to test the mlp model\n",
    "    for i in range(len(full_user_to_ratings_full_transform) - len(train_users.user_to_ratings_full_transform)):\n",
    "        test_users.feature_3.append(svd_out_full[i+len(train_users.user_to_ratings_full_transform)][full_user_to_target_index_full[i+len(train_users.user_to_ratings_full_transform)]])\n",
    "\n",
    "load_feature_3()\n",
    "\n",
    "print(train_users.feature_3[0:5])\n",
    "print(train_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(test_users.feature_3[0:5])\n",
    "print(test_users.user_to_target_rating[0:5])\n",
    "\n",
    "\n",
    "#used to see what the text data looks like...\n",
    "# not applicable with dict to list change...\n",
    "# file = open(\"test_dicts_1.txt\", 'w', encoding=\"utf-8\")\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_train))\n",
    "# file.close()\n",
    "\n",
    "# file = open(\"test_dicts_2.txt\", 'w', encoding=\"utf-8\")\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_test))\n",
    "# file.close()\n",
    "\n",
    "\n",
    "#this meight not be worth the deletion!!!\n",
    "# del user_to_data_train\n",
    "# del user_to_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07655955 0.54884398]\n",
      "[0.07251724 0.53422953]\n",
      "[0.07398554 0.53950872]\n",
      "[0.07893114 0.53707882]\n",
      "[0.07790385 0.54515307]\n",
      "[0.07621005 0.54280122]\n",
      "[0.07408352 0.54923166]\n",
      "[0.0758433  0.54757472]\n",
      "[0.07909377 0.54351962]\n",
      "[0.07422675 0.55054701]\n",
      "[0.0754062  0.55380725]\n",
      "[0.0735775  0.54133645]\n",
      "[0.07306207 0.54227272]\n",
      "[0.07318415 0.55481386]\n",
      "[0.07866763 0.54122154]\n",
      "[0.07641959 0.55014952]\n",
      "[0.07510819 0.53854389]\n",
      "[0.07709212 0.54089504]\n",
      "[0.07791481 0.55187469]\n",
      "[0.0770309  0.54681115]\n",
      "[0.07527401 0.53857398]\n",
      "[0.07125875 0.54996505]\n",
      "[0.07672478 0.54124431]\n",
      "[0.07449761 0.5365991 ]\n",
      "[0.07700223 0.5585927 ]\n",
      "[0.0731843  0.53812009]\n",
      "[0.07340178 0.54033766]\n",
      "[0.07516149 0.54115591]\n",
      "[0.07465853 0.56061812]\n",
      "[0.07405626 0.55137907]\n",
      "[0.07400895 0.5419615 ]\n",
      "[0.07677643 0.53409133]\n",
      "[0.07398305 0.55226476]\n",
      "[0.07892379 0.55685729]\n",
      "[0.07238151 0.54825332]\n",
      "[0.07507103 0.54598566]\n",
      "[0.07428719 0.5476295 ]\n",
      "[0.07369303 0.55169743]\n",
      "[0.07607859 0.55844905]\n",
      "[0.0772086 0.5402641]\n",
      "[0.07822428 0.54304877]\n",
      "[0.07551385 0.53977194]\n",
      "[0.07305404 0.52771885]\n",
      "[0.07705965 0.53734755]\n",
      "[0.0740848 0.5494893]\n",
      "[0.0730521  0.55338704]\n",
      "[0.08045907 0.55214487]\n",
      "[0.07661559 0.53917979]\n",
      "[0.07044912 0.54721127]\n",
      "[0.07311216 0.5466872 ]\n",
      "[0.07474687 0.54062667]\n",
      "[0.07806905 0.5572739 ]\n",
      "[0.07259565 0.53451983]\n",
      "[0.07563702 0.55483081]\n",
      "[0.07563761 0.55031664]\n",
      "[0.07662178 0.5463821 ]\n",
      "[0.07202406 0.53513469]\n",
      "[0.07400453 0.54504599]\n",
      "[0.07919974 0.54791187]\n",
      "[0.07677905 0.54089787]\n",
      "[0.07125206 0.53132603]\n",
      "[0.07347896 0.54100651]\n",
      "[0.07901239 0.54139018]\n",
      "[0.0738979  0.54457802]\n",
      "[0.0797384  0.55291781]\n",
      "[0.07837002 0.55017778]\n",
      "[0.07412571 0.54737291]\n",
      "[0.07464886 0.55650282]\n",
      "[0.07806069 0.54478607]\n",
      "[0.07693732 0.5427919 ]\n",
      "[0.07854382 0.54261958]\n",
      "[0.07568792 0.54679238]\n",
      "[0.07562559 0.54990028]\n",
      "[0.07785919 0.55125317]\n",
      "[0.07372391 0.55175167]\n",
      "[0.07841402 0.54835309]\n",
      "[0.07477702 0.546495  ]\n",
      "[0.07588283 0.53968873]\n",
      "[0.07449332 0.54454733]\n",
      "[0.07348797 0.53869026]\n",
      "[0.07671418 0.55318521]\n",
      "[0.07454098 0.54192265]\n",
      "[0.07387343 0.5380318 ]\n",
      "[0.07740367 0.54338081]\n",
      "[0.07557237 0.55439168]\n",
      "[0.0814212  0.55390882]\n",
      "[0.07621395 0.55075399]\n",
      "[0.07442755 0.54626076]\n",
      "[0.07315792 0.54760006]\n",
      "[0.07424171 0.534985  ]\n",
      "[0.07250829 0.53819614]\n",
      "[0.0760646  0.54567087]\n",
      "[0.07196198 0.55383264]\n",
      "[0.07423601 0.55212625]\n",
      "[0.07488291 0.53460995]\n",
      "[0.07406652 0.5278335 ]\n",
      "[0.07883989 0.53965175]\n",
      "[0.07832415 0.55240479]\n",
      "[0.07699448 0.54155194]\n",
      "[0.0719714 0.5474528]\n",
      "(0.3692252282147528, 0.35771948109887597)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "#average the performance results for a number of models with identical inputs\n",
    "def test_parameters(nof_runs, layers, train_input_features, test_input_features):\n",
    "    train_inputs = [list(pair) for pair in train_input_features]\n",
    "    test_inputs = [list(pair) for pair in test_input_features]\n",
    "    return average_results(nof_runs, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "def average_results(nof_runs, layers, train_inputs, text_inputs):\n",
    "    no_rounding = 0\n",
    "    rounding = 0\n",
    "    for _ in range(nof_runs):\n",
    "        #best performance analysis is analysis_1\n",
    "        pair = analysis_1(layers, train_inputs, text_inputs)\n",
    "        no_rounding+=pair[0]\n",
    "        rounding+=pair[1]\n",
    "    return float(no_rounding/nof_runs), float(rounding/nof_runs)\n",
    "\n",
    "\n",
    "#no scaling (best performance):\n",
    "def analysis_1(layers, train_inputs, test_inputs):\n",
    "    # build and train model\n",
    "    # nn model (worse performance)\n",
    "    # reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    # linear regression (better performance)\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(train_inputs, train_users.user_to_target_rating)\n",
    "\n",
    "    #show importance of different inputs features to the model\n",
    "    results = permutation_importance(reg, train_inputs, train_users.user_to_target_rating)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "    #make predictions\n",
    "    predictions = reg.predict(test_inputs)\n",
    "\n",
    "    #test with and without roundings...\n",
    "    #in a sense this is logical sense becasue the actual ratings a user makes must be divisable by .5 \n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#scale inputs and targets:\n",
    "def analysis_2(layers, train_inputs, test_inputs):\n",
    "    #scale input features\n",
    "    train_inputs_scaled = StandardScaler().fit_transform(train_inputs)\n",
    "\n",
    "    #scale target values\n",
    "    target_scalar = StandardScaler()\n",
    "    true_rating_train_scaled = target_scalar.fit_transform(np.reshape(train_users.user_to_target_rating, (-1, 1)))\n",
    "    true_rating_train_scaled = np.reshape(true_rating_train_scaled, len(true_rating_train_scaled))\n",
    "\n",
    "    #build and train model\n",
    "    reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    reg.fit(train_inputs_scaled, true_rating_train_scaled)\n",
    "\n",
    "    #show importance of different inputs features...\n",
    "    results = permutation_importance(reg, train_inputs_scaled,true_rating_train_scaled)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "    #scale inputs features\n",
    "    test_inputs_scaled = StandardScaler().fit_transform(test_inputs)\n",
    "\n",
    "    #predict the scaled verison of ouptuts\n",
    "    scaled_predictions = reg.predict(test_inputs_scaled)\n",
    "\n",
    "    #get actual predictions from scaled predictions...\n",
    "    predictions = target_scalar.inverse_transform(scaled_predictions.reshape(-1, 1))\n",
    "    predictions = list(predictions.reshape(len(predictions)))\n",
    "\n",
    "    #test with and without roundings...\n",
    "    #in a sense this is logical sense becasue the actual ratings a user makes must be divisable by .5 \n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#only scale inputs:\n",
    "def analysis_3(layers, train_inputs, test_inputs):\n",
    "    #scale input features\n",
    "    train_inputs_scaled = StandardScaler().fit_transform(train_inputs)\n",
    "\n",
    "    #build and train model\n",
    "    reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    reg.fit(train_inputs_scaled, train_users.user_to_target_rating)\n",
    "\n",
    "    #show importance of different inputs features...\n",
    "    results = permutation_importance(reg, train_inputs_scaled, train_users.user_to_target_rating)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "    #scale inputs features\n",
    "    test_inputs_scaled = StandardScaler().fit_transform(test_inputs)\n",
    "\n",
    "    #predict the scaled verison of ouptuts\n",
    "    predictions = reg.predict(test_inputs_scaled)\n",
    "\n",
    "    #test with and without roundings...\n",
    "    #in a sense this is logical sense becasue the actual ratings a user makes must be divisable by .5 \n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "\n",
    "#the current test is the average of the r2 scores for 100 different models trained on the same input\n",
    "#the hidden layers are (10,10,10) and the best combinatio of inputs features(feature_2 and feature_3) are used\n",
    "print(test_parameters(100, (10,10,10), \n",
    "    zip(train_users.feature_1, train_users.feature_3),\n",
    "      zip(test_users.feature_1, test_users.feature_3)))\n",
    "\n",
    "\n",
    "#this shows the side by side comparision between all the features and the actual rating\n",
    "#each feature provides a reasonable guess of the target rating\n",
    "#the combination of the feature used above (feature_2 and feature_3) proves stronger than any feature alone and any other combination of features\n",
    "# print(test_users.feature_1)\n",
    "# print(test_users.feature_2)\n",
    "# print(test_users.feature_3)\n",
    "# print(test_users.user_to_target_rating)\n",
    "\n",
    "\n",
    "\n",
    "#with linear regression:\n",
    "#with cossine similarity:\n",
    "\n",
    "#feature_2 and feature_3:\n",
    "#(0.3749823647027071, 0.348993902575555)\n",
    "#(0.3749823647027071, 0.348993902575555)\n",
    "\n",
    "#feature_1 and feature_3: \n",
    "#(0.37665923268552526, 0.35436777366278627)\n",
    "#(0.37665923268552526, 0.35436777366278627)\n",
    "\n",
    "\n",
    "#with linear regression:\n",
    "#with linear_kernel:\n",
    "\n",
    "#feature_2 and feature_3:\n",
    "#(0.3749823647027071, 0.348993902575555)...\n",
    "\n",
    "#feature_1 and feature_3: \n",
    "#(0.37665923268552526, 0.35436777366278627)...\n",
    "#(0.3692252282147528, 0.35771948109887597)\n",
    "\n",
    "\n",
    "\n",
    "#with nn model:\n",
    "#feature_2 and feature_3:\n",
    "#(0.368986238493678, 0.34709385529828507)\n",
    "#feature_1 and feature_3: \n",
    "#(0.3692192733203262, 0.34905915672447185)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
