{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        userId    id rating               title  \\\n",
      "8830418      1  2959    4.0      License to Wed   \n",
      "1690638      1  1968    4.0       Fools Rush In   \n",
      "2343877      1  2762    4.5  Young and Innocent   \n",
      "2943547      1   147    4.5       The 400 Blows   \n",
      "8434697      1  1246    5.0        Rocky Balboa   \n",
      "\n",
      "                                                                                                genres  \\\n",
      "8830418                                                                 [{'id': 35, 'name': 'Comedy'}]   \n",
      "1690638  [{'id': 18, 'name': 'Drama'}, {'id': 35, 'name': 'Comedy'}, {'id': 10749, 'name': 'Romance'}]   \n",
      "2343877                                     [{'id': 18, 'name': 'Drama'}, {'id': 80, 'name': 'Crime'}]   \n",
      "2943547                                                                  [{'id': 18, 'name': 'Drama'}]   \n",
      "8434697                                                                  [{'id': 18, 'name': 'Drama'}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                production_companies  \\\n",
      "8830418  [{'name': 'Village Roadshow Pictures', 'id': 79}, {'name': 'Robert Simonds Productions', 'id': 3929}, {'name': 'Warner Bros.', 'id': 6194}, {'name': 'Phoenix Pictures', 'id': 11317}, {'name': 'Underground', 'id': 49326}, {'name': 'Proposal Productions', 'id': 49327}]   \n",
      "1690638                                                                                                                                                                                                                                     [{'name': 'Columbia Pictures', 'id': 5}]   \n",
      "2343877                                                                                                                                                                                                                [{'name': 'Gaumont British Picture Corporation', 'id': 4978}]   \n",
      "2943547                                                                                                                                 [{'name': 'Les Films du Carrosse', 'id': 53}, {'name': 'Sédif Productions', 'id': 10897}, {'name': 'The Criterion Collection', 'id': 10932}]   \n",
      "8434697                                                                                                  [{'name': 'Columbia Pictures', 'id': 5}, {'name': 'Revolution Studios', 'id': 497}, {'name': 'Rogue Marble', 'id': 696}, {'name': 'Metro-Goldwyn-Mayer (MGM)', 'id': 8411}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       keywords  \\\n",
      "8830418                                                                                                                                                                                                                                                                                                                                             [{'id': 1605, 'name': 'new love'}, {'id': 2856, 'name': 'ten commandments'}, {'id': 3582, 'name': 'bride'}, {'id': 3583, 'name': 'bridegroom'}, {'id': 6038, 'name': 'marriage'}, {'id': 6192, 'name': 'relation'}, {'id': 6281, 'name': 'partnership'}, {'id': 6704, 'name': 'civil registry office'}, {'id': 10093, 'name': 'priest'}, {'id': 13027, 'name': 'wedding'}, {'id': 14765, 'name': 'church'}]   \n",
      "1690638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [{'id': 828, 'name': 'waitress'}, {'id': 1463, 'name': 'culture clash'}, {'id': 9799, 'name': 'romantic comedy'}, {'id': 13149, 'name': 'pregnancy'}]   \n",
      "2343877                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [{'id': 769, 'name': 'falsely accused'}, {'id': 1655, 'name': 'country house'}, {'id': 9826, 'name': 'murder'}, {'id': 9937, 'name': 'suspense'}]   \n",
      "2943547                                                                                                                                                                                                                                                                                                                                                                      [{'id': 6930, 'name': 'fondling'}, {'id': 10183, 'name': 'independent film'}, {'id': 155518, 'name': 'nouvelle vague'}, {'id': 170268, 'name': 'skipping school'}, {'id': 170272, 'name': 'mise en scene'}, {'id': 170273, 'name': 'fingerprinting'}, {'id': 170279, 'name': '\\xa0mugshot'}, {'id': 170286, 'name': 'strict teacher'}, {'id': 170293, 'name': 'montmartre paris'}]   \n",
      "8434697  [{'id': 276, 'name': 'philadelphia'}, {'id': 396, 'name': 'transporter'}, {'id': 1721, 'name': 'fight'}, {'id': 2038, 'name': \"love of one's life\"}, {'id': 2416, 'name': 'publicity'}, {'id': 2792, 'name': 'boxer'}, {'id': 2968, 'name': 'grave'}, {'id': 3393, 'name': 'tombstone'}, {'id': 3586, 'name': 'tv station'}, {'id': 4487, 'name': 'boxing match'}, {'id': 4610, 'name': 'comeback'}, {'id': 4613, 'name': 'training'}, {'id': 5167, 'name': 'restaurant owner'}, {'id': 5378, 'name': 'world champion'}, {'id': 5379, 'name': 'challenger'}, {'id': 5380, 'name': 'virtual fight'}, {'id': 6066, 'name': 'defeat'}, {'id': 6067, 'name': 'victory'}, {'id': 10163, 'name': 'cancer'}, {'id': 155464, 'name': 'over-the-hill fighter'}]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        cast  \\\n",
      "8830418                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [{'cast_id': 18, 'character': 'Reverend Frank', 'credit_id': '52fe4376c3a36847f8056039', 'gender': 2, 'id': 2157, 'name': 'Robin Williams', 'order': 0, 'profile_path': '/sojtJyIV3lkUeThD7A2oHNm8183.jpg'}, {'cast_id': 19, 'character': 'Sadie Jones', 'credit_id': '52fe4376c3a36847f805603d', 'gender': 1, 'id': 16855, 'name': 'Mandy Moore', 'order': 1, 'profile_path': '/15sDtRpe301tZWrRYV31wjMuFpx.jpg'}, {'cast_id': 20, 'character': 'Ben Murphy', 'credit_id': '52fe4376c3a36847f8056041', 'gender': 2, 'id': 17697, 'name': 'John Krasinski', 'order': 2, 'profile_path': '/nOWwdZURikW22qo6OUSGFCTukgc.jpg'}, {'cast_id': 21, 'character': 'Carlisle', 'credit_id': '52fe4376c3a36847f8056045', 'gender': 2, 'id': 29020, 'name': 'Eric Christian Olsen', 'order': 3, 'profile_path': '/clbouet8o9IJlUd8WILD0lzHAtG.jpg'}, {'cast_id': 22, 'character': 'Lindsey Jones', 'credit_id': '52fe4376c3a36847f8056049', 'gender': 1, 'id': 15286, 'name': 'Christine Taylor', 'order': 4, 'profile_path': '/99OssnGmgGjduXFA7syxjNqt9tQ.jpg'}, {'cast_id': 23, 'character': 'Choir Boy', 'credit_id': '52fe4376c3a36847f805604d', 'gender': 2, 'id': 216, 'name': 'Josh Flitter', 'order': 5, 'profile_path': '/6RCA8tDWBxIVk9N3IqUjJEAzYGv.jpg'}, {'cast_id': 24, 'character': 'Joel', 'credit_id': '52fe4376c3a36847f8056051', 'gender': 2, 'id': 11827, 'name': 'DeRay Davis', 'order': 6, 'profile_path': '/w2JYPRLwXhNCpxpJc2v4UQYyMv8.jpg'}, {'cast_id': 25, 'character': 'Mr. Jones', 'credit_id': '52fe4376c3a36847f8056055', 'gender': 2, 'id': 21368, 'name': 'Peter Strauss', 'order': 7, 'profile_path': '/ufx1trct43k7UcT4DpoIMPZXi5A.jpg'}, {'cast_id': 26, 'character': 'Grandma Jones', 'credit_id': '52fe4376c3a36847f8056059', 'gender': 1, 'id': 6465, 'name': 'Grace Zabriskie', 'order': 8, 'profile_path': '/ibBabuSM1UyPYFFo0wBXhGbqElk.jpg'}, {'cast_id': 27, 'character': 'Mrs. Jones', 'credit_id': '52fe4376c3a36847f805605d', 'gender': 1, 'id': 29021, 'name': 'Roxanne Hart', 'order': 9, 'profile_path': '/yWGMW6HdhUGT2oIcQ4jmnkw7ZAM.jpg'}, {'cast_id': 28, 'character': 'Shelly', 'credit_id': '5586ee469251417f6f0059c8', 'gender': 1, 'id': 125167, 'name': 'Mindy Kaling', 'order': 10, 'profile_path': '/Agpd4tJyZ95hk74RifjnfnJpn9U.jpg'}, {'cast_id': 30, 'character': 'Expectant Father', 'credit_id': '56c3467cc3a36847c5001f66', 'gender': 2, 'id': 1368801, 'name': 'David Quinlan', 'order': 11, 'profile_path': '/2m75rrBhvOTtdUS9jlKW8GOHCBV.jpg'}, {'cast_id': 31, 'character': 'Judith', 'credit_id': '58e26093c3a36872f600dcf2', 'gender': 1, 'id': 113867, 'name': 'Angela Kinsey', 'order': 12, 'profile_path': '/omLdRLdwMLliVeVIualEnWVhm1a.jpg'}]   \n",
      "1690638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [{'cast_id': 2, 'character': 'Alex Whitman', 'credit_id': '52fe4327c3a36847f803e629', 'gender': 2, 'id': 14408, 'name': 'Matthew Perry', 'order': 0, 'profile_path': '/oSKEEDXDNnwWdQ68qfDVD6Q7Pxp.jpg'}, {'cast_id': 3, 'character': 'Isabel Fuentes', 'credit_id': '52fe4327c3a36847f803e62d', 'gender': 1, 'id': 3136, 'name': 'Salma Hayek', 'order': 1, 'profile_path': '/u5mg73xKVqm8oT93HoMmsgQHyoK.jpg'}, {'cast_id': 4, 'character': 'Jeff', 'credit_id': '52fe4327c3a36847f803e631', 'gender': 2, 'id': 4602, 'name': 'Jon Tenney', 'order': 2, 'profile_path': '/fiG1bW6DX1szsRDPIYjfIKPQ0kV.jpg'}, {'cast_id': 5, 'character': 'Lanie', 'credit_id': '52fe4327c3a36847f803e635', 'gender': 1, 'id': 6751, 'name': 'Siobhan Fallon', 'order': 3, 'profile_path': '/wVFa8GiY0xdOLFsvGygy9RMtcBc.jpg'}, {'cast_id': 16, 'character': 'Great Grandma', 'credit_id': '52fe4327c3a36847f803e675', 'gender': 1, 'id': 20360, 'name': 'Angelina Torres', 'order': 4, 'profile_path': None}, {'cast_id': 17, 'character': 'Richard Whitman', 'credit_id': '52fe4327c3a36847f803e679', 'gender': 2, 'id': 20361, 'name': 'John Bennett Perry', 'order': 5, 'profile_path': '/bzFhwuXsdZiOHRtBgz4XVELIFYO.jpg'}, {'cast_id': 18, 'character': 'Nan Whitman', 'credit_id': '52fe4327c3a36847f803e67d', 'gender': 1, 'id': 20362, 'name': 'Jill Clayburgh', 'order': 6, 'profile_path': '/twrfhIvbqHuJ7nXVpehvU6nyi6R.jpg'}, {'cast_id': 19, 'character': 'Cathy Stewart', 'credit_id': '52fe4327c3a36847f803e681', 'gender': 1, 'id': 20363, 'name': 'Suzanne Snyder', 'order': 7, 'profile_path': '/90FrTcjJudpeIYUjUzlO6XAmvnt.jpg'}, {'cast_id': 20, 'character': 'Amalia', 'credit_id': '52fe4327c3a36847f803e685', 'gender': 0, 'id': 13029, 'name': 'Anne Betancourt', 'order': 8, 'profile_path': '/6UU5P4DzjJTSBFztIu1nALT2tk0.jpg'}, {'cast_id': 21, 'character': 'Juan Fuentes', 'credit_id': '52fe4327c3a36847f803e689', 'gender': 2, 'id': 4511, 'name': 'Mark Adair-Rios', 'order': 9, 'profile_path': '/rX4d1e5jlF5P73qynjjUzJslB0c.jpg'}, {'cast_id': 22, 'character': 'Judd Marshall', 'credit_id': '52fe4327c3a36847f803e68d', 'gender': 2, 'id': 4171, 'name': 'Stanley DeSantis', 'order': 10, 'profile_path': '/4cHxkhTd7oklyNkdva9WJp0FLrX.jpg'}, {'cast_id': 23, 'character': 'Antonio Fuentes', 'credit_id': '52fe4327c3a36847f803e691', 'gender': 0, 'id': 4665, 'name': 'Josh Cruze', 'order': 11, 'profile_path': '/v3QrQzH0uGV9pd1dNR5Ue6a74qO.jpg'}, {'cast_id': 24, 'character': 'Petra', 'credit_id': '52fe4327c3a36847f803e695', 'gender': 0, 'id': 4666, 'name': 'Angela Lanza', 'order': 12, 'profile_path': '/zmf6TMWMVCdnuUfpgdnioaICk1L.jpg'}, {'cast_id': 25, 'character': 'Phil', 'credit_id': '52fe4327c3a36847f803e699', 'gender': 2, 'id': 4445, 'name': 'Chris Bauer', 'order': 13, 'profile_path': '/3KYVMaGkWTEDQ0T9lsu85pVbP4T.jpg'}, {'cast_id': 26, 'character': 'Chuy', 'credit_id': '577e438f925141440c000d63', 'gender': 0, 'id': 115874, 'name': 'Carlos Gómez', 'order': 14, 'profile_path': '/nBxwoMv1zrhNXyEjYXbcdmAdmF0.jpg'}]   \n",
      "2343877                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [{'cast_id': 18, 'character': 'Erica Burgoyne', 'credit_id': '52fe436bc3a36847f8052cd5', 'gender': 1, 'id': 27939, 'name': 'Nova Pilbeam', 'order': 0, 'profile_path': '/l6oHJaRYrVxsvoTSmMS5wIXaei5.jpg'}, {'cast_id': 19, 'character': 'Robert Tisdall', 'credit_id': '52fe436bc3a36847f8052cd9', 'gender': 0, 'id': 27940, 'name': 'Derrick De Marney', 'order': 1, 'profile_path': '/7VRZ7K0EZ50haOlbVr7DHZ5550O.jpg'}, {'cast_id': 20, 'character': 'Col. Burgoyne', 'credit_id': '52fe436bc3a36847f8052cdd', 'gender': 2, 'id': 27929, 'name': 'Percy Marmont', 'order': 2, 'profile_path': '/p3DIyvlxx6B0SVIxcDaPUPlEV0U.jpg'}, {'cast_id': 21, 'character': 'Old Will', 'credit_id': '52fe436bc3a36847f8052ce1', 'gender': 2, 'id': 27941, 'name': 'Edward Rigby', 'order': 3, 'profile_path': '/B7GJ0jPtODqZVgVtZHPtvZl2tO.jpg'}, {'cast_id': 22, 'character': 'Ericas Tante Margaret', 'credit_id': '52fe436bc3a36847f8052ce5', 'gender': 1, 'id': 14304, 'name': 'Mary Clare', 'order': 4, 'profile_path': '/lAdEwCGiSUj9CCMPB4L9X4oujLe.jpg'}, {'cast_id': 23, 'character': 'Det. Insp. Kent', 'credit_id': '52fe436bc3a36847f8052ce9', 'gender': 2, 'id': 7383, 'name': 'John Longden', 'order': 5, 'profile_path': '/rsCoUEx2ThNIz12fBR6vPncCICk.jpg'}, {'cast_id': 24, 'character': 'Guy', 'credit_id': '52fe436bc3a36847f8052ced', 'gender': 2, 'id': 27942, 'name': 'George Curzon', 'order': 6, 'profile_path': None}, {'cast_id': 25, 'character': 'Ericas Onkel Basil', 'credit_id': '52fe436bc3a36847f8052cf1', 'gender': 2, 'id': 14303, 'name': 'Basil Radford', 'order': 7, 'profile_path': '/9STo7Tgdutplo78ZtyeINGWkXUk.jpg'}, {'cast_id': 26, 'character': 'Christine Clay', 'credit_id': '52fe436bc3a36847f8052cf5', 'gender': 1, 'id': 27943, 'name': 'Pamela Carme', 'order': 8, 'profile_path': None}, {'cast_id': 27, 'character': 'Detective Sergeant Miller', 'credit_id': '52fe436bc3a36847f8052cf9', 'gender': 2, 'id': 27944, 'name': 'George Merritt', 'order': 9, 'profile_path': None}, {'cast_id': 28, 'character': 'Henry Briggs', 'credit_id': '52fe436bc3a36847f8052cfd', 'gender': 2, 'id': 27945, 'name': 'J.H. Roberts', 'order': 10, 'profile_path': None}, {'cast_id': 29, 'character': \"Truckfahrer bei Tom's Hat\", 'credit_id': '52fe436bc3a36847f8052d01', 'gender': 2, 'id': 27946, 'name': 'Jerry Verno', 'order': 11, 'profile_path': None}, {'cast_id': 30, 'character': 'Police Sergeant Ruddock', 'credit_id': '52fe436bc3a36847f8052d05', 'gender': 2, 'id': 27947, 'name': 'H.F. Maltby', 'order': 12, 'profile_path': None}, {'cast_id': 31, 'character': 'Police Constable', 'credit_id': '52fe436bc3a36847f8052d09', 'gender': 2, 'id': 27948, 'name': 'John Miller', 'order': 13, 'profile_path': None}]   \n",
      "2943547  [{'cast_id': 6, 'character': 'Antoine Doinel', 'credit_id': '52fe421ec3a36847f8005661', 'gender': 2, 'id': 1653, 'name': 'Jean-Pierre Léaud', 'order': 0, 'profile_path': '/dzkPODapVe4CSubEqI9ytTCqnZ7.jpg'}, {'cast_id': 7, 'character': 'Gilberte Doinel', 'credit_id': '52fe421ec3a36847f8005665', 'gender': 1, 'id': 1654, 'name': 'Claire Maurier', 'order': 1, 'profile_path': '/cP1n7zMsMKr77xJeR3CncomxEZ0.jpg'}, {'cast_id': 8, 'character': 'Julien Doinel', 'credit_id': '52fe421ec3a36847f8005669', 'gender': 0, 'id': 1655, 'name': 'Albert Rémy', 'order': 2, 'profile_path': '/6b8eyIXAV6oA5eX6ltc3hF7ZB3d.jpg'}, {'cast_id': 10, 'character': 'Mr. Bigey', 'credit_id': '52fe421ec3a36847f8005673', 'gender': 2, 'id': 1658, 'name': 'Georges Flamant', 'order': 3, 'profile_path': '/lQwmtPsFWME63x5M7IRF6g8bLrR.jpg'}, {'cast_id': 11, 'character': 'René', 'credit_id': '52fe421ec3a36847f8005677', 'gender': 0, 'id': 1659, 'name': 'Patrick Auffay', 'order': 4, 'profile_path': None}, {'cast_id': 12, 'character': 'Director of the school', 'credit_id': '52fe421ec3a36847f800567b', 'gender': 0, 'id': 1660, 'name': 'Robert Beauvais', 'order': 5, 'profile_path': None}, {'cast_id': 13, 'character': 'Mme Bigey', 'credit_id': '52fe421ec3a36847f800567f', 'gender': 0, 'id': 1661, 'name': 'Yvonne Claudie', 'order': 6, 'profile_path': None}, {'cast_id': 14, 'character': 'English Teacher', 'credit_id': '52fe421ec3a36847f8005683', 'gender': 0, 'id': 1662, 'name': 'Pierre Repp', 'order': 7, 'profile_path': '/1AUhiNGBAR0C6AU9iK1IXBs3QTz.jpg'}, {'cast_id': 17, 'character': 'French Teacher', 'credit_id': '52fe421ec3a36847f8005693', 'gender': 0, 'id': 1656, 'name': 'Guy Decomble', 'order': 8, 'profile_path': '/34iexAuqI1asyFounbSXSCFphen.jpg'}, {'cast_id': 20, 'character': 'Betrand Mauricet', 'credit_id': '52fe421ec3a36847f8005697', 'gender': 0, 'id': 1077237, 'name': 'Daniel Couturier', 'order': 9, 'profile_path': None}, {'cast_id': 21, 'character': 'Child', 'credit_id': '52fe421ec3a36847f800569b', 'gender': 0, 'id': 1077238, 'name': 'François Nocher', 'order': 10, 'profile_path': None}, {'cast_id': 22, 'character': 'Child', 'credit_id': '52fe421ec3a36847f800569f', 'gender': 2, 'id': 150939, 'name': 'Richard Kanayan', 'order': 11, 'profile_path': '/vCMDk3ifj2vJKZYCISXT3K6DYXF.jpg'}, {'cast_id': 23, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056a3', 'gender': 0, 'id': 1077239, 'name': 'Renaud Fontanarosa', 'order': 12, 'profile_path': None}, {'cast_id': 24, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056a7', 'gender': 0, 'id': 1077240, 'name': 'Michel Girard', 'order': 13, 'profile_path': None}, {'cast_id': 25, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056ab', 'gender': 0, 'id': 71997, 'name': 'Serge Moati', 'order': 14, 'profile_path': '/wccRQKHrX61sH4WlOtM1KBP4qaq.jpg'}, {'cast_id': 26, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056af', 'gender': 0, 'id': 1077241, 'name': 'Bernard Abbou', 'order': 15, 'profile_path': None}, {'cast_id': 27, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056b3', 'gender': 0, 'id': 1077242, 'name': 'Jean-François Bergouignan', 'order': 16, 'profile_path': None}, {'cast_id': 28, 'character': 'Child', 'credit_id': '52fe421ec3a36847f80056b7', 'gender': 0, 'id': 1077243, 'name': 'Michel Lesignor', 'order': 17, 'profile_path': None}, {'cast_id': 31, 'character': 'Man in Street', 'credit_id': '5457f0a1c3a3683993000156', 'gender': 2, 'id': 24299, 'name': 'Jean-Claude Brialy', 'order': 18, 'profile_path': '/g3kkYcAvq90tALMErxmdAIcIXsE.jpg'}, {'cast_id': 32, 'character': 'Woman with Dog', 'credit_id': '5457f0bec3a36839a0000144', 'gender': 1, 'id': 14812, 'name': 'Jeanne Moreau', 'order': 19, 'profile_path': '/uHJnVwCzehEoz0mIlwN7xkymql8.jpg'}, {'cast_id': 33, 'character': 'Man in Funfair', 'credit_id': '5457f0d3c3a368399300015b', 'gender': 2, 'id': 34613, 'name': 'Philippe de Broca', 'order': 20, 'profile_path': '/yrvmXE2SJBX659r2Y7eWwlmwfYd.jpg'}, {'cast_id': 34, 'character': 'Man in Funfair', 'credit_id': '5457f0e5c3a368399d00014c', 'gender': 0, 'id': 1650, 'name': 'François Truffaut', 'order': 21, 'profile_path': '/apCCV99N3FqB5NsEPqOzetlkprL.jpg'}]   \n",
      "8434697                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [{'cast_id': 24, 'character': 'Rocky Balboa', 'credit_id': '52fe42e9c3a36847f802c61b', 'gender': 2, 'id': 16483, 'name': 'Sylvester Stallone', 'order': 0, 'profile_path': '/gnmwOa46C2TP35N7ARSzboTdx2u.jpg'}, {'cast_id': 25, 'character': 'Paulie', 'credit_id': '52fe42e9c3a36847f802c61f', 'gender': 2, 'id': 4521, 'name': 'Burt Young', 'order': 1, 'profile_path': '/rbSsSkQ72FoGcvwIHUxQWJ92I3W.jpg'}, {'cast_id': 26, 'character': 'Rocky Jr.', 'credit_id': '52fe42e9c3a36847f802c623', 'gender': 2, 'id': 16501, 'name': 'Milo Ventimiglia', 'order': 2, 'profile_path': '/maJeS6bA6ku21rSRceISQtwHL2h.jpg'}, {'cast_id': 27, 'character': 'Marie', 'credit_id': '52fe42e9c3a36847f802c627', 'gender': 1, 'id': 16502, 'name': 'Geraldine Hughes', 'order': 3, 'profile_path': '/bTXux3EJq25Fh2ixbet6MjdG3Fb.jpg'}, {'cast_id': 28, 'character': 'Steps', 'credit_id': '52fe42e9c3a36847f802c62b', 'gender': 2, 'id': 16503, 'name': 'James Francis Kelly III', 'order': 4, 'profile_path': '/iZyTQ2UlwNXrqLqPeNHbofFXubP.jpg'}, {'cast_id': 29, 'character': 'Duke', 'credit_id': '52fe42e9c3a36847f802c62f', 'gender': 2, 'id': 16504, 'name': 'Tony Burton', 'order': 5, 'profile_path': '/ue54hK217thXeQMzN4qUYXLImLd.jpg'}, {'cast_id': 30, 'character': 'L.C.', 'credit_id': '52fe42e9c3a36847f802c633', 'gender': 2, 'id': 16505, 'name': 'A. J. Benza', 'order': 6, 'profile_path': '/5hVinC6C1ZyD7c8EmZFTiEaF7vH.jpg'}, {'cast_id': 31, 'character': 'Adrian', 'credit_id': '52fe42e9c3a36847f802c637', 'gender': 1, 'id': 3094, 'name': 'Talia Shire', 'order': 7, 'profile_path': '/liNfrVB3eZFBOjcUGltISCucews.jpg'}, {'cast_id': 32, 'character': 'Martin', 'credit_id': '52fe42e9c3a36847f802c63b', 'gender': 2, 'id': 16506, 'name': 'Henry G. Sanders', 'order': 8, 'profile_path': '/2SU75g2CAIzGWbgfIlNvKZQhYTZ.jpg'}, {'cast_id': 33, 'character': \"Mason 'The Line' Dixon\", 'credit_id': '52fe42e9c3a36847f802c63f', 'gender': 2, 'id': 16507, 'name': 'Antonio Tarver', 'order': 9, 'profile_path': '/kJEljjHwBvrjoxqcSVntXlejgl1.jpg'}, {'cast_id': 34, 'character': 'Spider Rico', 'credit_id': '52fe42e9c3a36847f802c643', 'gender': 2, 'id': 16508, 'name': 'Pedro Lovell', 'order': 10, 'profile_path': None}, {'cast_id': 35, 'character': 'Isabel', 'credit_id': '52fe42e9c3a36847f802c647', 'gender': 1, 'id': 16509, 'name': 'Ana Gerena', 'order': 11, 'profile_path': None}, {'cast_id': 36, 'character': 'Angie', 'credit_id': '52fe42e9c3a36847f802c64b', 'gender': 1, 'id': 16510, 'name': 'Angela Boyd', 'order': 12, 'profile_path': None}, {'cast_id': 37, 'character': 'Bar Thug', 'credit_id': '52fe42e9c3a36847f802c64f', 'gender': 0, 'id': 16511, 'name': 'Louis Giansante', 'order': 13, 'profile_path': None}, {'cast_id': 38, 'character': \"Lucky's Bartender\", 'credit_id': '52fe42e9c3a36847f802c653', 'gender': 0, 'id': 16512, 'name': 'Maureen Schilling', 'order': 14, 'profile_path': None}, {'cast_id': 40, 'character': 'X-Cell', 'credit_id': '5761db05c3a3682f20000302', 'gender': 2, 'id': 98298, 'name': 'Lahmard J. Tate', 'order': 15, 'profile_path': '/4WcFReePSxyGQJWV5wXGNfY0Y7o.jpg'}]   \n",
      "\n",
      "                                                                               tagline  \\\n",
      "8830418                                   First came love... then came Reverend Frank.   \n",
      "1690638  What if finding the love of your life meant changing the life that you loved?   \n",
      "2343877                                                          A Brilliant Melodrama   \n",
      "2943547                                            Angel faces hell-bent for violence.   \n",
      "8434697                                                  It ain't over 'til it's over.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        overview  \n",
      "8830418                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Newly engaged, Ben and Sadie can't wait to start their life together and live happily ever after. However Sadie's family church's Reverend Frank won't bless their union until they pass his patented, \"foolproof\" marriage prep course consisting of outrageous classes, outlandish homework assignments and some outright invasion of privacy.  \n",
      "1690638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Alex Whitman (Matthew Perry) is a designer from New York City who is sent to Las Vegas to supervise the construction of a nightclub that his firm has been hired to build. Alex is a straight-laced WASP-ish type who, while enjoying a night on the town, meets Isabel Fuentes (Salma Hayek), a free-spirited Mexican-American photographer. Alex and Isabel are overtaken by lust at first sight and end up sp  \n",
      "2343877  Derrick De Marney finds himself in a 39 Steps situation when he is wrongly accused of murder. While a fugitive from the law, De Marney is helped by heroine Nova Pilbeam, who three years earlier had played the adolescent kidnap victim in Hitchcock's The Man Who Knew Too Much. The obligatory \"fish out of water\" scene, in which the principals are briefly slowed down by a banal everyday event, occurs during a child's birthday party. The actual villain, whose identity is never in doubt (Hitchcock made thrillers, not mysteries) is played by George Curzon, who suffers from a twitching eye. Curzon's revelation during an elaborate nightclub sequence is a Hitchcockian tour de force, the sort of virtuoso sequence taken for granted in these days of flexible cameras and computer enhancement, but which in 1937 took a great deal of time, patience and talent to pull off. Released in the US as The Girl Was Young, Young and Innocent was based on a novel by Josephine Tey.  \n",
      "2943547                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     For young Parisian boy Antoine Doinel, life is one difficult situation after another. Surrounded by inconsiderate adults, including his neglectful parents, Antoine spends his days with his best friend, Rene, trying to plan for a better life. When one of their schemes goes awry, Antoine ends up in trouble with the law, leading to even more conflicts with unsympathetic authority figures.  \n",
      "8434697                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              When he loses a highly publicized virtual boxing match to ex-champ Rocky Balboa, reigning heavyweight titleholder, Mason Dixon retaliates by challenging Rocky to a nationally televised, 10-round exhibition bout. To the surprise of his son and friends, Rocky agrees to come out of retirement and face an opponent who's faster, stronger and thirty years his junior.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 3\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "\n",
    "#Data source:\n",
    "#https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "#new:\n",
    "#userIds are in order\n",
    "# every_id = list(unique(list(complete[\"userId\"])))\n",
    "#userIds are out of order\n",
    "# sample_ids  = random.sample(every_id, 1000)\n",
    "# completeNew = pd.DataFrame()\n",
    "#This is a very expensive task...\n",
    "#it is possile to choose a subset of users from here instead of\n",
    "#ibcluing  the entire set of users\n",
    "# for user in sample_ids:\n",
    "#     completeNew = pd.concat([completeNew, complete.loc[complete[\"userId\"] == user]])\n",
    "\n",
    "#new:\n",
    "# complete = complete.sample(frac=1, random_state = seed_int, axis =0)\n",
    "#this is not the desired behavior\n",
    "#the users ids need to show up in a true random order\n",
    "# complete = complete.groupby(by = \"userId\", sort = False, group_keys = True).apply(lambda x: x)\n",
    "#this is omitted since the values should not be sorted by userId just grouped by userId\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "\n",
    "print(complete.head())\n",
    "\n",
    "# f = open(\"test_dicts.txt\", \"w\", encoding=\"utf-8\")\n",
    "# f.write(str(list(complete[\"tagline\"])))\n",
    "# f.write(str(list(complete[\"overview\"])))\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261306\n",
      "4204\n",
      "176.65865842055186\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "# seed_int = 3\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    #this is probably not needed due to the dropNa function used above...\n",
    "    # length = len(array[8])\n",
    "    # if(array[8][length-4:]==\"<NA>\"):\n",
    "    #     return False\n",
    "    # length = len(array[9])\n",
    "    # if(array[9][length-4:]==\"<NA>\"):\n",
    "    #     return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#new method\n",
    "list_of_user_ids = list(complete[\"userId\"].unique())\n",
    "# print(list_of_user_ids)\n",
    "\n",
    "counts = complete['userId'].value_counts()\n",
    "# print(counts)\n",
    "\n",
    "gaps = [counts[id] for id in list_of_user_ids]\n",
    "# print(gaps)\n",
    "\n",
    "# gaps = []\n",
    "# for id in list_of_user_ids:\n",
    "#     gaps.append(counts[id])\n",
    "\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "# print(complete)\n",
    "# print(complete_array)\n",
    "\n",
    "\n",
    "\n",
    "#old method...\n",
    "# gaps = []\n",
    "# size = 0\n",
    "# list_of_user_ids = []\n",
    "# last_id  = -1\n",
    "# past_first_it = False\n",
    "\n",
    "\n",
    "# for row in complete_array:\n",
    "#     if(row[0]!= last_id):\n",
    "#         list_of_user_ids.append(row[0])\n",
    "#         last_id = row[0]\n",
    "#         if(past_first_it ==True):\n",
    "#             gaps.append(size)\n",
    "#             size =0 \n",
    "#     size+=1\n",
    "#     past_first_it = True\n",
    "\n",
    "# #there is always a gap for the last iteration\n",
    "# gaps.append(size)\n",
    "\n",
    "\n",
    "\n",
    "index  = 0\n",
    "\n",
    "user_to_data = []\n",
    "#this is the total number of users in the whole dataset\n",
    "#total number of users: 261306\n",
    "total_nof_users = len(list_of_user_ids)\n",
    "#this is the number of desired users before filtering\n",
    "#note: set back to 40000\n",
    "desired_nof_users_before_filter = 61306\n",
    "\n",
    "print(total_nof_users)\n",
    "\n",
    "avg = 0.0\n",
    "cnt = 0.0\n",
    "\n",
    "\n",
    "#instead of using this random pass generaion \n",
    "#the users can be picked randomly before hand and replace list_of_user_ids[i]\n",
    "#will index still work???\n",
    "\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, total_nof_users):\n",
    "    #generate a random float to determine a pass for the user\n",
    "    if (random.random()<float(desired_nof_users_before_filter/total_nof_users)):\n",
    "        # if(gaps[i] >= 50 and gaps[i]<=75):\n",
    "        if(gaps[i] >= 100):\n",
    "            user_to_data.append([])\n",
    "            last_index = len(user_to_data) -1\n",
    "            for j in range(index, len(complete_array)):\n",
    "                if complete_array[j][0] == list_of_user_ids[i]:\n",
    "                    #condition is checked for complete_array[j]\n",
    "                    if(condition(complete_array[j])):\n",
    "                        #this is where data is tranformed\n",
    "                        transformed = provide_data(complete_array[j])\n",
    "                        #is copy needed here???\n",
    "                        user_to_data[last_index].append(transformed)\n",
    "\n",
    "                    #last iteration\n",
    "                    # if (j==len(complete_array)-1):\n",
    "                        #note: what if a similair condition could be checked before running this loop to prevent redundant processing\n",
    "                        # if (len(user_to_data[last_index])<50 or len(user_to_data[last_index])>75):\n",
    "                        # if (len(user_to_data[last_index])<100):\n",
    "                        #     del user_to_data[last_index]       \n",
    "                else:\n",
    "                    avg += len(user_to_data[last_index])\n",
    "                    cnt+=1\n",
    "                    #note: what if a similair condition could be checked before running this loop to prevent redundant processing\n",
    "                    #if (len(user_to_data[last_index])<50 or len(user_to_data[last_index])>75):\n",
    "                    # if (len(user_to_data[last_index])<100):\n",
    "                    #     del user_to_data[last_index]  \n",
    "                    #note: changed from (index = j+1)\n",
    "                    index = j\n",
    "                    break\n",
    "        else:\n",
    "            index += gaps[i]             \n",
    "    else:\n",
    "        index += gaps[i]\n",
    "\n",
    "\n",
    "#last iteration...\n",
    "\n",
    "\n",
    "\n",
    "#runtime test, go through user_to_data and re-index the users in list order\n",
    "for i in range(len(user_to_data)):\n",
    "    for j in range(len(user_to_data[i])):\n",
    "        user_to_data[i][j][0] = i\n",
    "\n",
    "\n",
    "\n",
    "#needs to be sure that there are enough users after the condiiton\n",
    "print(len(user_to_data))\n",
    "\n",
    "#average number of ratings per users\n",
    "print(float(avg/cnt))\n",
    "\n",
    "\n",
    "#gaps[i] >= 100\n",
    "#desired_nof_users_before_filter = 61306\n",
    "#time taken: 11 minutes\n",
    "#average rating: 176.65865842055186\n",
    "#number of users: 4204\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "\n",
    "#question: would renaming the user ids as indexes in their order be helpful???\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for i in range(len(user_to_data)):\n",
    "        writer.writerows(user_to_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "#used with every random function except for the last cell where a certain number of models are tested and accumulated with identiacal test and train data\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#user to data rows \n",
    "user_to_data = []\n",
    "user_to_data_train = []\n",
    "user_to_data_test = []\n",
    "user_id = -1\n",
    "\n",
    "#note: works when row[0] is also an index\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_data.append([row])\n",
    "    else:\n",
    "        user_to_data[int(row[0])].append(row)\n",
    "\n",
    "\n",
    "#these both can be increased for consistency as long as there is enough data\n",
    "#with the current configuration there are 4204 users\n",
    "#this can be increased by increasing the desired_nof_users_before_filter parameter above\n",
    "for i in range(3600):\n",
    "    index = random.randint(0, len(user_to_data)-1)\n",
    "    user_to_data_train.append(user_to_data[index])\n",
    "    del user_to_data[index]\n",
    "\n",
    "\n",
    "for i in range(600):\n",
    "    index = random.randint(0, len(user_to_data_train)-1)\n",
    "    user_to_data_test.append(user_to_data_train[index])\n",
    "    del user_to_data_train[index]\n",
    "\n",
    "\n",
    "del user_to_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4107075993636333, 3.539308868817238, 3.061163513511754, 3.310828661631756, 4.403429829174557]\n",
      "[1.5, 4.0, 5.0, 3.0, 4.5]\n",
      "[3.3650977256546217, 1.852360424015049, 3.1602965447591753, 4.007377997918515, 4.019528390228162]\n",
      "[3.0, 1.5, 2.0, 4.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import json\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#the linalg is used from numpy instea of scipy\n",
    "import numpy as np\n",
    "#the version from numpy is used instead\n",
    "from scipy import linalg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.linalg import sqrtm\n",
    "import math\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "class user_type_vars():\n",
    "    def __init__(self):\n",
    "        #for each user movie_id to its rating for each movie the user watched\n",
    "        self.user_to_movie_id_to_rating = [] \n",
    "\n",
    "        #for each user, a random choice of movie_id from all the movies the user watched\n",
    "        self.user_to_target_movie_id = [] \n",
    "\n",
    "        #for each user, the index is the placement of the target movie in the entire set of train movies inorder\n",
    "        self.user_to_target_index_full = [] \n",
    "\n",
    "        #for each user, includes ratings for all the movies in the entire train set (not used for the test set)\n",
    "        #missing ratings and target movie ratings are set to that movies average rating\n",
    "        self.user_to_ratings_full = [] \n",
    "\n",
    "        #for each user, includes ratings for all the movies in the entire train set\n",
    "        #the movie mean is subtracted from\n",
    "        #  (not used for the test set)\n",
    "        #missing ratings and target movie rating are set to zero\n",
    "        self.user_to_ratings_full_transform = []\n",
    "\n",
    "        #for every movie watched by the user_type, a list of ratings\n",
    "        self.movie_id_to_ratings = dict()\n",
    "\n",
    "        #model features x\n",
    "        self.feature_1 = []\n",
    "        self.feature_2 = []\n",
    "        self.feature_3 = []\n",
    "\n",
    "        #output feature y\n",
    "        self.user_to_target_rating  = [] \n",
    "\n",
    "\n",
    "        \n",
    "train_users = user_type_vars()\n",
    "test_users = user_type_vars()\n",
    "\n",
    "\n",
    "train_movies_in_order = OrderedSet()\n",
    "test_movies_in_order = OrderedSet()\n",
    "\n",
    "\n",
    "#this not being used currently...\n",
    "#it is simply the users average rating not including the chosen target movie\n",
    "#this is for all train users in order followed by the test users in order\n",
    "user_to_average_rating = []\n",
    "\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "#train_users.user_to_movie_id_to_rating\n",
    "#train_users.user_to_target_movie_id\n",
    "#test_users.user_to_movie_id_to_rating\n",
    "#test_users.user_to_target_movie_id\n",
    "\n",
    "\n",
    "def load_feature_1(movies_in_order, user_to_data, movie_id_to_ratings, user_to_movie_id_to_rating, user_to_target_movie_id, user_to_target_rating, feature_1, feature_2):\n",
    "    #how do I integrate pop the other populate functions???\n",
    "    overall_rating_sum = 0\n",
    "    overall_rating_count = 0\n",
    "    for i in range(len(user_to_data)):\n",
    "        movie_id_to_words_temp = dict()\n",
    "        movie_id_to_rating_temp = dict()\n",
    "        cnt = 0\n",
    "        total =0\n",
    "        rand_int = random.randint(0, len(user_to_data[i])-1)\n",
    "        for movie_data in user_to_data[i]:\n",
    "            if cnt == rand_int:    \n",
    "                user_to_target_movie_id.append(movie_data[1])\n",
    "            else:\n",
    "                overall_rating_sum += float(movie_data[2])\n",
    "                overall_rating_count += 1\n",
    "                total += float(movie_data[2])\n",
    "            \n",
    "            if movie_data[1] in movie_id_to_ratings.keys():\n",
    "                movie_id_to_ratings[movie_data[1]].append(float(movie_data[2]))\n",
    "            else:\n",
    "                movie_id_to_ratings[movie_data[1]] = [float(movie_data[2])]\n",
    "\n",
    "            movie_string = \"\"\n",
    "            #avoid the first three data points (user id, movieid, and rating)\n",
    "            #use only the text data...\n",
    "\n",
    "            #use all the test data...\n",
    "            # for index in range (3,len(movie_data)):\n",
    "            #     if(index!= len(movie_data)-1):\n",
    "            #         movie_string+= movie_data[index]+\" \"\n",
    "            #     else:\n",
    "            #         movie_string+= movie_data[index]\n",
    "\n",
    "            #only use the genres...\n",
    "            #all of the corpus and a few single columns were tested and with multple...\n",
    "            #technique the simple complete and unweighted average proved as a better featur einthe model than\n",
    "            movie_string = movie_data[4]\n",
    "\n",
    "            #lematization\n",
    "            cleaned = remove_stopwords(movie_string)\n",
    "            cleaned = [wnl.lemmatize(word) for word in cleaned.split(\" \")]\n",
    "            cleaned = [word[:-1] for word in cleaned if word.endswith(\".\")] + [word for word in cleaned if not word.endswith(\".\")]\n",
    "            #is copy really needed with this scope ???\n",
    "            movie_id_to_words_temp[movie_data[1]] = cleaned\n",
    "            movie_id_to_rating_temp[movie_data[1]] = float(movie_data[2])\n",
    "            movies_in_order.add(movie_data[1])\n",
    "            cnt+=1\n",
    "        user_to_movie_id_to_rating.append(movie_id_to_rating_temp)\n",
    "        user_to_average_rating.append(float(total/(cnt-1)))\n",
    "\n",
    "        #the current users list of words from all the movies they rated\n",
    "        users_words_in_order = OrderedSet()\n",
    "        for movie_id in movie_id_to_words_temp.keys():\n",
    "            for word in movie_id_to_words_temp[movie_id]:\n",
    "                users_words_in_order.add(word)\n",
    "\n",
    "\n",
    "        word_counts = []\n",
    "        target_word_counts = []\n",
    "\n",
    "        # these are only relevant with user averages scalingas opposed to movie average scaling...\n",
    "        word_counts_transformed = []\n",
    "        target_word_counts_transformed = []\n",
    "\n",
    "        #this will transform into the word count averages for each word for each movie rated by the user\n",
    "        averages = dict()\n",
    "\n",
    "        #for each movie the user watched record the wordcount for each word in the movies the user watched\n",
    "        for movie_id in movie_id_to_words_temp.keys():\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                temp_dict = Counter(movie_id_to_words_temp[movie_id])\n",
    "                temp_list = []\n",
    "                # avg = 0\n",
    "                # cnt =0\n",
    "                for word in users_words_in_order:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                        # avg+=temp_dict[word]\n",
    "                        # cnt+=1\n",
    "                        if word in averages.keys():\n",
    "                            averages[word] += temp_dict[word] \n",
    "                        else:\n",
    "                            averages[word] = temp_dict[word] \n",
    "                    else:\n",
    "                        temp_list.append(0) \n",
    "                        if word in averages.keys():\n",
    "                            averages[word] += 0 \n",
    "                        else:\n",
    "                            averages[word] = 0                        \n",
    "                #option 1:\n",
    "                # avg = float(avg/len(users_words_in_order))\n",
    "                #option 2:\n",
    "                # avg = float(avg/cnt)\n",
    "                word_counts.append(temp_list)\n",
    "                #option 1:\n",
    "                # temp_list_normalized = []\n",
    "                # for item in temp_list:\n",
    "                #     if item == 0:\n",
    "                #         temp_list_normalized.append(0)\n",
    "                #     else:\n",
    "                #         temp_list_normalized.append(item - avg)\n",
    "                # word_counts_transformed.append(temp_list_normalized)\n",
    "                #option 2:\n",
    "                # word_counts_transformed.append([x - avg for x in temp_list])\n",
    "            else:\n",
    "\n",
    "                temp_dict = Counter(movie_id_to_words_temp[movie_id])\n",
    "                temp_list = []\n",
    "                # avg = 0\n",
    "                # cnt =0\n",
    "                for word in users_words_in_order:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                        # avg+=temp_dict[word]\n",
    "                        # cnt +=1\n",
    "                        if word in averages.keys():\n",
    "                            averages[word] += temp_dict[word] \n",
    "                        else:\n",
    "                            averages[word] = temp_dict[word]             \n",
    "                    else:\n",
    "                        temp_list.append(0)\n",
    "\n",
    "                        if word in averages.keys():\n",
    "                            averages[word] += 0 \n",
    "                        else:\n",
    "                            averages[word] = 0\n",
    "                # option 1:\n",
    "                # avg = float(avg/len(users_words_in_order))\n",
    "                # option 2:\n",
    "                # avg = float(avg/cnt)\n",
    "                target_word_counts = temp_list\n",
    "                #option 1:\n",
    "                # target_word_counts_transformed = []\n",
    "                # for item in temp_list:\n",
    "                #     if item == 0:\n",
    "                #         target_word_counts_transformed.append(0)\n",
    "                #     else:\n",
    "                #         target_word_counts_transformed.append(item - avg)\n",
    "                #option 2:\n",
    "                #target_word_counts_transformed = [x - avg for x in temp_list]\n",
    "        \n",
    "\n",
    "        #create actual averages list... \n",
    "        #lambda can be used here...\n",
    "        def scale(x):\n",
    "            return float(x/len(movie_id_to_words_temp))\n",
    "        averages = [scale(averages[key]) for key in averages.keys()]\n",
    "\n",
    "        #option 1:\n",
    "        # complete = word_counts.copy()\n",
    "        # complete.append(target_word_counts)\n",
    "        # movie_averages = np.tile(averages, (len(complete),1))\n",
    "        # complete_array = np.array(complete)\n",
    "        # transformed_list  = list(complete_array - movie_averages)\n",
    "\n",
    "        #option 2:\n",
    "        complete_word_counts = word_counts.copy()\n",
    "        complete_word_counts.append(target_word_counts)\n",
    "        # tfidf = TfidfTransformer().fit_transform(complete_word_counts).toarray()\n",
    "        tfidf = complete_word_counts\n",
    "\n",
    "        #for content based reccomendation, may need to focus on words from certain sections to not overwelm the model with useless information!!!\n",
    "\n",
    "        def svd_u(a, n):\n",
    "            U, s, V = np.linalg.svd(a, full_matrices=False)\n",
    "            #adjust with n\n",
    "            s=np.diag(s)\n",
    "            s=s[0:n,0:n]\n",
    "            U=U[:,0:n]\n",
    "            V=V[0:n,:]\n",
    "            #could try returning UsV instead...\n",
    "            #could try truncated svd...\n",
    "            return list(U)\n",
    "        \n",
    "        #option 1:\n",
    "        #transformed_word_counts = svd_u(transformed_list, 5)\n",
    "        #option 2:\n",
    "        #transformed_word_counts = svd_u(tfidf, 5)\n",
    "        #option 3:\n",
    "        #transformed_word_counts = TruncatedSVD(n_components = 20, random_state = seed_int).fit_transform(tfidf)\n",
    "\n",
    "\n",
    "        #populate ratings with the exception of the target rating and also record\n",
    "        #the users target rating\n",
    "        ratings = []\n",
    "        for movie_id in movie_id_to_rating_temp.keys():\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                ratings.append(movie_id_to_rating_temp[movie_id])\n",
    "            else:\n",
    "                #this signifies the ratings to be predicted by the model\n",
    "                user_to_target_rating.append(movie_id_to_rating_temp[movie_id])\n",
    "        \n",
    "        #return the average ratings from movies that are alike the target movie...\n",
    "        def predict(word_counts_transformed, word_counts,\n",
    "            target_word_counts_transformed, target_word_counts, ratings):\n",
    "            item_1 = 0 \n",
    "            item_2 = 0\n",
    "            item_3 = 0\n",
    "\n",
    "            #option 0:\n",
    "            # mdl = KMeans(n_clusters=5, n_init = \"auto\", random_state= seed_int)\n",
    "            # predictions = mdl.fit_predict(list(transformed_word_counts))\n",
    "            # target_cluster = predictions[len(predictions)-1]\n",
    "\n",
    "            # same_cluster = list(filter((lambda item: item == target_cluster), predictions))\n",
    "\n",
    "            # if(len(same_cluster)==1):\n",
    "            #     #there are no other ratings in the target movies cluster...\n",
    "            #     distance = np.inf\n",
    "            #     index =0\n",
    "            #     i = 0 \n",
    "            #     for cord in mdl.cluster_centers_[0:-1]:\n",
    "            #         if math.dist(cord, mdl.cluster_centers_[-1]) < distance:\n",
    "            #             distance = math.dist(cord, mdl.cluster_centers_[-1])\n",
    "            #             index = i\n",
    "            #         i+=1\n",
    "            #     closest_cluster = predictions[index]\n",
    "            #     i =0\n",
    "            #     cnt =0\n",
    "            #     avg = 0\n",
    "            #     for item in predictions[0:-1]:\n",
    "            #         if item == closest_cluster:\n",
    "            #             avg+= ratings[i]\n",
    "            #             cnt+=1\n",
    "            #         i+=1\n",
    "            #     item_3 = float(avg/cnt)      \n",
    "\n",
    "            # else:\n",
    "            #     #there is at least one other movie in the target movies cluster\n",
    "            #     i =0\n",
    "            #     cnt =0\n",
    "            #     avg = 0\n",
    "            #     for item in predictions[0:-1]:\n",
    "            #         if item == target_cluster:\n",
    "            #             avg+= ratings[i]\n",
    "            #             cnt+=1\n",
    "            #         i+=1\n",
    "            #     item_3 = float(avg/cnt)\n",
    "\n",
    "            #option 1: \n",
    "            # cosine_sim = linear_kernel(X = tfidf[0:-1],Y = [tfidf[-1]])\n",
    "            # cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            # combined = zip(cosine_sim, ratings)\n",
    "            # combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "            # avg = 0\n",
    "            # nof = 10.0\n",
    "            # for i in range(int(nof)):\n",
    "            #     avg += combined[i][1]\n",
    "            # item_3 =  float(avg/nof)\n",
    "\n",
    "            #option 2:\n",
    "            avg = 0\n",
    "            for i in range(len(ratings)):\n",
    "                avg += ratings[i]\n",
    "            item_1 = float(avg/len(ratings))\n",
    "\n",
    "            #option 3:\n",
    "            # cosine_sim = linear_kernel(X = transformed_list[0:-1],Y = [transformed_list[-1]])\n",
    "            # cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            # combined = zip(cosine_sim, ratings)\n",
    "            # combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "            # avg = 0\n",
    "            # nof = 10.0\n",
    "            # for i in range(int(nof)):\n",
    "            #     avg += combined[i][1]\n",
    "            # item_1 = float(avg/nof)\n",
    "\n",
    "            #option 4:\n",
    "            # cosine_sim = linear_kernel(X = tfidf[0:-1],Y = [tfidf[-1]])\n",
    "            cosine_sim = cosine_similarity(X = tfidf[0:-1],Y = [tfidf[-1]])\n",
    "            cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            numerator = 0\n",
    "            denominator = 0\n",
    "            item_2 = item_1\n",
    "            for i in range(len(ratings)):\n",
    "                numerator += float(cosine_sim[i]*ratings[i])\n",
    "                denominator += cosine_sim[i]\n",
    "            \n",
    "            if denominator != 0:\n",
    "                item_2 = float(numerator/denominator)\n",
    "        \n",
    "            return (item_1, item_2)\n",
    "        \n",
    "        \n",
    "        items = predict(word_counts_transformed, word_counts,\n",
    "                                    target_word_counts_transformed, target_word_counts, ratings)\n",
    "\n",
    "        # feature_1.append(items[0])\n",
    "        feature_1.append(items[0])\n",
    "        feature_2.append(items[1])\n",
    "            \n",
    "        \n",
    "    return float(overall_rating_sum/overall_rating_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pre_svd(movie_id_to_average_rating, movies_in_order, user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, \n",
    "               user_to_movie_id_to_rating, user_to_target_movie_id):\n",
    "    for i in range(len(user_to_movie_id_to_rating)):\n",
    "        ratings = []\n",
    "        transformed_ratings = []\n",
    "        index = 0\n",
    "\n",
    "\n",
    "        #what if there is no movie_id == user_to_target_movie_id[i]\n",
    "        #this can happen when a test users target movie is not in the train_movies_in_order...\n",
    "\n",
    "        #solution:\n",
    "\n",
    "        #this could be run once with only train_movies\n",
    "        #and then used to populate the train svd\n",
    "        #and then extract the prediction to train the model\n",
    "\n",
    "\n",
    "        #then again with all movies train_movies + test_movies\n",
    "        #then used to populate the full svd\n",
    "        #and then extract the prediction to test model\n",
    "\n",
    "\n",
    "        #note: movie_id_to_average_rating_train shouold onyl be used for the train run of this function\n",
    "        #for the test version of the this movie_id_to_average_rating_full should be used\n",
    "\n",
    "        for movie_id in movies_in_order:\n",
    "            if movie_id == user_to_target_movie_id[i]:\n",
    "                user_to_target_index_full.append(index)\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(movie_id_to_average_rating[movie_id] - movie_id_to_average_rating[movie_id])             \n",
    "            elif movie_id in user_to_movie_id_to_rating[i].keys():\n",
    "                ratings.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(user_to_movie_id_to_rating[i][movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            else:\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(movie_id_to_average_rating[movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            index +=1\n",
    "        user_to_ratings_full.append(ratings)\n",
    "        user_to_ratings_full_transform.append(transformed_ratings)\n",
    "\n",
    "\n",
    "#note: before passing to this function the data is normalized about the average movie ratings (not average user ratings)\n",
    "#each user train and test users have a single rating that needs to be trained against in the train case\n",
    "#and predicted in the test case\n",
    "\n",
    "#the svd can be applied to the combined data of the train and test sets\n",
    "#both movies that the user didn't watch and movies that should be guesses are...\n",
    "#transformed to have a value of zero before svd\n",
    "\n",
    "#the movie columns are taken from the train dataset...\n",
    "#senario: suppose a test user has a rating of a movie not part of the train set and it is not the target movie (ignore it)\n",
    "#senario: suppose a test user has a rating of a movie not part of the train set and it is the target movie (guess the rating instead of using svd)\n",
    "\n",
    "#...Once the UsV is created...\n",
    "#take the rating from the new UsV for the user row and movie column for the target movie\n",
    "#other option: cossine similairty on the U ignoring other test users\n",
    "\n",
    "\n",
    "def svd_full(user_to_ratings_full_transform, n, movie_id_to_average_rating):\n",
    "    U, s, V = np.linalg.svd(user_to_ratings_full_transform, full_matrices=False)\n",
    "    \n",
    "    #simplify ratings to n features\n",
    "    s=np.diag(s)\n",
    "    s=s[0:n,0:n]\n",
    "    U=U[:,0:n]\n",
    "    V=V[0:n,:]\n",
    "\n",
    "    #reconstrcut to a new array\n",
    "    Us = np.dot(U,s)\n",
    "    UsV = np.dot(Us,V)\n",
    "    \n",
    "\n",
    "    #the keys of movie_id_to_ratings is in the same order of movies_in_order and therefore so is movie_id_to_average_rating_train\n",
    "    x = np.tile(list(movie_id_to_average_rating.values()), (UsV.shape[0],1))\n",
    "\n",
    "    #this tranforms the UsV row by row into the original rating scale (1-5)\n",
    "    UsV = UsV + x\n",
    "\n",
    "    #be consistent with data structures...\n",
    "    return list(UsV)\n",
    "\n",
    "\n",
    "\n",
    "overall_average_train = load_feature_1(train_movies_in_order, user_to_data_train, train_users.movie_id_to_ratings, train_users.user_to_movie_id_to_rating, \n",
    "                                                         train_users.user_to_target_movie_id, train_users.user_to_target_rating, train_users.feature_1, train_users.feature_2)\n",
    "\n",
    "\n",
    "load_feature_1(test_movies_in_order, user_to_data_test, test_users.movie_id_to_ratings, test_users.user_to_movie_id_to_rating, \n",
    "               test_users.user_to_target_movie_id,\n",
    "               test_users.user_to_target_rating, test_users.feature_1, test_users.feature_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#this has been replaced with load_feature_3...\n",
    "#it has much lower precision than load_feature_3\n",
    "#the feature simply provides an average rating from all users who rated the movie...\n",
    "#or fills in the next best guess...\n",
    "#note: with the current stack this depends on variables populated below it\n",
    "\n",
    "# def load_feature_2(train_user, feature_2):\n",
    "#     if(train_user):\n",
    "#         #populates feature_2_train\n",
    "#         for i in range(len(train_users.user_to_movie_id_to_rating)): \n",
    "#             if(len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]])==1):\n",
    "#                 #this means that there is no other rating besides the first train rating\n",
    "#                 feature_2.append(overall_average_train)\n",
    "#             else:\n",
    "#                 #omit the rating the user in question made\n",
    "#                 feature_2.append(float(((movie_id_to_average_rating_train[train_users.user_to_target_movie_id[i]]\n",
    "#                                 *len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]]))\n",
    "#                                 -train_users.user_to_movie_id_to_rating[i][train_users.user_to_target_movie_id[i]])\n",
    "#                                 /(len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]])-1)))\n",
    "#     else:   \n",
    "#         #populates feature_2_test\n",
    "#         for i in range(len(test_users.user_to_movie_id_to_rating)):\n",
    "#             if(test_users.user_to_target_movie_id[i] not in movie_id_to_ratings_total.keys()):\n",
    "#                 #if the movie is not in the train set make a guess\n",
    "#                 feature_2.append(overall_average_train)\n",
    "#             else:\n",
    "#                 #only use the train data\n",
    "#                 feature_2.append(movie_id_to_average_rating_train[test_users.user_to_target_movie_id[i]])\n",
    "\n",
    "\n",
    "# depreciated...\n",
    "# load_feature_2(True, train_users.feature_2)\n",
    "# load_feature_2(False, test_users.feature_2)\n",
    "\n",
    "\n",
    "#Unlike the other feature loading functions it only makes sense to run this once since...\n",
    "#there is significantly difference processes for train and test data\n",
    "def load_feature_3():\n",
    "\n",
    "    movie_id_to_average_rating_train = dict()\n",
    "    movie_id_to_average_rating_full = dict()\n",
    "\n",
    "    #is all_movies_in_order still in order???\n",
    "    all_movies_in_order = train_movies_in_order|test_movies_in_order\n",
    "\n",
    "    for movie in all_movies_in_order:\n",
    "        temp = 0\n",
    "        if(movie in train_users.movie_id_to_ratings and movie in test_users.movie_id_to_ratings):\n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_train[movie] = float(temp/len(train_users.movie_id_to_ratings[movie])) \n",
    "\n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_full[movie] = float(temp/(len(train_users.movie_id_to_ratings[movie])+len(test_users.movie_id_to_ratings[movie])))  \n",
    "\n",
    "        elif(movie in train_users.movie_id_to_ratings):\n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_train[movie] = float(temp/len(train_users.movie_id_to_ratings[movie]))\n",
    "            movie_id_to_average_rating_full[movie] = movie_id_to_average_rating_train[movie]\n",
    "\n",
    "        else:\n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_full[movie] = float(temp/len(test_users.movie_id_to_ratings[movie]))\n",
    "   \n",
    "\n",
    "    full_user_to_ratings_full_transform = []\n",
    "    full_user_to_ratings_full = []\n",
    "    full_user_to_target_index_full = []\n",
    "\n",
    "\n",
    "    full_user_to_movie_id_to_rating  = train_users.user_to_movie_id_to_rating + test_users.user_to_movie_id_to_rating\n",
    "    full_user_to_target_movie_id = train_users.user_to_target_movie_id + test_users.user_to_target_movie_id\n",
    "\n",
    "\n",
    "    #This is used to scale the ratings and store in train_users.user_to_ratings_full_transform and full_user_to_ratings_full_transform\n",
    "    #This will transform the target movie ratings and unrated movies to zero\n",
    "\n",
    "    #run once with only train data to train model\n",
    "    #run again with train and test data to evaluate model...\n",
    "\n",
    "    pre_svd(movie_id_to_average_rating_train, train_movies_in_order, train_users.user_to_ratings_full_transform, train_users.user_to_ratings_full, train_users.user_to_target_index_full, \n",
    "                train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id)\n",
    "\n",
    "    pre_svd(movie_id_to_average_rating_full, all_movies_in_order, full_user_to_ratings_full_transform, full_user_to_ratings_full, full_user_to_target_index_full, \n",
    "                full_user_to_movie_id_to_rating, full_user_to_target_movie_id)\n",
    "\n",
    "\n",
    "    #In practice, there is a train and a test set, the train set is what the database has on record\n",
    "    #the test data will usually be data that hasn't been seen before that can include any number of test users\n",
    "\n",
    "    #When train_users.user_to_ratings_full_transform is used as the input of the svd function, \n",
    "    #svd_out_train is used to produce predictions used to train the model\n",
    "\n",
    "    #When full_user_to_ratings_full_transform is used as the input of the svd function,\n",
    "    #svd_out_full is used to produce predictions used to test the model\n",
    "    \n",
    "\n",
    "    #n = 20 proved to be close to the highest performing constant for the above configuration\n",
    "    svd_out_train = svd_full(train_users.user_to_ratings_full_transform, 20, movie_id_to_average_rating_train)\n",
    "    svd_out_full = svd_full(full_user_to_ratings_full_transform, 20, movie_id_to_average_rating_full)\n",
    "\n",
    "    #here the smaller svd provides predictions used to train the mlp model\n",
    "    for i in range(len(train_users.user_to_ratings_full_transform)):\n",
    "        train_users.feature_3.append(svd_out_train[i][train_users.user_to_target_index_full[i]])\n",
    "\n",
    "    #here the larger svd provides predictions used to test the mlp model\n",
    "    for i in range(len(full_user_to_ratings_full_transform) - len(train_users.user_to_ratings_full_transform)):\n",
    "        test_users.feature_3.append(svd_out_full[i+len(train_users.user_to_ratings_full_transform)][full_user_to_target_index_full[i+len(train_users.user_to_ratings_full_transform)]])\n",
    "\n",
    "load_feature_3()\n",
    "\n",
    "print(train_users.feature_3[0:5])\n",
    "print(train_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(test_users.feature_3[0:5])\n",
    "print(test_users.user_to_target_rating[0:5])\n",
    "\n",
    "\n",
    "#used to see what the text data looks like...\n",
    "# not applicable with dict to list change...\n",
    "# file = open(\"test_dicts_1.txt\", 'w', encoding=\"utf-8\")\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_train))\n",
    "# file.close()\n",
    "\n",
    "# file = open(\"test_dicts_2.txt\", 'w', encoding=\"utf-8\")\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_test))\n",
    "# file.close()\n",
    "\n",
    "\n",
    "#this meight not be worth the deletion!!!\n",
    "# del user_to_data_train\n",
    "# del user_to_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "#what if populate 1 2 3 and 4 were combined???\n",
    "#this would mean that user_to_movie_id_to_corpus and user_to_words_in_order would be changed to temporary instead\n",
    "#to save memory...\n",
    "\n",
    "\n",
    "# def populate_2(user_to_movie_id_to_corpus, user_to_words_in_order):\n",
    "#     for i in range(len(user_to_movie_id_to_corpus)):\n",
    "#         user_to_words_in_order.append(OrderedSet())\n",
    "#         for movie_id in user_to_movie_id_to_corpus[i].keys():\n",
    "#             for word in user_to_movie_id_to_corpus[i][movie_id]:\n",
    "#                 user_to_words_in_order[i].add(word)\n",
    "\n",
    "# populate_2(train_users.user_to_movie_id_to_corpus,  train_users.user_to_words_in_order)\n",
    "# populate_2(test_users.user_to_movie_id_to_corpus,  test_users.user_to_words_in_order)\n",
    "\n",
    "\n",
    "\n",
    "# def populate_3(user_to_movie_id_to_corpus, user_to_target_movie_id, user_to_movie_id_to_rating,\n",
    "#     user_to_target_rating, user_to_ratings):\n",
    "#     for i in range(len(user_to_movie_id_to_corpus)):\n",
    "#         temp = []\n",
    "#         cnt = 0\n",
    "#         for movie_id in user_to_movie_id_to_corpus[i].keys():\n",
    "#             if movie_id != user_to_target_movie_id[i]:\n",
    "#                 temp.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "#             else:\n",
    "#                 user_to_target_rating.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "#                 temp.append(-1)\n",
    "#             cnt+=1\n",
    "#         user_to_ratings.append(temp)\n",
    "\n",
    "\n",
    "# populate_3(train_users.user_to_movie_id_to_corpus, train_users.user_to_target_movie_id, train_users.user_to_movie_id_to_rating,\n",
    "#             train_users.user_to_target_rating, train_users.user_to_ratings)\n",
    "# populate_3(test_users.user_to_movie_id_to_corpus, test_users.user_to_target_movie_id, test_users.user_to_movie_id_to_rating,\n",
    "#             test_users.user_to_target_rating, test_users.user_to_ratings)\n",
    "\n",
    "\n",
    "\n",
    "# def populate_4(user_to_word_counts_transformed, user_to_target_word_counts_transformed,\n",
    "#                 user_to_movie_id_to_corpus, user_to_word_counts,\n",
    "#                 user_to_target_movie_id, user_to_words_in_order, user_to_target_word_counts):\n",
    "#     for i in range(len(user_to_movie_id_to_corpus)):\n",
    "#         user_to_word_counts.append([])\n",
    "#         user_to_word_counts_transformed.append([])\n",
    "#         for movie_id in user_to_movie_id_to_corpus[i].keys():\n",
    "#             if movie_id != user_to_target_movie_id[i]:\n",
    "#                 temp_dict = Counter(user_to_movie_id_to_corpus[i][movie_id])\n",
    "#                 temp_list = []\n",
    "#                 avg = 0\n",
    "#                 for word in user_to_words_in_order[i]:\n",
    "#                     if word in temp_dict.keys():\n",
    "#                         temp_list.append(temp_dict[word])\n",
    "#                         avg+=temp_dict[word]\n",
    "#                     else:\n",
    "#                         temp_list.append(0)\n",
    "#                 avg = float(avg/len(user_to_words_in_order[i]))\n",
    "#                 user_to_word_counts[i].append(temp_list)\n",
    "#                 user_to_word_counts_transformed[i].append([x - avg for x in temp_list])\n",
    "#             else:\n",
    "\n",
    "#                 temp_dict = Counter(user_to_movie_id_to_corpus[i][movie_id])\n",
    "#                 temp_list = []\n",
    "#                 avg = 0\n",
    "#                 for word in user_to_words_in_order[i]:\n",
    "#                     if word in temp_dict.keys():\n",
    "#                         temp_list.append(temp_dict[word])\n",
    "#                         avg+=temp_dict[word]\n",
    "#                     else:\n",
    "#                         temp_list.append(0)\n",
    "#                 avg = float(avg/len(user_to_words_in_order[i]))\n",
    "#                 user_to_target_word_counts.append(temp_list)\n",
    "#                 user_to_target_word_counts_transformed.append([x - avg for x in temp_list])\n",
    "\n",
    "\n",
    "# populate_4(train_users.user_to_word_counts_transformed, train_users.user_to_target_word_counts_transformed,\n",
    "#            train_users.user_to_movie_id_to_corpus, train_users.user_to_word_counts, train_users.user_to_target_movie_id,\n",
    "#        train_users.user_to_words_in_order, train_users.user_to_target_word_counts)\n",
    "\n",
    "# populate_4(test_users.user_to_word_counts_transformed, test_users.user_to_target_word_counts_transformed,\n",
    "#            test_users.user_to_movie_id_to_corpus, test_users.user_to_word_counts, \n",
    "#        test_users.user_to_target_movie_id, test_users.user_to_words_in_order, test_users.user_to_target_word_counts)\n",
    "\n",
    "# del train_users.user_to_words_in_order\n",
    "# del train_users.user_to_movie_id_to_corpus\n",
    "# del test_users.user_to_words_in_order\n",
    "# del test_users.user_to_movie_id_to_corpus\n",
    "\n",
    "\n",
    "\n",
    "#now for each user, use the user_to_index_full and find the most similair users by omitting that index across the\n",
    "#sim_matrix\n",
    "\n",
    "\n",
    "\n",
    "#collaboritive filtering idea\n",
    "#data structures:\n",
    "#need a user to movies to ratings dictionary (done) (only needed for train data)\n",
    "#need a ordered set of all movies (done) (only needed for train data)\n",
    "\n",
    "#need to transform it into a user to list of all movies with user ratings or otherwise filled in ratings with sutiable average\n",
    "#(the list needs to be in a consistent order across users)\n",
    "#to fill in the averages we need a movie to average rating dictionary\n",
    "#(note: if no other user has rated the movie then fill it in with the overall movie average)\n",
    "#standardize the data row wise (why not columnwise???)\n",
    "#filter the users that have rated the movie to predict\n",
    "#then use cossine similairity on this set to find the most similair user the the chosen user\n",
    "#ignore the movie rating to predict with the cossine similarity function\n",
    "\n",
    "#Note: this is an expensive task and the number of users may have to be truncated before running\n",
    "\n",
    "\n",
    "\n",
    "#ideas: \n",
    "#idea1: \n",
    "#collaborative filtering:\n",
    "#https://towardsdatascience.com/predict-movie-ratings-with-user-based-collaborative-filtering-392304b988af\n",
    "#https://www.geeksforgeeks.org/user-based-collaborative-filtering/#\n",
    "#https://www.youtube.com/watch?v=3ecNC-So0r4&ab_channel=CodeHeroku\n",
    "#idea2: \n",
    "#note: the model can be scored based on how close a predition is to a threshold of .5 \n",
    "\n",
    "#idea 3:\n",
    "#there may be a replacement for cossine similarity:\n",
    "#perhaps it is not properly matching similair people and similair movies \n",
    "#maybe there is a replacement function\n",
    "\n",
    "#idea 4: \n",
    "#there should be more train and test users\n",
    "\n",
    "#idea 5: (basic model)\n",
    "#https://www.kaggle.com/code/muhammadayman/recommendation-system-using-cosine-similarity#Data-Cleaning\n",
    "\n",
    "\n",
    "\n",
    "#10 minutes for 2500 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "# def predict(user, user_to_word_counts_transformed, user_to_target_word_counts_transformed,\n",
    "#             user_to_target_word_counts, user_to_word_counts, user_to_ratings):\n",
    "\n",
    "\n",
    "#     cosine_sim = cosine_similarity(X = user_to_word_counts_transformed[user] ,Y = [user_to_target_word_counts_transformed[user]])\n",
    "    \n",
    "#     cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "#     ratings = [user_to_ratings[user][x] for x in range(len(user_to_ratings[user])) if user_to_ratings[user][x] != -1]\n",
    "\n",
    "\n",
    "#     #use the movie thats are most similair to the movie in question \n",
    "\n",
    "#     #option 1: \n",
    "#     # combined = zip(cosine_sim, ratings)\n",
    "#     # combined = sorted(combined, key=lambda x: x[0], reverse=False)\n",
    "#     # avg = 0\n",
    "#     # nof = 10.0\n",
    "#     # for i in range(int(nof)):\n",
    "#     #     avg += combined[i][1]\n",
    "#     # return float(avg/nof)\n",
    "\n",
    "#     #option 2:\n",
    "#     # avg = 0\n",
    "#     # for i in range(len(ratings)):\n",
    "#     #     avg += ratings[i]\n",
    "#     # return float(avg/len(ratings))\n",
    "\n",
    "#     #option 3: \n",
    "#     combined = zip(cosine_sim, ratings)\n",
    "#     combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "#     avg = 0\n",
    "#     nof = 10.0\n",
    "#     for i in range(int(nof)):\n",
    "#         avg += combined[i][1]\n",
    "#     return float(avg/nof)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def populate_5(user_to_word_counts_transformed, user_to_target_word_counts_transformed,\n",
    "#                user_to_movie_id_to_rating, feature_1,\n",
    "#                 user_to_target_word_counts, user_to_word_counts,\n",
    "#                 user_to_ratings):\n",
    "#     for i in range(len(user_to_movie_id_to_rating)):\n",
    "#         feature_1.append(predict(i, user_to_word_counts_transformed, user_to_target_word_counts_transformed,\n",
    "#                                   user_to_target_word_counts, user_to_word_counts, user_to_ratings))\n",
    "\n",
    "\n",
    "\n",
    "# populate_5(train_users.user_to_word_counts_transformed, train_users.user_to_target_word_counts_transformed,\n",
    "#            train_users.user_to_movie_id_to_rating, train_users.feature_1,\n",
    "#                 train_users.user_to_target_word_counts, train_users.user_to_word_counts,\n",
    "#                 train_users.user_to_ratings)\n",
    "\n",
    "# populate_5(test_users.user_to_word_counts_transformed, test_users.user_to_target_word_counts_transformed,\n",
    "#            test_users.user_to_movie_id_to_rating, test_users.feature_1,\n",
    "#                 test_users.user_to_target_word_counts, test_users.user_to_word_counts,\n",
    "#                 test_users.user_to_ratings)\n",
    "\n",
    "# del train_users.user_to_word_counts\n",
    "# del test_users.user_to_word_counts\n",
    "# del train_users.user_to_word_counts_transformed\n",
    "# del test_users.user_to_word_counts_transformed\n",
    "# del train_users.user_to_target_word_counts\n",
    "# del test_users.user_to_target_word_counts\n",
    "# del train_users.user_to_target_word_counts_transformed\n",
    "# del test_users.user_to_target_word_counts_transformed\n",
    "# del train_users.user_to_ratings\n",
    "# del test_users.user_to_ratings\n",
    "\n",
    "\n",
    "#note: for some reason feature 2 out performs feature 3 as a feature...\n",
    "\n",
    "# for i in range(len(train_users.user_to_movie_id_to_rating)): \n",
    "#     if(len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]])==1):\n",
    "#         train_users.feature_2.append(overall_average_train)\n",
    "#     else:\n",
    "#         train_users.feature_2.append(float(((movie_id_to_average_rating_train[train_users.user_to_target_movie_id[i]]\n",
    "#                         *len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]]))\n",
    "#                         -train_users.user_to_movie_id_to_rating[i][train_users.user_to_target_movie_id[i]])\n",
    "#                         /(len(movie_id_to_ratings_total[train_users.user_to_target_movie_id[i]])-1)))\n",
    "\n",
    "\n",
    "# for i in range(len(test_users.user_to_movie_id_to_rating)):\n",
    "#     if(test_users.user_to_target_movie_id[i] not in movie_id_to_ratings_total.keys()):\n",
    "#         test_users.feature_2.append(overall_average_train)\n",
    "#     else:\n",
    "#         test_users.feature_2.append(movie_id_to_average_rating_train[test_users.user_to_target_movie_id[i]])\n",
    "\n",
    "\n",
    "\n",
    "# def populate_6(user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, \n",
    "#                user_to_movie_id_to_rating, user_to_target_movie_id):\n",
    "#     for i in range(len(user_to_movie_id_to_rating)):\n",
    "#         ratings = []\n",
    "#         index = 0\n",
    "#         for movie_id in movies_in_order:\n",
    "#             if movie_id == user_to_target_movie_id[i]:\n",
    "#                 user_to_target_index_full.append(index)\n",
    "#             if movie_id in user_to_movie_id_to_rating[i].keys():\n",
    "#                 ratings.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "#             elif movie_id in movie_id_to_average_rating_train.keys():\n",
    "#                 ratings.append(movie_id_to_average_rating_train[movie_id])\n",
    "#             else:   \n",
    "#                 ratings.append(overall_average_train)\n",
    "#             index +=1\n",
    "#         user_to_ratings_full.append(ratings)\n",
    "#         user_to_ratings_full_transform.append([x - user_to_average_rating[i] for x in ratings])\n",
    "\n",
    "\n",
    "# populate_6(train_users.user_to_ratings_full_transform, train_users.user_to_ratings_full, train_users.user_to_target_index_full, train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id)\n",
    "# populate_6(test_users.user_to_ratings_full_transform, test_users.user_to_ratings_full, test_users.user_to_target_index_full, test_users.user_to_movie_id_to_rating, test_users.user_to_target_movie_id)\n",
    "\n",
    "\n",
    "# del train_users.user_to_movie_id_to_rating\n",
    "# del test_users.user_to_movie_id_to_rating\n",
    "# del train_users.user_to_target_movie_id\n",
    "# del test_users.user_to_target_movie_id\n",
    "\n",
    "#why is feature 3 outperformed by feature 2 ???\n",
    "\n",
    "# def populate_7(user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, feature_3):\n",
    "#     for i in range(len(user_to_ratings_full)):\n",
    "#         list_of_list_of_ratings = []\n",
    "#         sample_ratings = []\n",
    "#         for j in range(len(train_users.user_to_ratings_full)):\n",
    "#             if i != j:\n",
    "#                 sample_ratings.append(train_users.user_to_ratings_full[j][user_to_target_index_full[i]])\n",
    "#                 ratings = [train_users.user_to_ratings_full_transform[j][x] \n",
    "#                           for x \n",
    "#                           in range(len(train_users.user_to_ratings_full_transform[j])) \n",
    "#                           if x != user_to_target_index_full[i]]\n",
    "#                 list_of_list_of_ratings.append(ratings)\n",
    "\n",
    "#         user_ratings = [user_to_ratings_full_transform[i][x] \n",
    "#                     for x \n",
    "#                     in range(len(user_to_ratings_full_transform[i])) \n",
    "#                     if x != user_to_target_index_full[i]]\n",
    "\n",
    "\n",
    "#         sim  = cosine_similarity(X = list_of_list_of_ratings, Y = [user_ratings])\n",
    "#         sim = np.reshape(sim,  (len(sim)))\n",
    "\n",
    "#         combined = zip(sim, sample_ratings)\n",
    "#         combined = sorted(combined, key=lambda x: x[0], reverse = True)\n",
    "\n",
    "#         avg = 0\n",
    "#         nof = 10.0\n",
    "#         for k in range(int(nof)):\n",
    "#             avg+= combined[k][1]\n",
    "\n",
    "#         feature_3.append(float(avg/nof))\n",
    "    \n",
    "\n",
    "# populate_7(train_users.user_to_ratings_full_transform, train_users.user_to_ratings_full, train_users.user_to_target_index_full, train_users.feature_3)\n",
    "# populate_7(test_users.user_to_ratings_full_transform, test_users.user_to_ratings_full, test_users.user_to_target_index_full, test_users.feature_3)\n",
    "\n",
    "# del train_users.user_to_ratings_full_transform\n",
    "# del test_users.user_to_ratings_full_transform\n",
    "# del train_users.user_to_ratings_full\n",
    "# del test_users.user_to_ratings_full\n",
    "# del train_users.user_to_target_index_full\n",
    "# del test_users.user_to_target_index_full\n",
    "\n",
    "\n",
    "#adjusted cossine simlairity\n",
    "#https://stackoverflow.com/questions/40716459/choice-between-an-adjusted-cosine-similarity-vs-regular-cosine-similarity\n",
    "#https://github.com/csaluja/JupyterNotebooks-Medium/blob/master/CF%20Recommendation%20System-Examples.ipynb?source=post_page-----ecbffe1c20b1--------------------------------\n",
    "#https://towardsdatascience.com/collaborative-filtering-based-recommendation-systems-exemplified-ecbffe1c20b1\n",
    "\n",
    "\n",
    "#note: in the item-based collabortive filtering the means is implementaed item-wise\n",
    "#this means with user-based collaboritve filtering the mean is implementad user-wise\n",
    "\n",
    "\n",
    "#idea 1:\n",
    "#use diferrent data structures...\n",
    "#numpy arrays...\n",
    "#https://www.geeksforgeeks.org/python-lists-vs-numpy-arrays/#\n",
    "#https://stackoverflow.com/questions/29839350/numpy-append-vs-python-append\n",
    "#https://www.geeksforgeeks.org/python-convert-list-to-python-array/\n",
    "\n",
    "\n",
    "#idea 2:\n",
    "#make use of both feature_2 and feature_3\n",
    "\n",
    "#idea 3: would it be possible to convert many of the main dictionaries to lists or numpy arrays\n",
    "#note: numpy arrays cant have variable number of items for a dimension\n",
    "#but this can be over come by filling missing values\n",
    "#this mean many matrices would be sparse\n",
    "#there is also alot of appending to lists which is better for lists\n",
    "\n",
    "\n",
    "#idea 4: would it be helpful to design my own data structures???\n",
    "\n",
    "#idea 5: would it be helpful to have features like the average 10 closest in similairy and the 10 farthest in similarity???\n",
    "\n",
    "#idea 6: before trying this there could be better data strcuture to use that would speed up the process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1002884  0.58549501]\n",
      "[0.10267874 0.58037645]\n",
      "[0.09645489 0.56732176]\n",
      "[0.1044672  0.55478207]\n",
      "[0.10889817 0.56820917]\n",
      "[0.09737916 0.56145858]\n",
      "[0.09682232 0.58438757]\n",
      "[0.09323977 0.564279  ]\n",
      "[0.08889688 0.58432735]\n",
      "[0.10497018 0.55868797]\n",
      "[0.09259249 0.57643067]\n",
      "[0.10672189 0.57893932]\n",
      "[0.10960773 0.5608381 ]\n",
      "[0.10309682 0.57998038]\n",
      "[0.10658678 0.57719339]\n",
      "[0.09039506 0.57027621]\n",
      "[0.11052998 0.57163792]\n",
      "[0.0932738 0.5852049]\n",
      "[0.09736236 0.58072844]\n",
      "[0.09782407 0.5625025 ]\n",
      "[0.09528488 0.58468399]\n",
      "[0.10943289 0.56503522]\n",
      "[0.10232406 0.55694792]\n",
      "[0.10555528 0.56646775]\n",
      "[0.09283921 0.57899795]\n",
      "[0.10873073 0.57654713]\n",
      "[0.10637628 0.57693963]\n",
      "[0.10416995 0.5647271 ]\n",
      "[0.09564712 0.54069205]\n",
      "[0.09881408 0.60598233]\n",
      "[0.09924902 0.55721728]\n",
      "[0.10415639 0.56590407]\n",
      "[0.10732535 0.5798068 ]\n",
      "[0.09405545 0.58141842]\n",
      "[0.09604926 0.56392561]\n",
      "[0.11072843 0.55735896]\n",
      "[0.10310933 0.56411879]\n",
      "[0.10759208 0.5897777 ]\n",
      "[0.10426132 0.57016029]\n",
      "[0.09157975 0.57855839]\n",
      "[0.11001998 0.57529705]\n",
      "[0.08725453 0.56814051]\n",
      "[0.0976769  0.55786494]\n",
      "[0.10158003 0.56989284]\n",
      "[0.11151479 0.56371335]\n",
      "[0.09712708 0.57946947]\n",
      "[0.10424316 0.5560085 ]\n",
      "[0.10102367 0.55519602]\n",
      "[0.10193381 0.56106138]\n",
      "[0.09573963 0.54520096]\n",
      "[0.10580694 0.57803195]\n",
      "[0.08540065 0.58105145]\n",
      "[0.09371309 0.56892466]\n",
      "[0.09493604 0.57013739]\n",
      "[0.10070953 0.55861048]\n",
      "[0.09864575 0.54991958]\n",
      "[0.09440476 0.56755211]\n",
      "[0.09776592 0.5689223 ]\n",
      "[0.0877469  0.58149984]\n",
      "[0.10473432 0.5743553 ]\n",
      "[0.10556443 0.57044264]\n",
      "[0.10771179 0.56527948]\n",
      "[0.11365543 0.57244778]\n",
      "[0.09909657 0.57378223]\n",
      "[0.10588642 0.56306497]\n",
      "[0.1047104  0.56560425]\n",
      "[0.09799281 0.56254403]\n",
      "[0.09677201 0.56134038]\n",
      "[0.08635096 0.5880908 ]\n",
      "[0.09921112 0.5575299 ]\n",
      "[0.11589922 0.55369311]\n",
      "[0.10477315 0.5468993 ]\n",
      "[0.09749742 0.56807954]\n",
      "[0.10008173 0.58335466]\n",
      "[0.08606591 0.55476262]\n",
      "[0.10106769 0.57071322]\n",
      "[0.09866437 0.57116369]\n",
      "[0.09749701 0.57722126]\n",
      "[0.11453939 0.57880276]\n",
      "[0.10591551 0.58114087]\n",
      "[0.11473469 0.57349367]\n",
      "[0.08835856 0.57586905]\n",
      "[0.09087185 0.58715165]\n",
      "[0.10500505 0.58093555]\n",
      "[0.09614651 0.58674665]\n",
      "[0.09601763 0.55921278]\n",
      "[0.10060295 0.55649518]\n",
      "[0.09040623 0.57585977]\n",
      "[0.09065338 0.56842516]\n",
      "[0.10635417 0.55032591]\n",
      "[0.10214976 0.56861445]\n",
      "[0.09965793 0.55867513]\n",
      "[0.1069093 0.5839666]\n",
      "[0.10313258 0.55575983]\n",
      "[0.10065396 0.54450395]\n",
      "[0.11544046 0.57600224]\n",
      "[0.09857378 0.57552817]\n",
      "[0.10232527 0.5806048 ]\n",
      "[0.10342973 0.56542557]\n",
      "[0.11495422 0.58201972]\n",
      "(0.37405806894377536, 0.354045341397552)\n",
      "[3.3650977256546217, 1.852360424015049, 3.1602965447591753, 4.007377997918515, 4.019528390228162, 2.974487592401399, 3.203900414114437, 3.732996314428444, 3.4961901056441427, 3.212316635145916, 3.4165621136762767, 3.5607363081920353, 3.3189006926420674, 3.472614731175266, 3.8374658410131532, 2.871285560148952, 3.488722417570457, 2.896598220235676, 3.1045236395746585, 3.8795588807156305, 3.310857584048099, 3.6014656044083684, 2.3325656402126125, 3.791714684513611, 3.603185676309702, 1.8904628406507495, 2.6643122682132128, 3.6066068309171273, 2.804850553747488, 2.822420564534038, 3.2180136693206527, 2.8871769546356156, 3.655539058673886, 3.9548017256544035, 3.0690893098390006, 3.0879518434120605, 3.5375058827889987, 2.330649720282588, 3.961922379658738, 3.485401644915236, 3.4061656916196026, 4.002965316206901, 3.5580496190546094, 3.784964828783868, 2.862727445192683, 4.014340897186878, 3.553110223476928, 4.18496798930544, 3.8133178508712, 3.081454031746149, 3.0663632677573447, 3.3700896011644064, 3.9445527684323896, 3.1956511144086006, 3.9687107903372087, 3.1142055740155965, 4.0413876805363085, 3.1199151703823427, 4.14367216524699, 2.9617564075118237, 3.3585943688274194, 3.6506948100896293, 2.041480644230976, 3.922924230828511, 2.8980676815917867, 3.9570958620334498, 3.359347152891135, 3.4695908467421614, 3.4961909424266686, 3.8232936533492583, 4.149668018861601, 3.592303913283714, 3.7528074573342365, 4.100624504183156, 3.7336536708107477, 2.438804455580438, 3.2468980574873463, 2.946480096041062, 3.0347004657216425, 3.918461974506721, 3.5919140565828918, 3.37606892320325, 3.352089690253362, 3.730640677631347, 3.374801429479198, 4.412607748293237, 3.167298316159601, 4.665957366030662, 4.021790631141062, 3.5444264928297993, 3.7903047793131943, 4.287730460454853, 3.4520755230277773, 2.955476397712479, 3.837830086777251, 4.258884434417478, 3.808950037943345, 3.448905173720391, 3.882195981687448, 2.9731454042274286, 2.939461225031036, 3.1935919315353036, 3.8580789995059765, 2.970637637601396, 3.0306121993996507, 3.4450326276718024, 3.8719779469268527, 3.9606112845778587, 3.994013463484621, 3.4379444154405916, 3.742306567826624, 2.373204063760706, 2.018086908695323, 4.18191310054929, 4.7824097921099655, 2.9335881584579013, 3.956229902384773, 3.353637137140987, 2.9404335501828682, 4.721976894969463, 3.15833766801832, 2.9543294368572868, 3.8457491431144963, 2.9218869755634067, 3.812792877435786, 1.7743735774557365, 2.8080765469972837, 2.8151365543048477, 3.2085279519711007, 3.9033940364335056, 3.0612284486337935, 3.5511024037313708, 3.3532201346427, 3.451486268906044, 4.011722775695243, 2.6529075272692313, 2.7862370112656323, 3.0444029044919003, 2.8540865630054046, 1.9884471646795374, 3.8305133670930105, 3.2997091281648148, 3.671025574771004, 3.9562564470517767, 2.7247136551236366, 3.3329296124352696, 3.0813276738738966, 3.365794428204768, 3.046650183557527, 3.7173904788167076, 4.060558469444862, 3.171117302521096, 2.5947994487077835, 3.6800037043432097, 3.3316194491865767, 4.618926788583501, 3.7851278581533268, 2.851121640188609, 3.092585362756674, 3.172572311829725, 3.261721362591005, 2.2396920936176503, 3.7129187022998504, 4.062834551646776, 3.5625435206330867, 2.9945891567445706, 3.4159370368940585, 3.7532476778736807, 4.115990475661957, 4.308632212682972, 3.4123957995707137, 3.0401935929155743, 2.8895024538530913, 2.628262139392598, 3.2229483366202167, 2.9355684689985115, 2.669848577998891, 3.2669712495688885, 3.4110233638631033, 3.662477080034405, 3.8266854694222348, 4.261835032182392, 3.7188579732771117, 3.277763879028799, 2.4666749424951915, 3.581057703100114, 3.8705032759246962, 2.740647586597388, 3.1356769322952913, 3.248360670197518, 2.977880209790235, 3.759824580726293, 3.278650178321135, 2.4307426921718998, 3.058205717424674, 3.5356272660115917, 3.0523579469127005, 4.208838272331503, 3.1568884247475886, 2.347582265290697, 2.922206224098454, 3.1214284035412665, 3.971169830208947, 3.987363128869399, 3.1312718693284176, 3.8235260204069217, 2.989232694685713, 3.671972526708711, 4.203109997009187, 3.779959395065043, 2.229400622323538, 3.8545054507807084, 4.024583519433196, 2.904014870151159, 3.9030978781710837, 3.356123731849646, 3.830613204879219, 3.27833888857096, 3.100168376415013, 3.768111670316035, 2.8392540975634937, 3.551865525726832, 3.12238532912823, 3.425904712021293, 2.5604706394992136, 3.5684512307437632, 2.6079370252328973, 3.2835884369027863, 3.7786281663898493, 3.75499251937548, 3.0498135771249855, 3.9159707407953785, 4.338564302675976, 4.291345891410753, 3.184813900921387, 4.161638899411144, 3.0252704131499617, 2.8698043348279985, 4.191654117297902, 3.4738287698456407, 3.5510518503768824, 3.9372466507139854, 4.539007702161857, 4.119626879739171, 3.6864550508156606, 3.382035980517527, 3.5945133375094334, 3.7984535190463764, 4.117204540699398, 4.155028503888841, 2.197278512086013, 3.848870848679133, 3.925356671931358, 4.250075842508045, 4.059812668519149, 3.1302050723538404, 3.4097246892759063, 3.145552599497309, 3.5039517052163514, 2.7784206399448754, 3.8138485313414088, 3.31129606335012, 3.634424160956027, 2.8034457140298175, 2.1489779931012487, 3.77064694536779, 2.493129608965516, 3.5032550593140526, 3.6308482295038327, 3.3344105713275325, 3.5999515350905456, 3.901179366825173, 3.666822763577389, 3.827864318385169, 1.7461717171342965, 3.4881873366202236, 3.4579244042144053, 4.209840098613993, 4.239191616054719, 3.3992053867265977, 3.419335191194928, 3.605383829565964, 3.9284586124432685, 2.4059367964647227, 3.5042629134004355, 3.9376519441449296, 3.7887321284177116, 3.562530922253369, 4.1236179342683865, 3.523276948290892, 3.0786173358665447, 3.775995994107172, 3.5145144170229954, 3.391327922259851, 4.170579625048663, 2.644523630324226, 4.2180618167644095, 2.6344376368202593, 3.4850152421058986, 3.750176415727695, 3.398352503777223, 2.982126513811762, 3.587550540921382, 2.7146802050292074, 4.208860348287237, 3.9844237425786555, 3.7425733190018637, 3.8457291809272163, 3.1238493671738623, 3.7377406988246396, 2.316200594166015, 3.993433800212957, 4.056398771128311, 3.1764637422626447, 3.1686291414880703, 2.6210778306149978, 3.6604177792233736, 3.8073828733647948, 3.365673536223957, 1.737554424450643, 4.188762724381523, 3.9976744072199097, 3.4904099843854137, 4.908134604719407, 3.4449793828555966, 3.732159463337612, 3.3274414529527157, 2.446140217639606, 3.069648375665208, 3.9842020626378796, 3.5807454097400426, 3.287725464588999, 3.5646430724890146, 3.6186013244209354, 3.2629483205137664, 3.5999415321432657, 3.0348599832253726, 3.606333241004452, 4.286104418220781, 3.2804154434785704, 3.5159726016479476, 3.952848492922891, 2.834335311501785, 2.4692117055742018, 2.9213218231585674, 2.8769622514487962, 3.9433320228536486, 3.288138084030867, 3.8369001281898867, 3.8784434927600087, 2.9730091838573984, 3.299565953790827, 3.107835039424861, 3.8562507690177026, 3.2676162492178378, 1.6647331130141116, 3.1479657727877246, 3.690565466005664, 3.7182102822130148, 3.072328517174416, 3.2241605079913995, 3.8365087507825613, 3.6649665797986204, 3.007997544412018, 2.6673072094087247, 2.407830757208174, 3.563718542409411, 2.6108347789123956, 3.7758846038765714, 2.520698284735158, 3.227304024554394, 3.233781040618367, 2.8551664447668665, 3.3681168223224573, 2.7541096474349573, 3.6459299560284237, 3.689253846823392, 4.319776839261774, 2.240205734575318, 3.709157286706734, 4.194524136829373, 3.7860884700378343, 2.581502581542103, 4.104017193511189, 3.8116148641303766, 3.9608255417728255, 3.7597829999725767, 3.7605788482175666, 3.1757154951853193, 2.892912425493011, 3.5898566398035903, 3.1152826824677087, 3.4638128941462334, 3.3431326788887237, 2.7831728537400067, 3.38023176842512, 4.617354206349211, 2.8880877436833754, 4.493915513354702, 4.298436175105424, 3.159580721824167, 3.316623246206317, 2.6398276390075925, 3.107018577270354, 3.5874235749822576, 4.313155655142473, 3.300454583428867, 3.5947585292661537, 3.7582022666736714, 3.7575936894164212, 4.127883938554051, 2.678499704579064, 3.554813932336939, 3.9805505620548143, 3.883299239838329, 3.640615483908823, 1.6703373412530247, 3.590539017853127, 3.4681753886552444, 3.587821026689189, 3.481140170416551, 2.782705770747209, 3.8929806529409827, 4.1874199282484525, 2.8027620671118836, 4.1369155231017665, 3.4792672646260456, 2.667372513177223, 3.431205418486971, 3.330649989961502, 3.6511359323698316, 3.6564896176022086, 2.858773991997761, 3.723953276467518, 3.6751409067985055, 2.726855122824691, 4.363690341594033, 4.013176433089318, 3.9507815304698655, 2.0492084225358456, 3.732925882251978, 3.095739375610809, 3.570545332897142, 3.7990034260997136, 3.8584126655846944, 4.053630913256251, 3.684510613068829, 3.5132886282751734, 3.905174605224, 3.6219715379953716, 3.888095584573066, 3.677199676860255, 3.834563765150865, 4.037316711480224, 2.2508529812880083, 3.749324984447184, 4.730706454620507, 3.1584856932818894, 3.2131438143303566, 2.2153044754765836, 3.7165092462126723, 3.6451684637014115, 2.9905566874140326, 3.40306573248071, 3.84314836796118, 3.1188324482701013, 3.5232878856895353, 3.36164603238857, 2.962590760241941, 3.7578890029077288, 4.224715851605429, 4.021860737275835, 2.9921740870115947, 3.912674451645228, 2.893039455565079, 3.3640621038131564, 3.314541794062033, 2.8094133661861895, 3.9829795531988497, 2.3554913904249277, 3.0684912636306567, 3.4553069510448706, 3.5126640860470224, 3.4979442782692396, 2.3595394320131433, 2.7904603594815973, 4.036777626534877, 3.5376986741872085, 2.926564343851867, 3.5237484762767832, 2.901850980821039, 3.172815995399973, 3.7991684210278236, 3.493634350440293, 3.2193446923140216, 3.852825775720111, 3.247901093511715, 3.77541060521013, 3.338670080643341, 2.533268164497229, 3.4564616531672723, 4.136430518676677, 3.908657752459001, 3.592406784260601, 2.3962784276496745, 3.421384519436515, 2.6015878265331835, 2.5235958457614642, 4.445039505426686, 3.4336638182939483, 2.9651277849017905, 3.5815435289218707, 3.6121593480161764, 3.7164011650665465, 2.8699836740737137, 3.195291228095312, 3.986278039253169, 4.184751371880753, 2.874728721134948, 3.666424432524321, 3.6327553474080503, 2.100175154399971, 4.039205573644062, 3.2225882699214177, 3.9548513304876547, 3.558814296555138, 3.8742784136670108, 3.646138825602892, 3.5835475017380447, 3.1371690348954586, 4.136874490270772, 3.9015679626763755, 3.1066775986338646, 3.5432236064493665, 4.4470061097057325, 3.6229906975602644, 3.6016247785819355, 3.1619281946695845, 2.833978854988142, 3.8293835651233064, 4.4919136977577585, 3.4375860609124493, 3.3667121403667357, 3.621548483268835, 2.889062343430227, 3.2275149925631905, 3.0927224319747775, 3.6646422381166035, 3.9773538282736287, 3.747356655539244, 2.5785457896920536, 4.039018636241962, 3.063914011178774, 2.8272019235730754, 3.661058409398669, 2.8912034700416065, 3.242232826450245, 3.6056799921349003, 2.5879146908719712, 3.4053986769070455, 3.7037715879136917, 3.0519774286559405, 4.0138055628909886, 3.881816625079434, 3.0205731748312945, 3.8182633409705913, 3.8416721258101476, 3.3820984439238804, 2.0820764482735914, 3.9373359786219124, 3.937006517089693, 2.194531355824148, 3.0385756663610324, 2.344305737276619, 3.166040209378397, 3.6270413908916987, 3.752613166132096, 3.200315658960558, 3.9795597041947093, 3.7903434825447535, 3.3032701843389876, 2.9920278948612604, 2.7103501103978602, 3.1662686590304427, 3.476502308250007, 3.61670890198857, 3.0774641111138363, 3.7699310981119107, 3.8868313189610033, 3.4586559603359164, 3.9179261415065527, 3.71481826305277, 1.5705862086400881, 3.065912598893055, 3.875234697542119, 3.524374906230822, 3.8576833709113227, 4.237669567123192, 2.865036877539427, 2.722748435038549]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#average the performance results for a number of models with identical inputs\n",
    "def test_parameters(nof_runs, layers, train_input_features, test_input_features):\n",
    "    train_inputs = [list(pair) for pair in train_input_features]\n",
    "    test_inputs = [list(pair) for pair in test_input_features]\n",
    "    return average_results(nof_runs, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "def average_results(nof_runs, layers, train_inputs, text_inputs):\n",
    "    no_rounding = 0\n",
    "    rounding = 0\n",
    "    for _ in range(nof_runs):\n",
    "        #best performance analysis is analysis_1\n",
    "        pair = analysis_1(layers, train_inputs, text_inputs)\n",
    "        no_rounding+=pair[0]\n",
    "        rounding+=pair[1]\n",
    "    return float(no_rounding/nof_runs), float(rounding/nof_runs)\n",
    "\n",
    "\n",
    "#no scaling (best performance):\n",
    "def analysis_1(layers, train_inputs, test_inputs):\n",
    "    #build and train model\n",
    "    reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    reg.fit(train_inputs, train_users.user_to_target_rating)\n",
    "\n",
    "    #show importance of different inputs features to the model\n",
    "    results = permutation_importance(reg, train_inputs, train_users.user_to_target_rating)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "    #make predictions\n",
    "    predictions = reg.predict(test_inputs)\n",
    "\n",
    "    #test with and without roundings...\n",
    "    #in a sense this is logical sense becasue the actual ratings a user makes must be divisable by .5 \n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#scale inputs and targets:\n",
    "def analysis_2(layers, train_inputs, test_inputs):\n",
    "    #scale input features\n",
    "    train_inputs_scaled = StandardScaler().fit_transform(train_inputs)\n",
    "\n",
    "    #scale target values\n",
    "    target_scalar = StandardScaler()\n",
    "    true_rating_train_scaled = target_scalar.fit_transform(np.reshape(train_users.user_to_target_rating, (-1, 1)))\n",
    "    true_rating_train_scaled = np.reshape(true_rating_train_scaled, len(true_rating_train_scaled))\n",
    "\n",
    "    #build and train model\n",
    "    reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    reg.fit(train_inputs_scaled, true_rating_train_scaled)\n",
    "\n",
    "    #show importance of different inputs features...\n",
    "    results = permutation_importance(reg, train_inputs_scaled,true_rating_train_scaled)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "    #scale inputs features\n",
    "    test_inputs_scaled = StandardScaler().fit_transform(test_inputs)\n",
    "\n",
    "    #predict the scaled verison of ouptuts\n",
    "    scaled_predictions = reg.predict(test_inputs_scaled)\n",
    "\n",
    "    #get actual predictions from scaled predictions...\n",
    "    predictions = target_scalar.inverse_transform(scaled_predictions.reshape(-1, 1))\n",
    "    predictions = list(predictions.reshape(len(predictions)))\n",
    "\n",
    "    #test with and without roundings...\n",
    "    #in a sense this is logical sense becasue the actual ratings a user makes must be divisable by .5 \n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#only scale inputs:\n",
    "def analysis_3(layers, train_inputs, test_inputs):\n",
    "    #scale input features\n",
    "    train_inputs_scaled = StandardScaler().fit_transform(train_inputs)\n",
    "\n",
    "    #build and train model\n",
    "    reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    reg.fit(train_inputs_scaled, train_users.user_to_target_rating)\n",
    "\n",
    "    #show importance of different inputs features...\n",
    "    results = permutation_importance(reg, train_inputs_scaled, train_users.user_to_target_rating)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "    #scale inputs features\n",
    "    test_inputs_scaled = StandardScaler().fit_transform(test_inputs)\n",
    "\n",
    "    #predict the scaled verison of ouptuts\n",
    "    predictions = reg.predict(test_inputs_scaled)\n",
    "\n",
    "    #test with and without roundings...\n",
    "    #in a sense this is logical sense becasue the actual ratings a user makes must be divisable by .5 \n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "\n",
    "#the current test is the average of the r2 scores for 100 different models trained on the same input\n",
    "#the hidden layers are (10,10,10) and the best combinatio of inputs features(feature_2 and feature_3) are used\n",
    "print(test_parameters(100, (10,10,10), \n",
    "    zip(train_users.feature_1, train_users.feature_3),\n",
    "      zip(test_users.feature_1, test_users.feature_3)))\n",
    "\n",
    "\n",
    "#this shows the side by side comparision between all the features and the actual rating\n",
    "#each feature provides a reasonable guess of the target rating\n",
    "#the combination of the feature used above (feature_2 and feature_3) proves stronger than any feature alone and any other combination of features\n",
    "# print(test_users.feature_1)\n",
    "# print(test_users.feature_2)\n",
    "print(test_users.feature_3)\n",
    "# print(test_users.user_to_target_rating)\n",
    "\n",
    "#n = 15\n",
    "#feature_2 and feature_3:\n",
    "#r_2 scores:\n",
    "#(0.3551835602886015, 0.33621944330534004)\n",
    "\n",
    "#feature_1 and feature_3:\n",
    "#r_2 scores:\n",
    "#(0.3539859098451409, 0.3389716329978715)\n",
    "\n",
    "#feature_1:\n",
    "#r_2 scores:\n",
    "#(0.13997553160939374, 0.12017831016064674)\n",
    "\n",
    "#feature_2\n",
    "#r_2 scores:\n",
    "#(0.1464331784040238, 0.12889933523935157)\n",
    "\n",
    "#feature_3:\n",
    "#r_2 scores:\n",
    "#(0.31424317374455685, 0.2999151056317528)\n",
    "\n",
    "\n",
    "# n = 20\n",
    "# feature_2 and feature_3:\n",
    "# (0.3652700035771331, 0.3439539791916036)\n",
    "\n",
    "# feature_1 and feature_3:\n",
    "# (0.36616132482898556, 0.3435471003807131)\n",
    "# (0.36607210775528126, 0.34202322407954866)\n",
    "\n",
    "\n",
    "#n = 25\n",
    "# feature_1 and feature_3:\n",
    "#(0.35985921904576285, 0.3294790735702145)\n",
    "\n",
    "#n = 30\n",
    "# feature_1 and feature_3:\n",
    "# (0.35995387004644597, 0.340802587646878)\n",
    "\n",
    "#(0.3657076246215354, 0.3445834898046792)\n",
    "\n",
    "#ideas:\n",
    "#what if stopwords were included or the words have more or less filtering???\n",
    "#for content based reccomendation, may need to focus on words from certain sections to not overwelm the model with useless information!!!\n",
    "#k-means clusting can be effected by the random initialization...\n",
    "#this is a reason that there is substantial variation in perfromance\n",
    "#note: there are some predictions over 5 stars???\n",
    "#idea: what about only focusing on movies with a specific number of ratings???\n",
    "#what if there are more ratings per user???\n",
    "#what if boolena values were used for content baed items instead of counts???\n",
    "\n",
    "\n",
    "\n",
    "#what if there was a way to not limit content based filtering to movies a single user watched???\n",
    "\n",
    "#even even even more (done):\n",
    "#https://www.kaggle.com/code/cast42/simple-svd-movie-recommender\n",
    "\n",
    "#even even more svd:\n",
    "#https://analyticsindiamag.com/singular-value-decomposition-svd-application-recommender-system/\n",
    "\n",
    "#even more svd:\n",
    "#https://towardsdatascience.com/beginners-guide-to-creating-an-svd-recommender-system-1fd7326d1f65\n",
    "\n",
    "#more svd:\n",
    "#https://machinelearningmastery.com/using-singular-value-decomposition-to-build-a-recommender-system/\n",
    "\n",
    "#even even more content base recomendations:\n",
    "#https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system\n",
    "\n",
    "#even more content base recomendations:\n",
    "#https://towardsdatascience.com/the-4-recommendation-engines-that-can-predict-your-movie-tastes-109dc4e10c52\n",
    "\n",
    "#more content based reccomendation:\n",
    "#https://medium.com/web-mining-is688-spring-2021/content-based-movie-recommendation-system-72f122641eab#:~:text=Content%20Based%20Recommendation%20System%3A%20It,a%20show%20similar%20to%20it.\n",
    "\n",
    "#content based reccomendation:\n",
    "#https://medium.com/geekculture/creating-content-based-movie-recommender-with-python-7f7d1b739c63\n",
    "\n",
    "#methods used to guess ratings\n",
    "#https://web.eecs.umich.edu/~cscott/past_courses/eecs545f11/projects/AsendorfMcgaffinPressSchwartz.pdf\n",
    "\n",
    "#svd...\n",
    "#https://www.youtube.com/watch?v=8wLKuscyO9I&ab_channel=SundogEducationwithFrankKane\n",
    "\n",
    "#pca...\n",
    "#https://www.youtube.com/watch?v=fkf4IBRSeEc&ab_channel=SteveBrunton\n",
    "\n",
    "#eigenvalues...\n",
    "#https://www.youtube.com/watch?v=OA6CkChbe0Q&ab_channel=AndrewMisseldine\n",
    "\n",
    "#more pca:\n",
    "#https://www.youtube.com/watch?v=TJdH6rPA-TI&ab_channel=Computerphile\n",
    "\n",
    "#svd lecture:\n",
    "#https://www.youtube.com/watch?v=rFemvJgXY7E&ab_channel=Tech4Trends\n",
    "\n",
    "#svd with tf-idf\n",
    "#https://www.kaggle.com/code/parnianmalekian/svd-and-its-application-in-tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is for notes and models with scaling and no scaling...\n",
    "\n",
    "\n",
    "#analysis without feature scaling...\n",
    "#feature scaling:\n",
    "#https://analyticsindiamag.com/why-data-scaling-is-important-in-machine-learning-how-to-effectively-do-it/#:~:text=Scaling%20the%20target%20value%20is,learn%20and%20understand%20the%20problem.&text=Scaling%20of%20the%20data%20comes,algorithms%20in%20the%20data%20set.\n",
    "#https://towardsdatascience.com/collaborative-filtering-based-recommendation-systems-exemplified-ecbffe1c20b1\n",
    "\n",
    "\n",
    "# with scaling start:\n",
    "\n",
    "# def test_parameters(layers, train_input_features, test_input_features):\n",
    "#     train_inputs = [list(pair) for pair in train_input_features]\n",
    "#     test_inputs = [list(pair) for pair in test_input_features]\n",
    "\n",
    "#     return average_results(20, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "# def average_results(nof_runs, layers, train_inputs, text_inputs):\n",
    "#     no_rounding = 0\n",
    "#     rounding = 0\n",
    "#     for _ in range(nof_runs):\n",
    "#         #issue here\n",
    "#         pair = analysis(layers, train_inputs, text_inputs)\n",
    "#         no_rounding+=pair[0]\n",
    "#         rounding+=pair[1]\n",
    "#     return float(no_rounding/nof_runs), float(rounding/nof_runs)\n",
    "\n",
    "\n",
    "# def analysis(layers, train_inputs, test_inputs):\n",
    "#     #scale input features\n",
    "#     #scalar 1:\n",
    "#     input_scalar = StandardScaler()\n",
    "#     train_inputs_scaled = input_scalar.fit_transform(train_inputs)\n",
    "\n",
    "#     #scale target values\n",
    "#     #scalar 2:\n",
    "#     target_scalar = StandardScaler()\n",
    "#     true_rating_train_scaled = target_scalar.fit_transform(np.array(train_users.user_to_target_rating).reshape(-1, 1))\n",
    "#     true_rating_train_scaled = np.reshape(true_rating_train_scaled, len(true_rating_train_scaled))\n",
    "\n",
    "#     #build and train model\n",
    "#     reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "#     reg.fit(train_inputs_scaled, true_rating_train_scaled)\n",
    "\n",
    "#     #show importance of different inputs features...\n",
    "#     results = permutation_importance(reg, train_inputs_scaled,true_rating_train_scaled)\n",
    "#     print(results[\"importances_mean\"])\n",
    "\n",
    "#     #scale inputs features\n",
    "#     #scalar 3: \n",
    "#     input_scalar = StandardScaler()\n",
    "#     test_inputs_scaled = input_scalar.fit_transform(test_inputs)\n",
    "\n",
    "#     #predict the scaled verison of ouptuts\n",
    "#     scaled_predictions = reg.predict(test_inputs_scaled)\n",
    "\n",
    "#     #change back into regular outputs to be tested with target scalar above...\n",
    "#     #not sure if this is correct???\n",
    "#     #would it be better to use a new scalar object???\n",
    "#     #need to know the mean and the std dev of the orginal predictions\n",
    "#     #scalar 4:\n",
    "#     predictions = target_scalar.inverse_transform(np.array(scaled_predictions).reshape(-1, 1))\n",
    "\n",
    "#     #test with and without roundings...\n",
    "#     rounded_predictions = []\n",
    "#     for item in predictions:\n",
    "#         rounded_predictions.append(float(round(item[0]*2)/2.0))\n",
    "\n",
    "#     #evaluation metric 1\n",
    "#     return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "#         r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#     #evaluation metric 2\n",
    "#     # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "#     #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "# print(test_parameters((10,10,10), \n",
    "#       zip(train_users.feature_1, train_users.feature_3),\n",
    "#       zip(test_users.feature_1, test_users.feature_3)))\n",
    "\n",
    "\n",
    "# print(test_users.feature_1)\n",
    "# print(test_users.feature_3)\n",
    "# print(test_users.user_to_target_rating)\n",
    "\n",
    "# ...with scaling done\n",
    "\n",
    "\n",
    "#without scaling:\n",
    "\n",
    "# def test_parameters(layers, train_input_features, test_input_features):\n",
    "#     train_inputs = [list(pair) for pair in train_input_features]\n",
    "#     test_inputs = [list(pair) for pair in test_input_features]\n",
    "\n",
    "#     return average_results(20, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "\n",
    "# def average_results(nof_runs, layers, train_inputs, text_inputs):\n",
    "#     no_rounding = 0\n",
    "#     rounding = 0\n",
    "#     for _ in range(nof_runs):\n",
    "#         #issue here\n",
    "#         pair = analysis(layers, train_inputs, text_inputs)\n",
    "#         no_rounding+=pair[0]\n",
    "#         rounding+=pair[1]\n",
    "#     return float(no_rounding/nof_runs), float(rounding/nof_runs)\n",
    "\n",
    "\n",
    "# def analysis(layers, train_inputs, test_inputs):\n",
    "#     #build and train model\n",
    "#     reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "#     reg.fit(train_inputs, train_users.user_to_target_rating)\n",
    "\n",
    "#     #show importance of different inputs features...\n",
    "#     results = permutation_importance(reg, train_inputs, train_users.user_to_target_rating)\n",
    "#     print(results[\"importances_mean\"])\n",
    "\n",
    "\n",
    "#     predictions = reg.predict(test_inputs)\n",
    "\n",
    "#     #testing if needed...\n",
    "#     # predictions = np.array(predictions).reshape(-1, 1)\n",
    "\n",
    "#     #test with and without roundings...\n",
    "#     rounded_predictions = []\n",
    "#     for item in predictions:\n",
    "#         # rounded_predictions.append(float(round(item[0]*2)/2.0))\n",
    "#         rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "#     #evaluation metric 1:\n",
    "#     return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "#         r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#     #evaluation metric 2:\n",
    "#     # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "#     #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "# print(test_parameters((10,10,10), \n",
    "#       zip(train_users.feature_1, train_users.feature_3),\n",
    "#       zip(test_users.feature_1, test_users.feature_3)))\n",
    "\n",
    "# print(test_users.feature_1)\n",
    "# print(test_users.feature_3)\n",
    "# print(test_users.user_to_target_rating)\n",
    "\n",
    "#done with scaling...\n",
    "\n",
    "\n",
    "#now for the user comparison option (need user to list of movie ratings)\n",
    "#fill in ratings that the user hasn't watched with the method above\n",
    "#then cluster the users by their ratings\n",
    "\n",
    "#note: agglomerative clustering might make more sense here since k-means has random init for centroids...\n",
    "#note: to guess a new users rating requires that none of that users ratings have been used to train the model\n",
    "#The data needs to be split into test and train before modeling the algorithm on the train data\n",
    "\n",
    "#Training process:\n",
    "#split data into test and train data\n",
    "#proceed with train data...\n",
    "#cluster movies by the tokens with range for k\n",
    "#cluster users by the ratings with range for k and (fill in ratings for movies a users hasn't watched with some guess)\n",
    "#guess: this can be obtained by clustering the movies that the user has watched...\n",
    "#for each movie the user hasn't watched find the cluster that it belongs to with the highest possible k value\n",
    "#that the user has at least one movie belonging to one of the clusters and then take the average of those movies\n",
    "#this is exactly like a later training step excpet it is applied to all the movies the user watched\n",
    "\n",
    "#for a single randomly chosen movie from each user in the trainging data...\n",
    "\n",
    "#find the cluster the movie belongs to \n",
    "#find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#take the average score of these movies\n",
    "#find the cluster the user belongs to\n",
    "#find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#train an mlp model with both averages and perhaps some extra statistics as features...\n",
    "#using the given movie ratings as actuals\n",
    "\n",
    "\n",
    "#The process of predicting a rating:\n",
    "#1. find the cluster the movie belongs to \n",
    "#2. find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#3. take the average score of these movies\n",
    "#4. find the cluster the user belongs to\n",
    "#5. find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#6. input into the trained mlp model both averages and perhaps some extra statistics\n",
    "#7. make predictions and test against the randomly chosen movies actual ratings\n",
    "\n",
    "\n",
    "#summary:\n",
    "#find cluster for movie -> find movies part of the same clusters that the users rated -> average\n",
    "#question: are the clusters unique to the movies the user has watched or to all movies???\n",
    "#what is the technical difference???\n",
    "#is this the same as finding the most simimlair movie the user rated and copying the rating???\n",
    "\n",
    "#find cluster for user -> find the ratings for the movie by people in the same cluster -> average\n",
    "\n",
    "#other avenues considered:\n",
    "#idea 1:\n",
    "#for the first process, instead of averaging the movies that only the user rated, find other users that are...\n",
    "#like the user in question and find the average for that movie cluster\n",
    "#Problem: it is better to get the users raw opionion rather than generalizing it to some like minded users\n",
    "#there is an extra costly step to this\n",
    "#idea 2: \n",
    "#for the second process, instead of finding the average rating for the movie in the same cluster of users...\n",
    "#also find the average rating of movies that are like the movie in question \n",
    "#Problem, it is better to get the movies rating itself as it would be the most accurate indicator\n",
    "#there is an extra costly step to this\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test with overall averges:\n",
    "#note: users have between 50 and 75 ratings each\n",
    "#note: there are 250 users who are taken into account \n",
    "#note: seed int is one for above cells (cells part of creating the csv file)\n",
    "\n",
    "#with seed int == 3, 4 taking overall averages: 0.09565753948597455\n",
    "#with seed_int == 1, 2 taking overall averages: 0.070404868516315\n",
    "#with seed_int == 2, 5 taking overall averages: 0.11310085954932936\n",
    "#with seed_int == 4, 6 taking overall averages: 0.07125374341347135\n",
    "#with seed_int == 5, 4 taking overall averages: 0.17736444913943628\n",
    "#compute time: 11 minutes\n",
    "\n",
    "\n",
    "#test with users related movies:\n",
    "#is there a magic proportion of movies to average???\n",
    "#note: this is taking around the same time as the above tests meaning \n",
    "#there could be more users to include in analysis with little increase in runtime\n",
    "#k fold cross validation could be effective\n",
    "#https://www.youtube.com/watch?v=TIgfjmp-4BA&ab_channel=Udacity\n",
    "\n",
    "#effect of choosing a random seed...\n",
    "#https://towardsdatascience.com/how-to-use-random-seeds-effectively-54a4cd855a79\n",
    "\n",
    "#try tinkering with the number of similair movies to average\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
