{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    length = len(array[8])\n",
    "    if(array[8][length-4:]==\"<NA>\"):\n",
    "        return False\n",
    "    length = len(array[9])\n",
    "    if(array[9][length-4:]==\"<NA>\"):\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#convert the dataframe into an array and build a dictionary\n",
    "user_to_data = dict()\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "\n",
    "#get all unique user ids\n",
    "list_of_user_ids = []\n",
    "last_id  = -1\n",
    "for item in complete_array:\n",
    "    if(item[0]!= last_id):\n",
    "        list_of_user_ids.append(item[0])\n",
    "        last_id = item[0]\n",
    "\n",
    "\n",
    "index  = 0\n",
    "#this has been tested with 5000, 10000, 20000, 100000\n",
    "nof_users = 20000\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, nof_users):\n",
    "    user_to_data[list_of_user_ids[i]] = []\n",
    "    for j in range(index, len(complete_array)):\n",
    "        if complete_array[j][0] == list_of_user_ids[i]:\n",
    "            #condition is checked for complete_array[j]\n",
    "            if(condition(complete_array[j])):\n",
    "                #this is where data is tranformed\n",
    "                transformed = provide_data(complete_array[j])\n",
    "                user_to_data[list_of_user_ids[i]].append(transformed)         \n",
    "        else:\n",
    "            #ignore if the number of ratings for a user is too small\n",
    "            if (len(user_to_data[list_of_user_ids[i]])<10):\n",
    "                del user_to_data[list_of_user_ids[i]]\n",
    "            index = j+1\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for key in user_to_data.keys():\n",
    "        writer.writerows(user_to_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie id to list of its ratings by all users\n",
    "movie_to_ratings = dict()\n",
    "\n",
    "#user id to the ratings of movies by the user\n",
    "#actually includes whole data row from constucted data...\n",
    "user_to_ratings = dict()\n",
    "\n",
    "#The list created by the constructed data csv is in order by user id\n",
    "#This code populates movie_to_ratings and user_to_ratings\n",
    "user_id = -1\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_ratings[row[0]] = [row]\n",
    "    else:\n",
    "        user_to_ratings[row[0]].append(row)\n",
    "\n",
    "    if(row[1] in movie_to_ratings.keys()):\n",
    "        movie_to_ratings[row[1]].append(row[2])\n",
    "    else:\n",
    "        movie_to_ratings[row[1]] = [row[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#dictionary of user id to a list of strings of combined textual features for each movie rated by the user\n",
    "#the strings do not include ratings or movie id\n",
    "user_to_corpus_list = dict()\n",
    "\n",
    "\n",
    "# WordNetLemmatizer().lemmatize(token.lower())\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "for key in user_to_ratings.keys():\n",
    "    movie_strings = []\n",
    "    for movie_data in user_to_ratings[key]:\n",
    "        movie_string = \"\"\n",
    "        #avoid the first three data points (user id, movieid, and rating)\n",
    "        #use only the text data\n",
    "        for index in range (3,len(movie_data)):\n",
    "            if(index!= len(movie_data)-1):\n",
    "                movie_string+= movie_data[index]+\" \"\n",
    "            else:\n",
    "                movie_string+= movie_data[index]\n",
    "        cleaned = remove_stopwords(movie_string)\n",
    "        cleaned = \" \".join([wnl.lemmatize(word) for word in cleaned.split(\" \")])\n",
    "        movie_strings.append(cleaned)\n",
    "    user_to_corpus_list[key] = movie_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:97: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:100: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:97: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:100: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:97: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:100: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:97: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:100: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:97: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:100: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:97: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_skew = skew(ratings)\n",
      "C:\\Users\\jackson\\AppData\\Local\\Temp\\ipykernel_16892\\926763809.py:100: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  user_rating_kurt = kurtosis(ratings)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#note user_to_corpus_list is a dictionary of users to list of strings with all the text data for the coresponding movies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "#convert text data into a list of word occurances\n",
    "#seperate test and train data\n",
    "\n",
    "user_to_word_count = dict()\n",
    "\n",
    "\n",
    "\n",
    "#this is a list of all the words for each movie for each user\n",
    "#note: keys are in insertion order for python 3.7+\n",
    "\n",
    "list_of_words = []\n",
    "\n",
    "for key in user_to_corpus_list:\n",
    "    for item in user_to_corpus_list[key]:\n",
    "        list_of_words.append(item)\n",
    "\n",
    "\n",
    "#this logic is used for training and simalir logic is used to fill in the\n",
    "#missing ratings for movies that the user hasn't seen\n",
    "#is there a more accuracte method besides using a single rating from the user\n",
    "#yes but this is a step towrds the more accurate user comaprison model\n",
    "\n",
    "for key in user_to_corpus_list.keys():\n",
    "    #make sure a copy happens here...\n",
    "    cv = CountVectorizer().fit_transform(user_to_corpus_list[key])\n",
    "    rand_index = random.randint(0, len(cv)-1)\n",
    "    rand_test_item = cv[rand_index]\n",
    "    del cv[rand_index]\n",
    "    cosine_sim = cosine_similarity(X = cv ,Y = [rand_test_item])\n",
    "    max = 0\n",
    "    pred_rating = 0\n",
    "    #find the rating of the most similair movie\n",
    "    for sim, rat in zip(cosine_sim, ratings):\n",
    "        if sim>max:\n",
    "            max = sim\n",
    "            pred_rating = rat\n",
    "\n",
    "\n",
    "#now for the user comparison logic (need user to list of movie ratings)\n",
    "#need an order for the movie ratings...\n",
    "#fill in ratings that the user hasn't watched with the method above\n",
    "#then cluster the users by their ratings\n",
    "\n",
    "\n",
    "\n",
    "#there is a dictionary of usernames to list of rating placeholders for movie ratings for all movies initially set to -1\n",
    "#go through the initial data list and keep a dictionary of movie names to an index which is the place\n",
    "#of the movie in coresponding user dictionary value list\n",
    "#while doing this change the rating from -1 into the coresponding place in the dictionary of usernames to list of rating placeholders\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get average rating for a single movie amoung all users who rated it\n",
    "def get_avg_movie_rating(movie_id):\n",
    "    ret =0 \n",
    "    cnt = 0\n",
    "    for item in movie_to_ratings[movie_id]:\n",
    "        ret+= float(item)\n",
    "        cnt+=1\n",
    "    return float(ret/cnt)\n",
    "\n",
    "\n",
    "#get all the movie ratings from a single user\n",
    "def get_user_ratings(user_id):\n",
    "    ret = []\n",
    "    for item in user_to_ratings[user_id]:\n",
    "        ret.append(float(item[2]))\n",
    "    return ret\n",
    "\n",
    "\n",
    "#user to model independent var X\n",
    "user_to_features = dict()\n",
    "#user to model dependent var y\n",
    "user_to_rand_rating = dict()\n",
    "\n",
    "#note: agglomerative clustering might make more sense here since k-means has random init for centroids...\n",
    "#note: to guess a new users rating requires that none of that users ratings have been used to train the model\n",
    "#The data needs to be split into test and train before modeling the algorithm on the train data\n",
    "\n",
    "#Training process:\n",
    "#split data into test and train data\n",
    "#proceed with train data...\n",
    "#cluster movies by the tokens with range for k\n",
    "#cluster users by the ratings with range for k and (fill in ratings for movies a users hasn't watched with some guess)\n",
    "#guess: this can be obtained by clustering the movies that the user has watched...\n",
    "#for each movie the user hasn't watched find the cluster that it belongs to with the highest possible k value\n",
    "#that the user has at least one movie belonging to one of the clusters and then take the average of those movies\n",
    "#this is exactly like a later training step excpet it is applied to all the movies the user watched\n",
    "\n",
    "#for a single randomly chosen movie from each user in the trainging data...\n",
    "\n",
    "#find the cluster the movie belongs to \n",
    "#find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#take the average score of these movies\n",
    "#find the cluster the user belongs to\n",
    "#find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#train an mlp model with both averages and perhaps some extra statistics as features...\n",
    "#using the given movie ratings as actuals\n",
    "\n",
    "\n",
    "#The process of predicting a rating:\n",
    "#1. find the cluster the movie belongs to \n",
    "#2. find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#3. take the average score of these movies\n",
    "#4. find the cluster the user belongs to\n",
    "#5. find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#6. input into the trained mlp model both averages and perhaps some extra statistics\n",
    "#7. make predictions and test against the randomly chosen movies actual ratings\n",
    "\n",
    "\n",
    "#summary:\n",
    "#find cluster for movie -> find movies part of the same clusters that the users rated -> average\n",
    "#question: are the clusters unique to the movies the user has watched or to all movies???\n",
    "#what is the technical difference???\n",
    "#is this the same as finding the most simimlair movie the user rated and copying the rating???\n",
    "\n",
    "#find cluster for user -> find the ratings for the movie by people in the same cluster -> average\n",
    "\n",
    "\n",
    "\n",
    "#other avenues considered:\n",
    "#idea 1:\n",
    "#for the first process, instead of averaging the movies that only the user rated, find other users that are...\n",
    "#like the user in question and find the average for that movie cluster\n",
    "#Problem: it is better to get the users raw opionion rather than generalizing it to some like minded users\n",
    "#there is an extra costly step to this\n",
    "#idea 2: \n",
    "#for the second process, instead of finding the average rating for the movie in the same cluster of users...\n",
    "#also find the average rating of movies that are like the movie in question \n",
    "#Problem, it is better to get the movies rating itself as it would be the most accurate indicator\n",
    "#there is an extra costly step to this\n",
    "\n",
    "\n",
    "\n",
    "#populate user_to_features and user_to_rand_rating\n",
    "for key in user_to_corpus_list.keys():\n",
    "\n",
    "    count_matrix = CountVectorizer().fit_transform(user_to_corpus_list[key]).toarray().tolist()\n",
    "    rand_index = random.randint(0, len(count_matrix)-1)\n",
    "    rand_test_item = count_matrix[rand_index]\n",
    "    del count_matrix[rand_index]\n",
    "\n",
    "    #find similarity by the count of each word between the random selected movie and the other movies rated by the user\n",
    "    cosine_sim = cosine_similarity(X = count_matrix ,Y = [rand_test_item])\n",
    "\n",
    "    #technically this should not include the current users rating for the randomly selected movie...\n",
    "    #that is what we want to find out...\n",
    "    ratings = copy.deepcopy(get_user_ratings(key))\n",
    "    similairities = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "    random_rating = ratings[rand_index]\n",
    "    user_to_rand_rating[key] = random_rating\n",
    "    del ratings[rand_index]\n",
    "\n",
    "    #technically this should not include the current users rating for the randomly selected movie...\n",
    "    #that is what we want to find out...\n",
    "    movie_rating_avg = get_avg_movie_rating(user_to_ratings[key][rand_index][1])\n",
    "\n",
    "    user_rating_avg =  float(np.sum(ratings)/(len(ratings)))\n",
    "    user_rating_skew = skew(ratings)\n",
    "    if(math.isnan(user_rating_skew)):\n",
    "        user_rating_skew = 0\n",
    "    user_rating_kurt = kurtosis(ratings)\n",
    "    if(math.isnan(user_rating_kurt)):\n",
    "        user_rating_kurt = 0\n",
    "    user_rating_var = statistics.variance(ratings)\n",
    "\n",
    "\n",
    "    sim_average = float(np.sum(similairities)/(len(similairities)))\n",
    "    sim_skew = skew(similairities) \n",
    "    if(math.isnan(sim_skew)):\n",
    "        sim_skew = 0\n",
    "    sim_kurt = kurtosis(similairities)\n",
    "    if(math.isnan(sim_kurt)):\n",
    "        sim_kurt = 0\n",
    "    sim_var = statistics.variance(similairities)\n",
    "\n",
    "\n",
    "    # there are many curve defining features used here that may be impotent and can be cut or kept in the next cell...\n",
    "    # there may stil be other distribution measures that improve the model...\n",
    "    # might try inputing some function of sim and rating rather than incluing them on their own\n",
    "\n",
    "\n",
    "    for sim, rating in zip(similairities, ratings):\n",
    "        if key not in user_to_features:\n",
    "            # user_to_features[key] = [[(rating - user_rating_avg)*sim, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var]]\n",
    "            user_to_features[key] = [[rating, sim, user_rating_avg, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var]]\n",
    "        else:\n",
    "            # user_to_features[key].append([(rating - user_rating_avg)*sim, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var])\n",
    "            user_to_features[key].append([rating, sim, user_rating_avg, movie_rating_avg, user_rating_skew, user_rating_kurt, user_rating_var, sim_average, sim_skew, sim_kurt, sim_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.50910138e-05  1.47934257e-07  2.58227704e-01  2.98268535e-01\n",
      "  2.47937052e-03  1.85234981e-03  4.94337078e-03  5.42048696e-03\n",
      "  8.14588308e-03  1.08693110e-02  1.46282495e-03]\n",
      "19\n",
      "Pred: 3.3420909617280894 Actual: 3.0\n",
      "Pred: 3.21471216837001 Actual: 1.0\n",
      "Pred: 3.9162525796936274 Actual: 3.0\n",
      "Pred: 4.150428622025074 Actual: 4.0\n",
      "Pred: 2.9536225623200463 Actual: 3.0\n",
      "Pred: 4.508382091092446 Actual: 4.0\n",
      "Pred: 3.5021362700246383 Actual: 1.5\n",
      "Pred: 4.544995408783995 Actual: 4.5\n",
      "Pred: 3.0144937107000183 Actual: 4.0\n",
      "Pred: 3.499556689402351 Actual: 4.0\n",
      "Pred: 3.9248830282144156 Actual: 3.0\n",
      "Pred: 2.9073696102993547 Actual: 1.0\n",
      "Pred: 4.26207807410446 Actual: 5.0\n",
      "Pred: 3.910856828826111 Actual: 3.0\n",
      "Pred: 3.4744587208277657 Actual: 3.0\n",
      "Pred: 3.233938480643781 Actual: 3.0\n",
      "Pred: 3.6413446168055024 Actual: 4.0\n",
      "Pred: 3.9905922784832084 Actual: 5.0\n",
      "Pred: 3.6745961653232224 Actual: 4.0\n",
      "Pred: 3.603680109800211 Actual: 3.0\n",
      "Pred: 3.1907465665030346 Actual: 3.0\n",
      "Pred: 3.8732037492567355 Actual: 3.0\n",
      "Pred: 4.0395584028160565 Actual: 4.5\n",
      "Pred: 2.7998326285577075 Actual: 2.5\n",
      "Pred: 4.398115379518874 Actual: 4.5\n",
      "Pred: 4.288890023227799 Actual: 1.0\n",
      "Pred: 4.336776147714853 Actual: 4.0\n",
      "Pred: 4.191935788350756 Actual: 4.0\n",
      "Pred: 3.6524880036648066 Actual: 5.0\n",
      "Pred: 4.275680568195534 Actual: 4.5\n",
      "Pred: 3.1647035326317745 Actual: 1.0\n",
      "Pred: 3.6333852993713487 Actual: 4.5\n",
      "Pred: 3.971591652553847 Actual: 5.0\n",
      "Pred: 2.9638149164783436 Actual: 3.0\n",
      "Pred: 4.546770782965027 Actual: 5.0\n",
      "Pred: 3.5446425721685637 Actual: 3.0\n",
      "Pred: 2.4984432994641463 Actual: 0.5\n",
      "Pred: 3.025270172957911 Actual: 4.0\n",
      "Pred: 3.3946887932300824 Actual: 5.0\n",
      "Pred: 3.419136282531315 Actual: 3.0\n",
      "Pred: 3.8282840000985057 Actual: 4.0\n",
      "Pred: 4.313987713718672 Actual: 4.0\n",
      "Pred: 3.001032417478275 Actual: 1.0\n",
      "Pred: 2.7844844981319286 Actual: 3.5\n",
      "Pred: 3.4550709824107235 Actual: 1.5\n",
      "Pred: 3.930446258771493 Actual: 4.5\n",
      "Pred: 3.8047438242414646 Actual: 2.0\n",
      "Pred: 2.8741136581495463 Actual: 2.0\n",
      "Pred: 4.242662235474252 Actual: 4.0\n",
      "Pred: 3.5927470531345103 Actual: 4.0\n",
      "Pred: 3.6921951683739547 Actual: 3.0\n",
      "Pred: 2.6497910024275106 Actual: 3.0\n",
      "Pred: 4.502693857105149 Actual: 4.0\n",
      "Pred: 3.8040076588206784 Actual: 3.0\n",
      "Pred: 4.400160292736489 Actual: 4.5\n",
      "Pred: 4.414673827430834 Actual: 5.0\n",
      "Pred: 4.5065539572488715 Actual: 5.0\n",
      "Pred: 3.5831619326497965 Actual: 4.0\n",
      "Pred: 4.112138446135167 Actual: 4.0\n",
      "Pred: 3.8701673777925345 Actual: 4.5\n",
      "Pred: 3.4096534418010185 Actual: 4.0\n",
      "Pred: 3.5064106727788644 Actual: 3.0\n",
      "Pred: 2.677432148195122 Actual: 3.0\n",
      "Pred: 3.5758712393704015 Actual: 1.0\n",
      "Pred: 3.6267489128631993 Actual: 1.0\n",
      "Pred: 3.4331701297074475 Actual: 2.0\n",
      "Pred: 3.9075092886796896 Actual: 5.0\n",
      "Pred: 4.0996332800896464 Actual: 3.0\n",
      "Pred: 3.7246179736337126 Actual: 4.0\n",
      "Pred: 3.787750129969332 Actual: 4.0\n",
      "Pred: 3.951630733219499 Actual: 4.5\n",
      "Pred: 4.154108358816459 Actual: 4.5\n",
      "Pred: 3.7010516622382474 Actual: 5.0\n",
      "Pred: 2.6104449826682203 Actual: 2.0\n",
      "Pred: 4.441012958530906 Actual: 4.0\n",
      "Pred: 3.191202871545644 Actual: 4.0\n",
      "Pred: 4.439538288836402 Actual: 4.5\n",
      "Pred: 3.612256560017144 Actual: 3.0\n",
      "Pred: 3.0933540334154834 Actual: 3.0\n",
      "Pred: 4.253087286383587 Actual: 5.0\n",
      "Pred: 3.2420044865688182 Actual: 2.0\n",
      "Pred: 4.440094973234761 Actual: 4.0\n",
      "Pred: 3.3570199107674976 Actual: 4.0\n",
      "Pred: 4.335367754818647 Actual: 4.5\n",
      "Pred: 3.7286234813142634 Actual: 2.0\n",
      "Pred: 4.344373830656039 Actual: 3.0\n",
      "Pred: 3.716206128532423 Actual: 3.0\n",
      "Pred: 2.395219116515392 Actual: 2.5\n",
      "Pred: 3.437503587653837 Actual: 4.0\n",
      "Pred: 4.401186186924267 Actual: 4.0\n",
      "Pred: 4.2494650872073265 Actual: 3.0\n",
      "Pred: 4.502656347812976 Actual: 4.0\n",
      "Pred: 2.4971730463730366 Actual: 4.0\n",
      "Pred: 4.020499676158483 Actual: 3.0\n",
      "Pred: 4.422456515072729 Actual: 5.0\n",
      "Pred: 4.261808814152567 Actual: 4.5\n",
      "Pred: 3.846334446101737 Actual: 3.0\n",
      "Pred: 4.021298066749469 Actual: 4.5\n",
      "Pred: 4.3911672122053576 Actual: 5.0\n",
      "Pred: 4.39325077188268 Actual: 4.0\n",
      "Pred: 2.7243544314925656 Actual: 3.0\n",
      "Pred: 3.428696199710566 Actual: 3.0\n",
      "Pred: 4.006952583804476 Actual: 5.0\n",
      "Pred: 3.4903986580219213 Actual: 4.0\n",
      "Pred: 4.232256600633179 Actual: 5.0\n",
      "Pred: 4.363667489598064 Actual: 5.0\n",
      "Pred: 3.3551197570909315 Actual: 4.0\n",
      "Pred: 2.5207995193992234 Actual: 1.0\n",
      "Pred: 4.058895404594819 Actual: 5.0\n",
      "Pred: 4.626023026951611 Actual: 5.0\n",
      "Pred: 3.3141286324432944 Actual: 4.0\n",
      "Pred: 4.27850735532294 Actual: 4.0\n",
      "Pred: 3.6461938434432457 Actual: 5.0\n",
      "Pred: 4.07034672101027 Actual: 4.0\n",
      "Pred: 3.28899485902658 Actual: 4.0\n",
      "Pred: 4.194483137356627 Actual: 4.0\n",
      "Pred: 4.472308707616277 Actual: 4.0\n",
      "Pred: 3.869041688857613 Actual: 5.0\n",
      "Pred: 3.536435414327799 Actual: 3.0\n",
      "Pred: 3.6485575593745856 Actual: 3.5\n",
      "Pred: 3.0542179366295996 Actual: 1.0\n",
      "Pred: 2.8222692042269064 Actual: 4.5\n",
      "Pred: 3.659523590322942 Actual: 3.0\n",
      "Pred: 4.419993082236355 Actual: 2.0\n",
      "Pred: 4.1417339468719305 Actual: 4.0\n",
      "Pred: 4.1555953715741305 Actual: 4.0\n",
      "Pred: 4.431800149566825 Actual: 5.0\n",
      "Pred: 3.549909175034149 Actual: 4.0\n",
      "Pred: 4.017163589888978 Actual: 2.0\n",
      "Pred: 2.8997264258966102 Actual: 3.0\n",
      "Pred: 4.5463993611168645 Actual: 4.0\n",
      "Pred: 3.4378877319923293 Actual: 3.0\n",
      "Pred: 2.4841233674462084 Actual: 3.0\n",
      "Pred: 3.8849858530414627 Actual: 5.0\n",
      "Pred: 4.181901180544591 Actual: 4.5\n",
      "Pred: 4.433358846979176 Actual: 5.0\n",
      "Pred: 4.219429486641437 Actual: 3.0\n",
      "Pred: 2.9386538456790596 Actual: 3.0\n",
      "Pred: 3.2212708195735655 Actual: 3.5\n",
      "Pred: 3.8130505427409087 Actual: 3.5\n",
      "Pred: 2.639711807151279 Actual: 3.5\n",
      "Pred: 3.25289531711756 Actual: 3.0\n",
      "Pred: 4.177855601290407 Actual: 4.5\n",
      "Pred: 3.5366316885296443 Actual: 3.0\n",
      "Pred: 2.76566656182922 Actual: 1.0\n",
      "Pred: 4.074181001143812 Actual: 5.0\n",
      "Pred: 3.509568877681742 Actual: 4.0\n",
      "Pred: 3.813839024328258 Actual: 2.0\n",
      "Pred: 3.9577959572970207 Actual: 4.5\n",
      "Pred: 4.366662844910177 Actual: 5.0\n",
      "Pred: 4.266601902515059 Actual: 4.5\n",
      "Pred: 4.219756798791795 Actual: 5.0\n",
      "Pred: 3.1025265439408476 Actual: 3.0\n",
      "Pred: 3.567721153041501 Actual: 4.0\n",
      "Pred: 3.1324199421228567 Actual: 4.0\n",
      "Pred: 4.073077428608058 Actual: 4.0\n",
      "Pred: 3.376922470330624 Actual: 3.0\n",
      "Pred: 3.128053677833432 Actual: 3.0\n",
      "Pred: 3.755372716811771 Actual: 3.0\n",
      "Pred: 3.17501412862882 Actual: 3.0\n",
      "Pred: 3.501726839298928 Actual: 4.0\n",
      "Pred: 3.9599430154421054 Actual: 3.0\n",
      "Pred: 3.777475362014941 Actual: 5.0\n",
      "Pred: 3.022273411789667 Actual: 3.0\n",
      "Pred: 3.661868085125156 Actual: 4.5\n",
      "Pred: 4.479630921698172 Actual: 4.5\n",
      "Pred: 3.1276695501346827 Actual: 3.0\n",
      "Pred: 3.9438453268976223 Actual: 4.5\n",
      "Pred: 3.7696880173100626 Actual: 4.0\n",
      "Pred: 3.496148772624619 Actual: 4.0\n",
      "Pred: 3.7759485587640165 Actual: 2.5\n",
      "Pred: 3.5421184631610845 Actual: 5.0\n",
      "Pred: 3.9530912555937494 Actual: 3.0\n",
      "Pred: 3.057726489651515 Actual: 1.0\n",
      "Pred: 4.52269015650579 Actual: 4.5\n",
      "Pred: 3.3223722147710077 Actual: 4.0\n",
      "Pred: 3.1784902758554012 Actual: 3.0\n",
      "Pred: 2.9859026026958135 Actual: 2.0\n",
      "Pred: 4.455910876094381 Actual: 5.0\n",
      "Pred: 3.2194594886545054 Actual: 2.0\n",
      "Pred: 3.5674429365848797 Actual: 5.0\n",
      "Pred: 3.3288239464289227 Actual: 2.5\n",
      "Pred: 3.175312246641771 Actual: 3.0\n",
      "Pred: 4.030465036932264 Actual: 4.0\n",
      "Pred: 3.5292266645528274 Actual: 3.5\n",
      "Pred: 0.997312187895996 Actual: 0.5\n",
      "Pred: 3.405261575626356 Actual: 2.5\n",
      "Pred: 3.794468878626338 Actual: 2.0\n",
      "Pred: 4.2037909608055255 Actual: 5.0\n",
      "Pred: 2.8571088618876446 Actual: 1.0\n",
      "Pred: 4.54792776732691 Actual: 5.0\n",
      "Pred: 3.4752536139063257 Actual: 4.0\n",
      "Pred: 3.918361920002034 Actual: 3.5\n",
      "Pred: 3.7055113634298658 Actual: 4.0\n",
      "Pred: 4.423048307475809 Actual: 4.5\n",
      "Pred: 3.6330825177681954 Actual: 3.0\n",
      "Pred: 3.6136402123368097 Actual: 4.0\n",
      "Pred: 3.3140965166599354 Actual: 3.0\n",
      "Pred: 3.0413451545381265 Actual: 3.0\n",
      "Pred: 4.373789228901127 Actual: 4.5\n",
      "Pred: 3.7638260021175114 Actual: 4.0\n",
      "Pred: 3.5004053474765926 Actual: 4.0\n",
      "Pred: 3.9211981640284077 Actual: 4.5\n",
      "Pred: 3.848455018402661 Actual: 5.0\n",
      "Pred: 3.2810196947372816 Actual: 5.0\n",
      "Pred: 4.061065557644254 Actual: 5.0\n",
      "Pred: 3.889115424739462 Actual: 3.5\n",
      "Pred: 3.8426786271771767 Actual: 3.5\n",
      "Pred: 4.45308742462365 Actual: 4.0\n",
      "Pred: 3.5935214801233166 Actual: 3.5\n",
      "Pred: 3.8797104545067684 Actual: 0.5\n",
      "Pred: 2.404429542641767 Actual: 3.0\n",
      "Pred: 4.398494409199958 Actual: 4.0\n",
      "Pred: 3.869455160508112 Actual: 5.0\n",
      "Pred: 4.3380277214721215 Actual: 5.0\n",
      "Pred: 4.0526441347498565 Actual: 3.0\n",
      "Pred: 4.198319925424629 Actual: 2.0\n",
      "Pred: 3.1820516845306326 Actual: 3.0\n",
      "Pred: 3.546838763198885 Actual: 3.0\n",
      "Pred: 3.500365392321734 Actual: 4.0\n",
      "Pred: 2.446201018001666 Actual: 0.5\n",
      "Pred: 2.9408869958953354 Actual: 4.0\n",
      "Pred: 4.235020861328975 Actual: 4.0\n",
      "Pred: 4.36311848344537 Actual: 5.0\n",
      "Pred: 4.411375630607295 Actual: 4.5\n",
      "Pred: 4.228322984761123 Actual: 5.0\n",
      "Pred: 2.7394658179533358 Actual: 4.0\n",
      "Pred: 4.009921256818619 Actual: 5.0\n",
      "Pred: 3.128437617679336 Actual: 4.5\n",
      "Pred: 4.3489497381938795 Actual: 2.0\n",
      "Pred: 3.3415432557537894 Actual: 3.0\n",
      "Pred: 3.665262047175167 Actual: 3.0\n",
      "Pred: 3.773721192365078 Actual: 3.0\n",
      "Pred: 4.510052183848792 Actual: 5.0\n",
      "Pred: 3.503062287034029 Actual: 3.5\n",
      "Pred: 4.187734124469332 Actual: 5.0\n",
      "Pred: 4.335439044072206 Actual: 4.0\n",
      "Pred: 3.7559630395777592 Actual: 4.0\n",
      "Pred: 4.163022631692176 Actual: 3.0\n",
      "Pred: 4.239698326641479 Actual: 4.0\n",
      "Pred: 4.178403039083167 Actual: 4.0\n",
      "Pred: 4.518654898091319 Actual: 4.5\n",
      "Pred: 4.073753768808268 Actual: 4.0\n",
      "Pred: 3.7329802313562603 Actual: 4.5\n",
      "Pred: 4.466126753659728 Actual: 5.0\n",
      "Pred: 4.366899847610525 Actual: 4.0\n",
      "Pred: 3.6714373687783373 Actual: 3.0\n",
      "Pred: 3.720031786724745 Actual: 4.0\n",
      "Pred: 3.9102327829698487 Actual: 3.0\n",
      "Pred: 4.335600825276629 Actual: 4.0\n",
      "Pred: 3.5867460689345534 Actual: 3.5\n",
      "Pred: 4.195047778224402 Actual: 4.0\n",
      "Pred: 3.8536975377502602 Actual: 3.5\n",
      "Pred: 4.33880162700607 Actual: 5.0\n",
      "Pred: 3.313968450101674 Actual: 2.5\n",
      "Pred: 2.7683038692916506 Actual: 3.0\n",
      "Pred: 3.714855978955111 Actual: 4.5\n",
      "Pred: 3.6029302926982294 Actual: 4.0\n",
      "Pred: 3.842230020267443 Actual: 2.0\n",
      "Pred: 3.589262135059114 Actual: 4.0\n",
      "Pred: 3.77372864517216 Actual: 2.0\n",
      "Pred: 3.5428055474774562 Actual: 2.5\n",
      "Pred: 4.408641686309527 Actual: 4.5\n",
      "Pred: 4.01256843980106 Actual: 3.0\n",
      "Pred: 4.150361779032355 Actual: 4.0\n",
      "Pred: 3.4978210893991135 Actual: 4.0\n",
      "Pred: 3.517500522711255 Actual: 3.5\n",
      "Pred: 3.571940600638214 Actual: 5.0\n",
      "Pred: 4.425913081804754 Actual: 5.0\n",
      "Pred: 4.462634464295288 Actual: 4.0\n",
      "Pred: 3.294026299597517 Actual: 3.5\n",
      "Pred: 4.379870781000318 Actual: 5.0\n",
      "Pred: 4.326817299969124 Actual: 5.0\n",
      "Pred: 4.438316225164188 Actual: 3.0\n",
      "Pred: 3.4982934162732504 Actual: 4.0\n",
      "Pred: 4.0132870409568575 Actual: 5.0\n",
      "Pred: 3.5973024161902343 Actual: 4.0\n",
      "Pred: 4.5021035044656434 Actual: 5.0\n",
      "Pred: 3.286296211469174 Actual: 3.0\n",
      "Pred: 4.370682928251852 Actual: 4.5\n",
      "Pred: 4.1338948799179 Actual: 3.5\n",
      "Pred: 3.4481630587722254 Actual: 4.0\n",
      "Pred: 3.8201273848147195 Actual: 5.0\n",
      "Pred: 4.526865577002304 Actual: 5.0\n",
      "Pred: 3.427699751158757 Actual: 4.0\n",
      "Pred: 3.2827679611792484 Actual: 4.0\n",
      "Pred: 4.622977180100053 Actual: 3.5\n",
      "Pred: 4.076870131916629 Actual: 3.5\n",
      "Pred: 3.493181310950466 Actual: 2.0\n",
      "Pred: 3.3413225405760825 Actual: 3.0\n",
      "Pred: 3.940224803161268 Actual: 4.5\n",
      "Pred: 2.945371713587642 Actual: 1.5\n",
      "Pred: 3.074142246672092 Actual: 4.0\n",
      "Pred: 2.9647727701133513 Actual: 4.0\n",
      "Pred: 3.89665558470924 Actual: 4.0\n",
      "Pred: 3.5312999844547837 Actual: 2.0\n",
      "Pred: 3.871113081029718 Actual: 3.0\n",
      "Pred: 3.7698528331794816 Actual: 4.0\n",
      "Pred: 3.735311070685938 Actual: 4.0\n",
      "Pred: 3.7529901774771366 Actual: 4.0\n",
      "Pred: 3.553012707112115 Actual: 3.0\n",
      "Pred: 3.6873485449540495 Actual: 3.0\n",
      "Pred: 4.497215509539187 Actual: 5.0\n",
      "Pred: 3.8392279633174837 Actual: 3.0\n",
      "Pred: 3.5540138546043587 Actual: 4.0\n",
      "Pred: 3.4425550714018662 Actual: 4.0\n",
      "Pred: 3.3261942708887915 Actual: 5.0\n",
      "Pred: 3.3663330442906596 Actual: 4.0\n",
      "Pred: 4.215144119987797 Actual: 5.0\n",
      "Pred: 3.9643354120745946 Actual: 4.0\n",
      "Pred: 3.815119565415692 Actual: 4.0\n",
      "Pred: 3.794058979059119 Actual: 2.0\n",
      "Pred: 4.117797913111825 Actual: 5.0\n",
      "Pred: 3.4267272299954725 Actual: 4.0\n",
      "Pred: 3.9332164388880178 Actual: 5.0\n",
      "Pred: 3.8458100202853904 Actual: 3.0\n",
      "Pred: 3.3092308285232455 Actual: 2.5\n",
      "Pred: 2.933665106257819 Actual: 3.0\n",
      "Pred: 3.7024238127045277 Actual: 3.0\n",
      "Pred: 2.4879998190849 Actual: 2.0\n",
      "Pred: 3.864583034273376 Actual: 4.5\n",
      "Pred: 3.194801570782885 Actual: 4.0\n",
      "Pred: 2.9078935558435144 Actual: 4.0\n",
      "Pred: 3.6991684063844508 Actual: 4.0\n",
      "Pred: 3.2308606629779875 Actual: 4.0\n",
      "Pred: 2.937107042618086 Actual: 3.0\n",
      "Pred: 3.587092260290686 Actual: 3.5\n",
      "Pred: 3.925155428919929 Actual: 5.0\n",
      "Pred: 3.937608619158609 Actual: 4.0\n",
      "Pred: 4.211927865246387 Actual: 5.0\n",
      "Pred: 3.1669058967897317 Actual: 5.0\n",
      "Pred: 3.7944475558993194 Actual: 3.0\n",
      "Pred: 3.6856448102886605 Actual: 5.0\n",
      "Pred: 2.7399310958035312 Actual: 2.5\n",
      "Pred: 3.378630048350009 Actual: 4.0\n",
      "Pred: 4.149866424994644 Actual: 4.0\n",
      "Pred: 3.3304993414639488 Actual: 4.0\n",
      "Pred: 3.7465091564417174 Actual: 5.0\n",
      "Pred: 3.9523035087844294 Actual: 4.0\n",
      "Pred: 3.1108319699417963 Actual: 4.0\n",
      "Pred: 3.1073794918668436 Actual: 3.0\n",
      "Pred: 3.5123601411568517 Actual: 3.5\n",
      "Pred: 3.788047285020357 Actual: 4.0\n",
      "Pred: 4.495670426309081 Actual: 5.0\n",
      "Pred: 3.2970824600172945 Actual: 5.0\n",
      "Pred: 4.462510345194457 Actual: 4.5\n",
      "Pred: 4.403450906754831 Actual: 5.0\n",
      "Pred: 3.9217933024788767 Actual: 4.0\n",
      "Pred: 3.5813815017722974 Actual: 4.0\n",
      "Pred: 2.9786696959022225 Actual: 3.0\n",
      "Pred: 3.0346286126000077 Actual: 1.0\n",
      "Pred: 4.378639648207975 Actual: 3.5\n",
      "Pred: 4.499948443904833 Actual: 4.0\n",
      "Pred: 3.407516684677577 Actual: 3.0\n",
      "Pred: 3.264421627978405 Actual: 3.5\n",
      "Pred: 3.675119593570847 Actual: 4.0\n",
      "Pred: 3.089920368979819 Actual: 3.0\n",
      "Pred: 3.9628978454595827 Actual: 3.0\n",
      "Pred: 3.7304747542191428 Actual: 3.5\n",
      "Pred: 4.472066823108327 Actual: 4.0\n",
      "Pred: 3.6268651572005344 Actual: 2.0\n",
      "Pred: 2.8324689116906616 Actual: 4.5\n",
      "Pred: 4.450122889067745 Actual: 5.0\n",
      "Pred: 3.9535958143421595 Actual: 5.0\n",
      "Pred: 4.277018619085853 Actual: 4.0\n",
      "Pred: 3.6847157279482943 Actual: 3.5\n",
      "Pred: 2.8068492929609388 Actual: 1.0\n",
      "Pred: 3.412069631700469 Actual: 3.0\n",
      "Pred: 4.039733447296402 Actual: 5.0\n",
      "Pred: 3.824534961716688 Actual: 4.0\n",
      "Pred: 4.17733846215401 Actual: 4.5\n",
      "Pred: 4.041320166069742 Actual: 4.0\n",
      "Pred: 3.8020265619433533 Actual: 3.5\n",
      "Pred: 4.492670694894516 Actual: 5.0\n",
      "Pred: 4.404746679918989 Actual: 4.0\n",
      "Pred: 4.101170466390471 Actual: 3.5\n",
      "Pred: 3.5111447455182905 Actual: 3.0\n",
      "Pred: 3.392928140237865 Actual: 5.0\n",
      "Pred: 3.851043908427697 Actual: 1.5\n",
      "Pred: 3.155490434895852 Actual: 3.5\n",
      "Pred: 3.3321324420963183 Actual: 1.0\n",
      "Pred: 2.067106787914616 Actual: 2.0\n",
      "Pred: 2.7014989552270765 Actual: 3.0\n",
      "Pred: 3.9489798566897156 Actual: 4.0\n",
      "Pred: 3.6045399780038205 Actual: 1.0\n",
      "Pred: 3.9107468698959003 Actual: 4.0\n",
      "Pred: 3.931604655243798 Actual: 3.0\n",
      "Pred: 3.21234334513072 Actual: 4.0\n",
      "Pred: 3.807179229793409 Actual: 4.0\n",
      "Pred: 1.663755263967887 Actual: 3.0\n",
      "Pred: 3.2622208052589072 Actual: 5.0\n",
      "Pred: 3.8737667272830296 Actual: 3.0\n",
      "Pred: 3.460664712846478 Actual: 3.0\n",
      "Pred: 4.0280090594226765 Actual: 4.0\n",
      "Pred: 3.4808055329956917 Actual: 5.0\n",
      "Pred: 3.8520958180140763 Actual: 4.0\n",
      "Pred: 4.106174706452695 Actual: 3.5\n",
      "Pred: 3.830869001358595 Actual: 3.0\n",
      "Pred: 4.270852582213797 Actual: 4.0\n",
      "Pred: 2.9245925739173035 Actual: 3.5\n",
      "Pred: 3.7755886158763503 Actual: 5.0\n",
      "Pred: 4.456743386532098 Actual: 5.0\n",
      "Pred: 4.255596605623847 Actual: 3.0\n",
      "Pred: 3.9129986552111107 Actual: 3.0\n",
      "Pred: 2.828271901034589 Actual: 2.0\n",
      "Pred: 4.501279539167177 Actual: 3.0\n",
      "Pred: 3.6569824657244143 Actual: 3.0\n",
      "Pred: 4.4255289323345774 Actual: 4.0\n",
      "Pred: 3.9213185335779364 Actual: 5.0\n",
      "Pred: 3.9929473570906193 Actual: 4.0\n",
      "Pred: 4.176260414808721 Actual: 4.5\n",
      "Pred: 3.7580281830123536 Actual: 4.0\n",
      "Pred: 3.721893114374391 Actual: 5.0\n",
      "Pred: 3.1567902550941693 Actual: 2.0\n",
      "Pred: 3.4960000080105242 Actual: 3.0\n",
      "Pred: 3.8270270975323255 Actual: 4.0\n",
      "Pred: 2.334994792747346 Actual: 3.0\n",
      "Pred: 4.334530796453076 Actual: 4.5\n",
      "Pred: 3.8874952188984264 Actual: 3.0\n",
      "Pred: 3.3182592625682825 Actual: 3.5\n",
      "Pred: 4.009321669379485 Actual: 4.0\n",
      "Pred: 4.34898334091577 Actual: 3.5\n",
      "Pred: 4.40772773059642 Actual: 4.0\n",
      "Pred: 3.566340818501271 Actual: 4.0\n",
      "Pred: 3.7752963474748857 Actual: 4.0\n",
      "Pred: 3.3521933723808965 Actual: 3.0\n",
      "Pred: 3.747710593473244 Actual: 3.5\n",
      "Pred: 4.047297426579383 Actual: 4.5\n",
      "Pred: 3.3643126463791098 Actual: 4.0\n",
      "Pred: 4.015615300645221 Actual: 3.5\n",
      "Pred: 4.404307355593446 Actual: 5.0\n",
      "Pred: 3.7216765085577794 Actual: 3.0\n",
      "Pred: 3.337596438909849 Actual: 4.0\n",
      "Pred: 3.5829746402081426 Actual: 3.0\n",
      "Pred: 3.4316814232421082 Actual: 3.0\n",
      "Pred: 3.5063342267331064 Actual: 4.0\n",
      "Pred: 4.076912452500439 Actual: 4.5\n",
      "Pred: 4.3596271768047785 Actual: 3.5\n",
      "Pred: 4.257305237803565 Actual: 5.0\n",
      "Pred: 3.6925078261276827 Actual: 3.5\n",
      "Pred: 3.808687564586225 Actual: 4.0\n",
      "Pred: 4.119291968974768 Actual: 5.0\n",
      "Pred: 4.0526659337557405 Actual: 3.0\n",
      "Pred: 3.3542174991382385 Actual: 3.0\n",
      "Pred: 3.6541680111222674 Actual: 4.0\n",
      "Pred: 3.4092900943814066 Actual: 3.0\n",
      "Pred: 4.065367371069162 Actual: 3.5\n",
      "Pred: 4.542466687737351 Actual: 4.0\n",
      "Pred: 2.6978080148266477 Actual: 4.0\n",
      "Pred: 2.7206370617127313 Actual: 1.0\n",
      "Pred: 3.7666066387414014 Actual: 4.0\n",
      "Pred: 3.740890018451745 Actual: 5.0\n",
      "Pred: 2.824072527014636 Actual: 3.0\n",
      "Pred: 3.607587320416793 Actual: 4.0\n",
      "Pred: 3.5296765818488214 Actual: 4.0\n",
      "Pred: 3.9245902160310684 Actual: 5.0\n",
      "Pred: 4.522064273922525 Actual: 3.0\n",
      "Pred: 3.786971503651813 Actual: 4.0\n",
      "Pred: 4.230319399325266 Actual: 3.0\n",
      "Pred: 4.15103075480756 Actual: 4.0\n",
      "Pred: 3.7705046282749057 Actual: 2.5\n",
      "Pred: 2.9080662328841203 Actual: 3.0\n",
      "Pred: 3.307038638287718 Actual: 4.0\n",
      "Pred: 2.7076417618446356 Actual: 3.0\n",
      "Pred: 3.597060735704407 Actual: 3.5\n",
      "Pred: 3.7427890145643565 Actual: 4.0\n",
      "Pred: 3.996417733179038 Actual: 3.0\n",
      "Pred: 3.0702462779754196 Actual: 3.0\n",
      "Pred: 3.857600726558838 Actual: 5.0\n",
      "Pred: 4.1043572148850735 Actual: 4.0\n",
      "Pred: 2.424682910953708 Actual: 1.0\n",
      "Pred: 3.9668833311612772 Actual: 3.0\n",
      "Pred: 3.662473566453601 Actual: 4.5\n",
      "Pred: 3.5355170570845242 Actual: 3.0\n",
      "Pred: 3.894213046551933 Actual: 4.0\n",
      "Pred: 3.6870698911575395 Actual: 4.0\n",
      "Pred: 3.492497042909342 Actual: 3.5\n",
      "Pred: 3.5834835542789927 Actual: 3.0\n",
      "Pred: 4.62602302695161 Actual: 5.0\n",
      "Pred: 3.868095854436763 Actual: 4.5\n",
      "Pred: 3.435369363588942 Actual: 3.0\n",
      "Pred: 3.412318006342457 Actual: 3.5\n",
      "Pred: 2.520720911350573 Actual: 3.0\n",
      "Pred: 3.6687372092872654 Actual: 3.0\n",
      "Pred: 3.517961443904317 Actual: 1.0\n",
      "Pred: 3.7783480291672307 Actual: 5.0\n",
      "Pred: 3.5497247794419264 Actual: 4.5\n",
      "Pred: 2.977038168197154 Actual: 4.5\n",
      "Pred: 2.8209013899664983 Actual: 3.0\n",
      "Pred: 3.641738982119844 Actual: 3.0\n",
      "Pred: 4.46567485271783 Actual: 5.0\n",
      "Pred: 2.700584311881712 Actual: 2.0\n",
      "Pred: 4.527630734547475 Actual: 5.0\n",
      "Pred: 3.4719413723309405 Actual: 3.0\n",
      "Pred: 3.781914801817799 Actual: 5.0\n",
      "Pred: 3.8893748127248613 Actual: 3.0\n",
      "Pred: 4.544953217511616 Actual: 5.0\n",
      "Pred: 3.347942946358992 Actual: 3.0\n",
      "Pred: 4.078673174588215 Actual: 5.0\n",
      "Pred: 3.723415120347226 Actual: 3.0\n",
      "Pred: 3.881636616606151 Actual: 4.0\n",
      "Pred: 4.328227056389975 Actual: 4.0\n",
      "Pred: 4.593310766388641 Actual: 5.0\n",
      "Pred: 3.662759431866172 Actual: 3.0\n",
      "Pred: 3.0779269381252816 Actual: 2.0\n",
      "Pred: 3.1144882251842274 Actual: 3.0\n",
      "Pred: 3.495354017816091 Actual: 2.0\n",
      "Pred: 4.140760131744823 Actual: 2.0\n",
      "Pred: 4.449478513948419 Actual: 4.5\n",
      "Pred: 4.457370820497114 Actual: 4.0\n",
      "Pred: 4.285188443100936 Actual: 4.5\n",
      "Pred: 3.9249984732935044 Actual: 4.0\n",
      "Pred: 4.092811428274772 Actual: 4.0\n",
      "Pred: 3.8900229612440946 Actual: 5.0\n",
      "Pred: 4.0170555828457895 Actual: 3.5\n",
      "Pred: 4.561642448821544 Actual: 5.0\n",
      "Pred: 3.912767797624389 Actual: 2.0\n",
      "Pred: 4.164961493078569 Actual: 5.0\n",
      "Pred: 4.129725066308376 Actual: 2.0\n",
      "Pred: 3.8063940211265197 Actual: 3.0\n",
      "Pred: 4.067451126076512 Actual: 4.0\n",
      "Pred: 4.385284925482171 Actual: 4.0\n",
      "Pred: 3.2648219725982166 Actual: 2.0\n",
      "Pred: 2.4734085425386567 Actual: 3.0\n",
      "Pred: 3.9500790661927896 Actual: 5.0\n",
      "Pred: 3.5857714791879034 Actual: 4.0\n",
      "Pred: 3.9037392142068748 Actual: 4.0\n",
      "Pred: 3.9421039053388807 Actual: 4.5\n",
      "Pred: 3.9691790612921882 Actual: 4.0\n",
      "Pred: 2.8716978081785873 Actual: 2.0\n",
      "Pred: 4.061532880461739 Actual: 4.0\n",
      "Pred: 3.9669722169587347 Actual: 5.0\n",
      "Pred: 4.1528662793427715 Actual: 4.0\n",
      "Pred: 3.435002642999291 Actual: 3.0\n",
      "Pred: 3.2551305561164057 Actual: 3.0\n",
      "Pred: 3.6147389316432297 Actual: 3.0\n",
      "Pred: 2.550966432526029 Actual: 2.0\n",
      "Pred: 3.4971985678371005 Actual: 4.0\n",
      "Pred: 3.6077358653554206 Actual: 3.5\n",
      "Pred: 3.8343294629262665 Actual: 5.0\n",
      "Pred: 4.101648392381893 Actual: 3.0\n",
      "Pred: 3.261593970892366 Actual: 2.0\n",
      "Pred: 4.0401588644803486 Actual: 3.0\n",
      "Pred: 3.2488549871651933 Actual: 3.0\n",
      "Pred: 3.947808686295721 Actual: 4.0\n",
      "Pred: 3.443822233089152 Actual: 3.0\n",
      "Pred: 3.6733771244832467 Actual: 2.0\n",
      "Pred: 3.368042390809544 Actual: 5.0\n",
      "Pred: 3.3153681015183936 Actual: 3.0\n",
      "Pred: 3.1936221244851333 Actual: 3.0\n",
      "Pred: 3.411990829708123 Actual: 4.0\n",
      "Pred: 3.4396453280434076 Actual: 3.0\n",
      "Pred: 2.9985041005836544 Actual: 3.0\n",
      "Pred: 3.9634710305058354 Actual: 1.5\n",
      "Pred: 2.459669204617378 Actual: 3.5\n",
      "Pred: 2.5289255879193044 Actual: 1.0\n",
      "Pred: 3.764422390337077 Actual: 2.0\n",
      "Pred: 4.492883081661478 Actual: 5.0\n",
      "Pred: 4.14194789117618 Actual: 4.0\n",
      "Pred: 3.835168581058905 Actual: 3.0\n",
      "Pred: 2.9826520385185873 Actual: 2.0\n",
      "Pred: 3.9138522208746354 Actual: 2.5\n",
      "Pred: 4.3352858830597745 Actual: 5.0\n",
      "Pred: 3.0674324146164182 Actual: 2.5\n",
      "Pred: 3.408708613716564 Actual: 3.5\n",
      "Pred: 4.41802716461483 Actual: 5.0\n",
      "Pred: 4.174922319280719 Actual: 5.0\n",
      "Pred: 4.102107422085629 Actual: 3.0\n",
      "Pred: 4.257305243198267 Actual: 4.5\n",
      "Pred: 3.9819432240208466 Actual: 4.0\n",
      "Pred: 4.162320787823134 Actual: 3.0\n",
      "Pred: 1.7948481410463135 Actual: 1.0\n",
      "Pred: 3.6170297599292573 Actual: 0.5\n",
      "Pred: 3.582894641525699 Actual: 3.5\n",
      "Pred: 3.228486495551046 Actual: 2.0\n",
      "Pred: 4.041202426014056 Actual: 5.0\n",
      "Pred: 3.844390221753607 Actual: 2.0\n",
      "Pred: 3.1927968662628277 Actual: 3.5\n",
      "Pred: 3.910441690638214 Actual: 5.0\n",
      "Pred: 3.8090335626991987 Actual: 1.0\n",
      "Pred: 4.397990063097483 Actual: 4.0\n",
      "Pred: 3.2426164605010728 Actual: 4.0\n",
      "Pred: 3.3315127840560774 Actual: 1.0\n",
      "Pred: 4.384280022495018 Actual: 4.0\n",
      "Pred: 4.399093605803489 Actual: 4.0\n",
      "Pred: 2.934119082765269 Actual: 2.5\n",
      "Pred: 3.388154167584379 Actual: 4.0\n",
      "Pred: 3.6630839950339515 Actual: 4.5\n",
      "Pred: 2.475978632860219 Actual: 2.0\n",
      "Pred: 4.378691097346085 Actual: 3.5\n",
      "Pred: 3.6722082370122364 Actual: 5.0\n",
      "Pred: 3.8780228031743396 Actual: 3.0\n",
      "Pred: 2.941134066216054 Actual: 3.0\n",
      "Pred: 3.276716986966812 Actual: 3.0\n",
      "Pred: 4.17269094758482 Actual: 4.5\n",
      "Pred: 3.5455853672753896 Actual: 3.0\n",
      "Pred: 4.503915611909148 Actual: 5.0\n",
      "Pred: 4.18495498177603 Actual: 3.0\n",
      "Pred: 3.8039410446421456 Actual: 3.0\n",
      "Pred: 2.936931481678233 Actual: 3.0\n",
      "Pred: 3.22797911556781 Actual: 3.0\n",
      "Pred: 4.128111533824978 Actual: 5.0\n",
      "Pred: 3.8453487016899146 Actual: 3.0\n",
      "Pred: 3.258547625689637 Actual: 3.0\n",
      "Pred: 2.5103016280712684 Actual: 3.0\n",
      "Pred: 3.471407442209949 Actual: 3.0\n",
      "Pred: 4.529592758297055 Actual: 4.5\n",
      "Pred: 3.3880652763008454 Actual: 3.0\n",
      "Pred: 4.165461283773958 Actual: 3.0\n",
      "Pred: 2.9855624173382522 Actual: 4.0\n",
      "Pred: 3.0057317381180333 Actual: 3.0\n",
      "Pred: 4.0610083417410285 Actual: 4.5\n",
      "Pred: 3.7690793916130745 Actual: 4.0\n",
      "Pred: 3.103768755883515 Actual: 3.0\n",
      "Pred: 3.1700429441229816 Actual: 3.0\n",
      "Pred: 3.271725600201793 Actual: 3.5\n",
      "Pred: 2.90946101346168 Actual: 2.0\n",
      "Pred: 4.166088658966209 Actual: 5.0\n",
      "Pred: 3.985013899439945 Actual: 4.0\n",
      "Pred: 4.364047300918154 Actual: 3.0\n",
      "Pred: 3.936056840349928 Actual: 3.0\n",
      "Pred: 2.861098655917984 Actual: 5.0\n",
      "Pred: 4.401122819320638 Actual: 4.0\n",
      "Pred: 3.1859213165965317 Actual: 3.0\n",
      "Pred: 2.813843301403229 Actual: 1.0\n",
      "Pred: 3.7486285028784203 Actual: 3.0\n",
      "Pred: 4.366398170767299 Actual: 4.5\n",
      "Pred: 4.467099052801185 Actual: 5.0\n",
      "Pred: 4.476830018392668 Actual: 3.5\n",
      "Pred: 4.416227887690427 Actual: 4.5\n",
      "Pred: 3.3016516804752816 Actual: 1.0\n",
      "Pred: 2.58666290140006 Actual: 1.0\n",
      "Pred: 4.350269759338332 Actual: 4.5\n",
      "Pred: 2.2417155871590584 Actual: 1.5\n",
      "Pred: 3.576148771153062 Actual: 5.0\n",
      "Pred: 3.948262445597678 Actual: 3.5\n",
      "Pred: 4.1803024325526295 Actual: 3.0\n",
      "Pred: 2.2384255371661417 Actual: 4.0\n",
      "Pred: 3.6166432268280833 Actual: 2.0\n",
      "Pred: 3.326114879337451 Actual: 2.5\n",
      "Pred: 2.994307935357741 Actual: 3.0\n",
      "Pred: 3.8574502915212627 Actual: 5.0\n",
      "Pred: 3.243834605663563 Actual: 3.0\n",
      "Pred: 4.076884670555017 Actual: 4.0\n",
      "Pred: 3.9053534199388302 Actual: 5.0\n",
      "Pred: 3.4413342240336084 Actual: 5.0\n",
      "Pred: 2.8215837317685533 Actual: 5.0\n",
      "Pred: 3.9482046582051877 Actual: 4.5\n",
      "Pred: 3.5684280035507636 Actual: 4.0\n",
      "Pred: 3.5135370892164786 Actual: 1.5\n",
      "Pred: 2.8158895677666473 Actual: 3.0\n",
      "Pred: 3.7677376398496603 Actual: 3.5\n",
      "Pred: 3.79217190017725 Actual: 3.0\n",
      "Pred: 4.0502718755278835 Actual: 4.0\n",
      "Pred: 4.061910743482959 Actual: 4.0\n",
      "Pred: 4.457526337949851 Actual: 4.0\n",
      "Pred: 3.496987382710465 Actual: 4.0\n",
      "Pred: 3.6410340122377667 Actual: 5.0\n",
      "Pred: 4.087455259200675 Actual: 4.0\n",
      "Pred: 3.462818342592862 Actual: 5.0\n",
      "Pred: 2.9454223083428865 Actual: 3.0\n",
      "Pred: 3.7080779264206956 Actual: 3.5\n",
      "Pred: 4.062897697129431 Actual: 3.0\n",
      "Pred: 4.128632385561112 Actual: 3.0\n",
      "Pred: 4.463253391836028 Actual: 5.0\n",
      "Pred: 4.028824909604429 Actual: 4.0\n",
      "Pred: 4.429218056360044 Actual: 5.0\n",
      "Pred: 3.5895284620890244 Actual: 3.0\n",
      "Pred: 4.426055876216929 Actual: 5.0\n",
      "Pred: 4.001028752871347 Actual: 5.0\n",
      "Pred: 3.539517459727824 Actual: 4.0\n",
      "Pred: 4.471347116727954 Actual: 4.0\n",
      "Pred: 4.272168461178156 Actual: 4.0\n",
      "Pred: 4.357639705441745 Actual: 5.0\n",
      "Pred: 2.8654661972593027 Actual: 4.0\n",
      "Pred: 3.3338443675383487 Actual: 4.0\n",
      "Pred: 4.122825555990943 Actual: 4.5\n",
      "Pred: 3.82089433992704 Actual: 1.0\n",
      "Pred: 2.632741631443634 Actual: 2.0\n",
      "Pred: 2.632150811891392 Actual: 2.5\n",
      "Pred: 4.441023430357466 Actual: 5.0\n",
      "Pred: 3.7728259044815893 Actual: 4.0\n",
      "Pred: 3.933632776451413 Actual: 5.0\n",
      "Pred: 3.119954035627649 Actual: 2.0\n",
      "Pred: 3.0034080141701125 Actual: 3.0\n",
      "Pred: 3.4394917954162763 Actual: 2.0\n",
      "Pred: 3.5244790953989957 Actual: 3.0\n",
      "Pred: 3.9963231009396107 Actual: 4.0\n",
      "Pred: 4.361508978368654 Actual: 4.0\n",
      "Pred: 2.9942620625015874 Actual: 4.0\n",
      "Pred: 4.382677769925624 Actual: 4.0\n",
      "Pred: 2.5238802933708646 Actual: 1.5\n",
      "Pred: 3.7559955329729044 Actual: 3.0\n",
      "Pred: 3.9960818575626837 Actual: 3.0\n",
      "Pred: 3.8367384575575887 Actual: 3.0\n",
      "Pred: 3.9066912694496145 Actual: 3.0\n",
      "Pred: 3.1449925652145705 Actual: 3.0\n",
      "Pred: 4.437298757378248 Actual: 4.5\n",
      "Pred: 4.584105682590904 Actual: 5.0\n",
      "Pred: 3.450738379881513 Actual: 3.0\n",
      "Pred: 3.9865036370316567 Actual: 5.0\n",
      "Pred: 3.614383024646514 Actual: 3.0\n",
      "Pred: 3.796078365072521 Actual: 4.0\n",
      "Pred: 2.876520530225063 Actual: 2.0\n",
      "Pred: 4.455013141062525 Actual: 4.0\n",
      "Pred: 3.718073802289544 Actual: 3.0\n",
      "Pred: 3.3462094691406086 Actual: 4.0\n",
      "Pred: 4.437985525055705 Actual: 4.5\n",
      "Pred: 3.8025054543155763 Actual: 5.0\n",
      "Pred: 1.4833283032103637 Actual: 0.5\n",
      "Pred: 3.415781544158757 Actual: 3.0\n",
      "Pred: 3.703618391982127 Actual: 4.5\n",
      "Pred: 3.531237647113228 Actual: 2.0\n",
      "Pred: 3.6777968721288694 Actual: 5.0\n",
      "Pred: 3.504646172817411 Actual: 3.0\n",
      "Pred: 2.9384457027468476 Actual: 4.0\n",
      "Pred: 3.7634977314964035 Actual: 4.0\n",
      "Pred: 2.2163142598553827 Actual: 1.5\n",
      "Pred: 3.459157483537565 Actual: 4.5\n",
      "Pred: 3.4489938209455424 Actual: 3.5\n",
      "Pred: 2.071166992827095 Actual: 1.0\n",
      "Pred: 2.7182590280857113 Actual: 3.0\n",
      "Pred: 4.061773194443311 Actual: 4.0\n",
      "Pred: 3.483369321463717 Actual: 4.0\n",
      "Pred: 3.592812169196048 Actual: 4.0\n",
      "Pred: 4.013391218072808 Actual: 5.0\n",
      "Pred: 4.399948375597288 Actual: 4.0\n",
      "Pred: 3.5664757474713387 Actual: 5.0\n",
      "Pred: 4.4798173182724526 Actual: 5.0\n",
      "Pred: 3.583453198714768 Actual: 3.5\n",
      "Pred: 3.415803775811441 Actual: 3.0\n",
      "Pred: 4.364485095184427 Actual: 5.0\n",
      "Pred: 3.7824139043422202 Actual: 3.0\n",
      "Pred: 3.9128898369002445 Actual: 4.0\n",
      "Pred: 3.032428817865162 Actual: 3.0\n",
      "Pred: 4.018473576164264 Actual: 3.0\n",
      "Pred: 4.476477793110713 Actual: 5.0\n",
      "Pred: 3.873901295159161 Actual: 4.5\n",
      "Pred: 3.819435949715382 Actual: 3.0\n",
      "Pred: 4.522931552651095 Actual: 4.0\n",
      "Pred: 4.372586263929179 Actual: 5.0\n",
      "Pred: 3.974401206877099 Actual: 4.5\n",
      "Pred: 4.447859077935254 Actual: 5.0\n",
      "Pred: 3.6034266488931306 Actual: 3.0\n",
      "Pred: 3.634542194279884 Actual: 3.0\n",
      "Pred: 3.8099321638265056 Actual: 3.0\n",
      "Pred: 3.450746755986677 Actual: 3.5\n",
      "Pred: 4.081708651527246 Actual: 4.0\n",
      "Pred: 4.265558345741669 Actual: 4.5\n",
      "Pred: 3.772908071090043 Actual: 4.0\n",
      "Pred: 3.803777662518895 Actual: 4.5\n",
      "Pred: 3.110010374024959 Actual: 3.0\n",
      "Pred: 3.1439942488175836 Actual: 4.0\n",
      "Pred: 3.757677380066397 Actual: 4.0\n",
      "Pred: 4.421203475171994 Actual: 4.0\n",
      "Pred: 3.406932877639292 Actual: 3.0\n",
      "Pred: 3.883781395609218 Actual: 4.0\n",
      "Pred: 4.604954894021957 Actual: 5.0\n",
      "Pred: 4.494608685431834 Actual: 4.0\n",
      "Pred: 3.9499122974091545 Actual: 4.0\n",
      "Pred: 4.452018966070839 Actual: 3.0\n",
      "Pred: 3.298967295164739 Actual: 3.0\n",
      "Pred: 3.0947042640150015 Actual: 3.0\n",
      "Pred: 3.6401060313773725 Actual: 4.0\n",
      "Pred: 3.4003447570089955 Actual: 2.5\n",
      "Pred: 4.03027952474109 Actual: 3.0\n",
      "Pred: 4.199878749890647 Actual: 4.0\n",
      "Pred: 4.4690076444187445 Actual: 2.5\n",
      "Pred: 3.727121659265079 Actual: 4.5\n",
      "Pred: 2.688962907192982 Actual: 1.5\n",
      "Pred: 3.8649411230398485 Actual: 4.0\n",
      "Pred: 4.458952901295913 Actual: 4.0\n",
      "Pred: 4.228948442927642 Actual: 4.0\n",
      "Pred: 3.0606211550672184 Actual: 3.0\n",
      "Pred: 2.9996653990988995 Actual: 3.0\n",
      "Pred: 4.086397195035871 Actual: 5.0\n",
      "Pred: 3.9466991156784794 Actual: 5.0\n",
      "Pred: 4.363018501195937 Actual: 4.0\n",
      "Pred: 3.8495669171648044 Actual: 5.0\n",
      "Pred: 2.9676007913897653 Actual: 4.5\n",
      "Pred: 3.109184378768103 Actual: 3.0\n",
      "Pred: 2.9950252584841253 Actual: 3.0\n",
      "Pred: 3.0347716839216656 Actual: 4.0\n",
      "Pred: 3.454653254793924 Actual: 3.0\n",
      "Pred: 3.520012304348039 Actual: 4.0\n",
      "Pred: 4.02966594188866 Actual: 3.5\n",
      "Pred: 4.304347641878939 Actual: 5.0\n",
      "Pred: 4.070354138445881 Actual: 5.0\n",
      "Pred: 3.494217499536916 Actual: 3.0\n",
      "Pred: 3.7277411507328253 Actual: 1.5\n",
      "Pred: 3.9972786374698703 Actual: 4.0\n",
      "Pred: 4.039790163283919 Actual: 2.0\n",
      "Pred: 3.666651330093082 Actual: 5.0\n",
      "Pred: 3.765116254223541 Actual: 4.5\n",
      "Pred: 3.5242763577826555 Actual: 3.0\n",
      "Pred: 3.9459788043913098 Actual: 4.0\n",
      "Pred: 3.679128654938344 Actual: 4.0\n",
      "Pred: 3.3391493826000325 Actual: 3.0\n",
      "Pred: 3.5712516562261727 Actual: 3.0\n",
      "Pred: 3.8119554795856945 Actual: 5.0\n",
      "Pred: 3.9104479579771825 Actual: 5.0\n",
      "Pred: 4.386189205492507 Actual: 3.5\n",
      "Pred: 4.156930983980685 Actual: 5.0\n",
      "Pred: 4.1358385092012 Actual: 5.0\n",
      "Pred: 3.880501391756781 Actual: 4.0\n",
      "Pred: 3.878089982519946 Actual: 3.0\n",
      "Pred: 4.134667642475224 Actual: 2.0\n",
      "Pred: 3.5757051415401824 Actual: 5.0\n",
      "Pred: 3.4045414131163243 Actual: 3.5\n",
      "Pred: 4.1725466432052025 Actual: 3.0\n",
      "Pred: 3.371627696932892 Actual: 3.5\n",
      "Pred: 3.68548515887765 Actual: 3.5\n",
      "Pred: 4.4033581245470605 Actual: 4.0\n",
      "Pred: 3.6467046820310274 Actual: 4.0\n",
      "Pred: 4.466099236466692 Actual: 2.0\n",
      "Pred: 2.546831638753119 Actual: 3.0\n",
      "Pred: 3.844567562004005 Actual: 3.0\n",
      "Pred: 4.212730128337828 Actual: 5.0\n",
      "Pred: 3.4902948700275394 Actual: 5.0\n",
      "Pred: 4.36987052139018 Actual: 4.0\n",
      "Pred: 4.2429609852635775 Actual: 3.0\n",
      "Pred: 3.1040048788494183 Actual: 4.0\n",
      "Pred: 3.2860926576936547 Actual: 3.0\n",
      "Pred: 3.627701849642823 Actual: 3.0\n",
      "Pred: 3.594786067066018 Actual: 4.5\n",
      "Pred: 2.7259125628546417 Actual: 2.5\n",
      "Pred: 2.74686394015719 Actual: 4.0\n",
      "Pred: 2.9646552760938363 Actual: 3.0\n",
      "Pred: 4.431789294362429 Actual: 4.0\n",
      "Pred: 4.354124224641315 Actual: 4.0\n",
      "Pred: 4.507802842083325 Actual: 5.0\n",
      "Pred: 3.780619801210413 Actual: 5.0\n",
      "Pred: 3.5392485888021383 Actual: 3.0\n",
      "Pred: 4.534631293009086 Actual: 5.0\n",
      "Pred: 3.5592404048732225 Actual: 3.0\n",
      "Pred: 3.9030949014309395 Actual: 3.0\n",
      "Pred: 3.2124352251943713 Actual: 5.0\n",
      "Pred: 2.981248695495752 Actual: 2.0\n",
      "Pred: 3.0110099964328176 Actual: 4.0\n",
      "Pred: 2.4739329477377408 Actual: 1.5\n",
      "Pred: 4.515559756524318 Actual: 3.5\n",
      "Pred: 3.9813548204617932 Actual: 4.0\n",
      "Pred: 3.0421801359457517 Actual: 3.0\n",
      "Pred: 3.979955821519018 Actual: 3.0\n",
      "Pred: 3.834549699936619 Actual: 3.0\n",
      "Pred: 4.040587147330977 Actual: 4.0\n",
      "Pred: 2.9308106315914473 Actual: 3.5\n",
      "Pred: 3.4213488980427735 Actual: 3.0\n",
      "Pred: 2.7142752638300474 Actual: 2.0\n",
      "Pred: 3.342289918600876 Actual: 4.0\n",
      "Pred: 3.8534472272502835 Actual: 3.5\n",
      "Pred: 3.8223549257428844 Actual: 3.0\n",
      "Pred: 4.306842856133985 Actual: 4.0\n",
      "Pred: 4.233396152491363 Actual: 3.5\n",
      "Pred: 3.3424246827776685 Actual: 3.0\n",
      "Pred: 3.9588761945185142 Actual: 5.0\n",
      "Pred: 3.656253774296362 Actual: 3.0\n",
      "Pred: 3.695090235461463 Actual: 3.5\n",
      "Pred: 3.9253720643321084 Actual: 5.0\n",
      "Pred: 2.1060414707464834 Actual: 0.5\n",
      "Pred: 4.1747870828349205 Actual: 4.0\n",
      "Pred: 4.493650162466352 Actual: 5.0\n",
      "Pred: 3.43960025788036 Actual: 4.0\n",
      "Pred: 3.290397210345919 Actual: 3.5\n",
      "Pred: 4.1737262119109975 Actual: 3.0\n",
      "Pred: 3.462447900013748 Actual: 4.0\n",
      "Pred: 2.9924214970925034 Actual: 3.0\n",
      "Pred: 3.6674398185596444 Actual: 3.0\n",
      "Pred: 3.075199937963323 Actual: 4.0\n",
      "Pred: 4.50087792074666 Actual: 5.0\n",
      "Pred: 3.5891868846372144 Actual: 3.0\n",
      "Pred: 3.362942532781858 Actual: 5.0\n",
      "Pred: 3.804188473601719 Actual: 3.0\n",
      "Pred: 3.3768139786681837 Actual: 1.0\n",
      "Pred: 2.875510841694621 Actual: 5.0\n",
      "Pred: 2.8941413810346406 Actual: 3.0\n",
      "Pred: 3.534803213943418 Actual: 2.5\n",
      "Pred: 4.372962770509017 Actual: 4.0\n",
      "Pred: 3.066554890335927 Actual: 3.0\n",
      "Pred: 3.971895361551287 Actual: 5.0\n",
      "Pred: 4.545889796599263 Actual: 4.0\n",
      "Pred: 3.443695062476302 Actual: 3.0\n",
      "Pred: 3.185081143534711 Actual: 3.0\n",
      "Pred: 4.380084930806369 Actual: 4.0\n",
      "Pred: 3.5698441904920193 Actual: 4.0\n",
      "Pred: 3.8064193823268884 Actual: 4.0\n",
      "Pred: 3.442482826534297 Actual: 3.5\n",
      "Pred: 3.8795521116741694 Actual: 4.5\n",
      "Pred: 2.246573542007418 Actual: 2.5\n",
      "Pred: 4.151971211801882 Actual: 5.0\n",
      "Pred: 3.9438380949995673 Actual: 4.0\n",
      "Pred: 3.1519131864538408 Actual: 3.0\n",
      "Pred: 3.850267021301682 Actual: 4.0\n",
      "Pred: 4.448484277472478 Actual: 5.0\n",
      "Pred: 3.821927126115253 Actual: 3.0\n",
      "Pred: 3.76292245212344 Actual: 3.0\n",
      "Pred: 3.296981992833768 Actual: 2.5\n",
      "Pred: 4.150412693154697 Actual: 4.0\n",
      "Pred: 3.137990951911094 Actual: 4.0\n",
      "Pred: 4.51513125596149 Actual: 4.5\n",
      "Pred: 3.363554430316083 Actual: 3.0\n",
      "Pred: 3.8107664234946705 Actual: 1.0\n",
      "Pred: 4.048958982281209 Actual: 5.0\n",
      "Pred: 3.362178681160415 Actual: 3.5\n",
      "Pred: 4.188398359749985 Actual: 4.5\n",
      "Pred: 4.158002030599922 Actual: 3.0\n",
      "Pred: 3.753049089140029 Actual: 3.0\n",
      "Pred: 4.445662704727358 Actual: 3.0\n",
      "Pred: 2.7522800966254706 Actual: 3.0\n",
      "Pred: 3.4280707070881915 Actual: 4.0\n",
      "Pred: 4.621088747910718 Actual: 4.0\n",
      "Pred: 3.4504928455993054 Actual: 5.0\n",
      "Pred: 4.626023026951608 Actual: 5.0\n",
      "Pred: 4.399069586563579 Actual: 4.5\n",
      "Pred: 4.098348549691584 Actual: 4.5\n",
      "Pred: 3.91425602389957 Actual: 4.5\n",
      "Pred: 3.6966592720967353 Actual: 3.0\n",
      "Pred: 4.179770470790651 Actual: 0.5\n",
      "Pred: 3.88339342602439 Actual: 5.0\n",
      "Pred: 4.043798948401505 Actual: 4.5\n",
      "Pred: 3.6612880415643088 Actual: 4.0\n",
      "Pred: 3.591322233129158 Actual: 1.0\n",
      "Pred: 3.738159865747849 Actual: 3.0\n",
      "Pred: 3.545377940032581 Actual: 3.0\n",
      "Pred: 3.586913643396312 Actual: 3.0\n",
      "Pred: 3.494308277027705 Actual: 2.5\n",
      "Pred: 4.125095857554077 Actual: 5.0\n",
      "Pred: 4.087891952912837 Actual: 4.0\n",
      "Pred: 3.778350335410043 Actual: 4.0\n",
      "Pred: 3.0531158395692524 Actual: 1.5\n",
      "Pred: 3.386254041757369 Actual: 3.0\n",
      "Pred: 4.036990190412269 Actual: 5.0\n",
      "Pred: 3.1616380639895776 Actual: 3.0\n",
      "Pred: 3.885795514201533 Actual: 3.5\n",
      "Pred: 2.756293040830493 Actual: 3.0\n",
      "Pred: 2.9418813155582813 Actual: 1.0\n",
      "Pred: 3.9185160082267716 Actual: 4.0\n",
      "Pred: 3.993940170984635 Actual: 4.0\n",
      "Pred: 3.4804241653433494 Actual: 4.0\n",
      "Pred: 2.968682565974867 Actual: 2.0\n",
      "Pred: 2.522788657627762 Actual: 2.0\n",
      "Pred: 2.6212975767867466 Actual: 3.0\n",
      "Pred: 3.8117477715440393 Actual: 4.0\n",
      "Pred: 3.1675541425720546 Actual: 3.0\n",
      "Pred: 3.004154942756056 Actual: 1.0\n",
      "Pred: 2.886667019067914 Actual: 4.0\n",
      "Pred: 3.1968464804685532 Actual: 3.0\n",
      "overall score: 0.25892058101787907\n",
      "Minutes: 0.3739824612935384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "#test the time taken to train and predict\n",
    "start = time.time()\n",
    "\n",
    "#this is where you may select certain features to be used to build the model\n",
    "new_user_to_features = dict()\n",
    "\n",
    "for key in user_to_features.keys():\n",
    "    new_list = []\n",
    "    for item in user_to_features[key]:\n",
    "        # can try reducing the features like below:\n",
    "        # old inputs...\n",
    "        # item[0:4]+ item[6:7]+ item[10:11]\n",
    "        # item[0:4]\n",
    "        # item[2:8] + item[8:]\n",
    "        #new inputs:\n",
    "        \n",
    "        new_list.append(item)\n",
    "    new_user_to_features[key] = new_list\n",
    "\n",
    "#seed\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#instead of using test train split...\n",
    "user_to_X_train = dict()\n",
    "user_to_y_train = dict()\n",
    "user_to_X_test = dict()\n",
    "user_to_y_test = dict()\n",
    "\n",
    "#There is a problem with using the same users in training and testing and this code ensures that it doesn't happen\n",
    "#The model should beable to be used effectively for new users and not just memorized for existing users\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "for key in new_user_to_features.keys():\n",
    "    if(random.randint(0,10) == 0):\n",
    "        user_to_X_test[key] = new_user_to_features[key]\n",
    "        user_to_y_test[key] = user_to_rand_rating[key]\n",
    "        c1+=1\n",
    "\n",
    "    else:\n",
    "        user_to_X_train[key] = new_user_to_features[key]\n",
    "        user_to_y_train[key] = user_to_rand_rating[key]\n",
    "        c2+=1\n",
    "\n",
    "#used to train model\n",
    "X_train = [] \n",
    "y_train = []\n",
    "\n",
    "#populate X_train and y_train\n",
    "for key in user_to_X_train.keys():\n",
    "    for item in user_to_X_train[key]:\n",
    "        X_train.append(item)\n",
    "        y_train.append(user_to_y_train[key])\n",
    "\n",
    "\n",
    "# scale training features...\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "\n",
    "#data transformation\n",
    "#https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer\n",
    "\n",
    "\n",
    "#train model\n",
    "#orginal model layers\n",
    "#layers = (2,2,2)\n",
    "layers = (2,2,2)\n",
    "# act = \"tanh\"\n",
    "# solve = \"adam\"\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# act = \"relu\"\n",
    "# solve = \"sgd\"\n",
    "# act = \"tanh\"\n",
    "# solve = \"sgd\"\n",
    "act = \"relu\"\n",
    "solve = \"adam\"\n",
    "\n",
    "\n",
    "regr = MLPRegressor(hidden_layer_sizes=layers,activation =act, solver =solve,  max_iter=10000, random_state =seed_int)\n",
    "fitted = regr.fit(X_train, y_train)\n",
    "\n",
    "#this needs to run before the final model is determined so that the best features are used\n",
    "#the results can also be displayed with a bar shart showing how each feature cotributes to a percentage of the models accuracy\n",
    "result = permutation_importance(fitted, X_train, y_train,random_state=seed_int)\n",
    "\n",
    "print(result[\"importances_mean\"])\n",
    "\n",
    "print(regr.n_iter_)\n",
    "\n",
    "#dictionary of users to test features that have been scaled\n",
    "new_user_to_X_test = dict()\n",
    "\n",
    "# used to scale test features then the new scaled features are returned ...\n",
    "# as the values of the approriate user key in new_user_to_X_test \n",
    "X_test = []\n",
    "\n",
    "#populate X_test, key, and counts that are later used to build new_user_to_X_test, a verison of...\n",
    "#user_to_X_test with scaled features \n",
    "#need to decompose then recompose\n",
    "keys = []\n",
    "counts = []\n",
    "for key in user_to_X_test.keys():\n",
    "    cnt = 0\n",
    "    for item in user_to_X_test[key]:\n",
    "        X_test.append(item)\n",
    "        cnt+=1\n",
    "    counts.append(cnt)\n",
    "    keys.append(key)\n",
    "\n",
    "#scale test features...\n",
    "scalar = StandardScaler()\n",
    "X_test = scalar.fit_transform(X_test)\n",
    "\n",
    "#populate new_user_to_X_test with scaled test features\n",
    "cnt = 0\n",
    "for num, key in zip(counts, keys):\n",
    "    new_user_to_X_test[key] = []\n",
    "    for i in range(num):\n",
    "        new_user_to_X_test[key].append(X_test[cnt])\n",
    "        cnt+=1\n",
    "\n",
    "\n",
    "# user id to the average predicted rating for the randomly chosen movie\n",
    "user_to_avg_rating = dict()\n",
    "\n",
    "# populate user_to_avg_rating by averaging the predictions from all the feature inputs of the...\n",
    "# movies a user has watched that are not the randomly chosen movie itself\n",
    "for key in new_user_to_X_test.keys():\n",
    "    sum =0\n",
    "    cnt =0 \n",
    "    predicted = regr.predict(new_user_to_X_test[key])\n",
    "    for item in predicted:\n",
    "        sum+=item\n",
    "        cnt+=1\n",
    "    user_to_avg_rating[key] = float(sum/cnt)\n",
    "\n",
    "\n",
    "#outputs\n",
    "actuals_list = []\n",
    "preds_list = []\n",
    "for key in user_to_avg_rating.keys():\n",
    "    print(\"Pred: \"+str(user_to_avg_rating[key]) , \"Actual: \"+str(user_to_y_test[key]))\n",
    "    actuals_list.append(user_to_y_test[key])\n",
    "    preds_list.append(user_to_avg_rating[key])\n",
    "print(\"overall score:\", r2_score(actuals_list, preds_list))\n",
    "\n",
    "#test the time taken to train and predict\n",
    "end = time.time()\n",
    "print(\"Minutes:\", float((end - start)/60))\n",
    "\n",
    "\n",
    "#feature importance scores:\n",
    "#https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance\n",
    "\n",
    "#introduction:\n",
    "#https://www.kaggle.com/code/dansbecker/permutation-importance\n",
    "\n",
    "#types of feature importance:\n",
    "#https://towardsdatascience.com/6-types-of-feature-importance-any-data-scientist-should-master-1bfd566f21c9\n",
    "\n",
    "\n",
    "#perhaps there is a way to visualize this of the model outputs below in a systematic way???\n",
    "\n",
    "# Tests:\n",
    "\n",
    "# full features:\n",
    "# with linear regression:\n",
    "# overall score: 0.2657455495660592\n",
    "\n",
    "# with mlp...:\n",
    "\n",
    "# first fours features:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2667063296881431\n",
    "\n",
    "#all features: \n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.26606490897808244\n",
    "\n",
    "#all features: \n",
    "# layers: (2,2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.260461734737799\n",
    "\n",
    "#all features: \n",
    "# layers: (4,4,4)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.22932745175064528\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2482634902547255\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (2,2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2616102684471122\n",
    "\n",
    "# first fours features and variance:\n",
    "# layers: (3,3,3)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.25487207187202243\n",
    "\n",
    "#first two featurs:\n",
    "# layers: (2,2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: -0.00430015574935827\n",
    "\n",
    "#first two featurs:\n",
    "# layers: (2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.04468421358737418\n",
    "\n",
    "#3rd and 4th features:\n",
    "# layers: (2,2)\n",
    "# act = \"relu\"\n",
    "# solve = \"adam\"\n",
    "# overall score: 0.2546480453878024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
