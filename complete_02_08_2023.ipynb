{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as random\n",
    "from pandas import unique\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('newdata/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "ratings = pd.read_csv('newdata/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('newdata/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"newdata/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "#new:\n",
    "#userIds are in order\n",
    "# every_id = list(unique(list(complete[\"userId\"])))\n",
    "#userIds are out of order\n",
    "# sample_ids  = random.sample(every_id, 1000)\n",
    "# completeNew = pd.DataFrame()\n",
    "#This is a very expensive task...\n",
    "#it is possile to choose a subset of users from here instead of\n",
    "#ibcluing  the entire set of users\n",
    "# for user in sample_ids:\n",
    "#     completeNew = pd.concat([completeNew, complete.loc[complete[\"userId\"] == user]])\n",
    "\n",
    "#new:\n",
    "# complete = complete.sample(frac=1, random_state = seed_int, axis =0)\n",
    "#this is not the desired behavior\n",
    "#the users ids need to show up in a true random order\n",
    "# complete = complete.groupby(by = \"userId\", sort = False, group_keys = True).apply(lambda x: x)\n",
    "#this is omitted since the values should not be sorted by userId just grouped by userId\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "\n",
    "# print(complete.head())\n",
    "\n",
    "# f = open(\"test_dicts.txt\", \"w\", encoding=\"utf-8\")\n",
    "# f.write(str(list(complete[\"tagline\"])))\n",
    "# f.write(str(list(complete[\"overview\"])))\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "29.325632563256324\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 1\n",
    "random.seed(seed_int)\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    #this is probably not needed due to the dropNa function used above...\n",
    "    # length = len(array[8])\n",
    "    # if(array[8][length-4:]==\"<NA>\"):\n",
    "    #     return False\n",
    "    # length = len(array[9])\n",
    "    # if(array[9][length-4:]==\"<NA>\"):\n",
    "    #     return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "#convert the dataframe into an array and build a dictionary\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "\n",
    "gaps = []\n",
    "size = 0\n",
    "list_of_user_ids = []\n",
    "last_id  = -1\n",
    "past_first_it = False\n",
    "\n",
    "\n",
    "#there seems to be a technical problem with gaps...\n",
    "for row in complete_array:\n",
    "    #need to omit the first iteration for gaps\n",
    "    if(row[0]!= last_id):\n",
    "        list_of_user_ids.append(row[0])\n",
    "        last_id = row[0]\n",
    "        if(past_first_it ==True):\n",
    "            gaps.append(size)\n",
    "            size =0 \n",
    "    size+=1\n",
    "    past_first_it = True\n",
    "\n",
    "#there is always a gap for the last iteration\n",
    "gaps.append(size)\n",
    "\n",
    "\n",
    "index  = 0\n",
    "user_to_data = dict()\n",
    "#this is the total number of users in the whole dataset\n",
    "total_nof_users = 261306\n",
    "#this is the number of desired users before filtering\n",
    "selected_nof_users_before_filter = 10000\n",
    "\n",
    "avg =0\n",
    "cnt =0\n",
    "\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, total_nof_users):\n",
    "    #generate a random float to determine a pass for the user\n",
    "    if (random.random()<float(selected_nof_users_before_filter/total_nof_users)):\n",
    "        user_to_data[list_of_user_ids[i]] = []\n",
    "        for j in range(index, len(complete_array)):\n",
    "            if complete_array[j][0] == list_of_user_ids[i]:\n",
    "                #condition is checked for complete_array[j]\n",
    "                if(condition(complete_array[j])):\n",
    "                    #this is where data is tranformed\n",
    "                    transformed = provide_data(complete_array[j])\n",
    "                    user_to_data[list_of_user_ids[i]].append(transformed)         \n",
    "            else:\n",
    "                avg += len(user_to_data[list_of_user_ids[i]])\n",
    "                cnt+=1\n",
    "                #this condition can be tweaked for better accuracy\n",
    "                if (len(user_to_data[list_of_user_ids[i]])<50):\n",
    "                    del user_to_data[list_of_user_ids[i]]\n",
    "                #note: changed from (index = j+1)\n",
    "                index = j\n",
    "                break\n",
    "    else:\n",
    "        #every iteration, index starts at first data point of the next user\n",
    "        index += gaps[i]\n",
    "\n",
    "#needs to be sure that there are enough users after the condiiton\n",
    "print(len(list(user_to_data.keys())))\n",
    "\n",
    "\n",
    "#average number of ratings per user\n",
    "print(float(avg/cnt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "import csv\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for key in user_to_data.keys():\n",
    "        writer.writerows(user_to_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructedData.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructedData/constructedData.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ordered_set import OrderedSet\n",
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#user to data rows \n",
    "user_to_data = dict()\n",
    "user_to_data_train = dict()\n",
    "user_to_data_test = dict()\n",
    "\n",
    "user_id = -1\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_data[row[0]] = [row]\n",
    "    else:\n",
    "        user_to_data[row[0]].append(row)\n",
    "\n",
    "#this can be tweaked...\n",
    "for i in range(250):\n",
    "    user = random.choice(list(user_to_data.keys()))\n",
    "    user_to_data_train[user] = user_to_data[user]\n",
    "    user_to_data.pop(user)\n",
    "\n",
    "for i in range(50):\n",
    "    user = random.choice(list(user_to_data_train.keys()))\n",
    "    user_to_data_test[user] = user_to_data_train[user]\n",
    "    user_to_data_train.pop(user)\n",
    "\n",
    "\n",
    "user_to_data.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import json\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#includes omitted movies\n",
    "user_to_movie_id_to_corpus_train = dict()\n",
    "movies_in_order = OrderedSet()\n",
    "user_to_movie_id_to_rating = dict()\n",
    "movie_id_to_ratings = dict()\n",
    "movie_id_to_average_rating = dict()\n",
    "\n",
    "#this is what determines ommited movies\n",
    "user_to_rand_movie_id = dict()\n",
    "\n",
    "#used to create overall_average\n",
    "#omits movies with user_to_rand_movie_id\n",
    "overall_sum = 0\n",
    "overall_counts = 0 \n",
    "\n",
    "# WordNetLemmatizer().lemmatize(token.lower())\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for user in user_to_data_train.keys():\n",
    "    movie_strings = dict()\n",
    "    movie_id_to_rating_temp = dict()\n",
    "    cnt = 0\n",
    "    #does this cover all of 0-len(user_to_data_train[user])-1 ???\n",
    "    rand_int = random.randint(0, len(user_to_data_train[user])-1)\n",
    "    for movie_data in user_to_data_train[user]:\n",
    "        if cnt == rand_int:    \n",
    "            user_to_rand_movie_id[user] = movie_data[1]\n",
    "        else:\n",
    "            overall_sum += float(movie_data[2])\n",
    "            overall_counts += 1\n",
    "\n",
    "        #this also includes omitted ratings (the rating to be predicted by the model)\n",
    "        if movie_data[1] in movie_id_to_ratings.keys():\n",
    "            movie_id_to_ratings[movie_data[1]].append(float(movie_data[2]))\n",
    "        else:\n",
    "            movie_id_to_ratings[movie_data[1]] = [float(movie_data[2])]\n",
    "\n",
    "        movies_in_order.add(movie_data[1])\n",
    "        movie_string = \"\"\n",
    "        #avoid the first three data points (user id, movieid, and rating)\n",
    "        #use only the text data\n",
    "        for index in range (3,len(movie_data)):\n",
    "            if(index!= len(movie_data)-1):\n",
    "                movie_string+= movie_data[index]+\" \"\n",
    "            else:\n",
    "                movie_string+= movie_data[index]\n",
    "        cleaned = remove_stopwords(movie_string)\n",
    "        #why recreate the string when you could accept it as a list???\n",
    "        #lematize and form a list of words\n",
    "        cleaned = [wnl.lemmatize(word) for word in cleaned.split(\" \")]\n",
    "        #removes periods\n",
    "        cleaned = [word[:-1] for word in cleaned if word.endswith(\".\")]\n",
    "        movie_strings[movie_data[1]] = cleaned\n",
    "        movie_id_to_rating_temp[movie_data[1]] = float(movie_data[2])\n",
    "        cnt+=1\n",
    "    user_to_movie_id_to_corpus_train[user] = movie_strings\n",
    "    user_to_movie_id_to_rating[user] = movie_id_to_rating_temp\n",
    "\n",
    "overall_average = float(overall_sum/overall_counts)\n",
    "\n",
    "#there needs to be a replacement when there is only a single rating in the dataset for a movie\n",
    "#there is no problem if there are 2 or more ratings for a single movie\n",
    "#this replacement can be the overall average of all ratings or perhaps some guess based on the users\n",
    "#other ratings \n",
    "\n",
    "for movie in movie_id_to_ratings.keys():\n",
    "    temp = 0\n",
    "    for rating in movie_id_to_ratings[movie]:\n",
    "        temp +=rating\n",
    "    movie_id_to_average_rating[movie] = float(temp/len(movie_id_to_ratings[movie]))\n",
    "\n",
    "\n",
    "#testing different encodings and seeing what the text data looks like...\n",
    "# file = open(\"test_dicts.txt\", 'w')\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_train))\n",
    "# file.close()\n",
    "\n",
    "# file = open(\"test_dicts2.txt\", 'w', encoding=\"utf-8\")\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_train))\n",
    "# file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012429588430375227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import statistics\n",
    "import math\n",
    "from ordered_set import OrderedSet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#these 2 data strctures all have omited values from user_to_rand_movie_id[user]\n",
    "user_to_word_counts = dict()\n",
    "words_in_order = OrderedSet()\n",
    "\n",
    "#the ommited movie rating is marked with -2 and the unrated movies to be filled in are marked with -1 \n",
    "user_to_ratings = dict()\n",
    "\n",
    "#this is the user to word counts of the omitted movies with user_to_rand_movie_id[user]\n",
    "user_to_rand_word_counts = dict()\n",
    "\n",
    "#this store the actual ratings of movies to be predicted by the model\n",
    "user_to_rating_to_predict = dict()\n",
    "\n",
    "\n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    temp = []\n",
    "    #note: length of user_to_ratings[user] is always equal to len(movies_in_order)\n",
    "    for movie_id in movies_in_order:\n",
    "        if movie_id != user_to_rand_movie_id[user]:\n",
    "            if movie_id in user_to_movie_id_to_rating[user].keys():\n",
    "                temp.append(user_to_movie_id_to_rating[user][movie_id])\n",
    "            else:\n",
    "                #this signifies the ratings to be filled in before predictions from the model take place\n",
    "                temp.append(-1)\n",
    "        else:\n",
    "            #this signifies the ratings to be predicted by the model\n",
    "            temp.append(-2)\n",
    "            user_to_rating_to_predict[user] = user_to_movie_id_to_rating[user][movie_id]\n",
    "    user_to_ratings[user] = temp\n",
    "\n",
    "\n",
    "#note: words_in_order has ommited words from a single movie (user_to_rand_movie_id[user])\n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    for movie_id in user_to_movie_id_to_corpus_train[user].keys():\n",
    "        if movie_id != user_to_rand_movie_id[user]:\n",
    "            for word in user_to_movie_id_to_corpus_train[user][movie_id]:\n",
    "                words_in_order.add(word)\n",
    "\n",
    "            \n",
    "\n",
    "#note: user_to_word_counts and movie_id_to_word_counts have an omitted movies...\n",
    "#by user_to_rand_movie_id[user]\n",
    "\n",
    "#note: user to word counts needs to omit movies with no rating\n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    user_to_word_counts[user] = []\n",
    "    for movie_id in user_to_movie_id_to_corpus_train[user].keys():\n",
    "        if movie_id != user_to_rand_movie_id[user]:\n",
    "            temp = []\n",
    "            for word in words_in_order:\n",
    "                temp.append(user_to_movie_id_to_corpus_train[user][movie_id].count(word))\n",
    "            user_to_word_counts[user].append(temp)\n",
    "        else:\n",
    "            temp = []\n",
    "            for word in words_in_order:\n",
    "                temp.append(user_to_movie_id_to_corpus_train[user][movie_id].count(word))\n",
    "            user_to_rand_word_counts[user] = temp\n",
    "\n",
    "#Q: how do we know if user_to_word_counts[user] and user_to_ratings[user] are in matching order???\n",
    "#user_to_ratings[user] is in the order of movies_in_order\n",
    "#user_to_word_counts[user] is in the order of user_to_movie_id_to_corpus_train[user] omiting the chosen movie\n",
    "\n",
    "\n",
    "\n",
    "#what if a new user to be tested has new words that are new to words_in_order???\n",
    "#what if a user simply has more words than normal, need to normalize(not this is doen with cossine similairity)\n",
    "#what if a user rated a movie that has not been rated by any users??? (is it omited ???)\n",
    "#do the randomly selected users meed to be omited from the words_in_order and user_to_word_counts\n",
    "#to emulate completely new users???\n",
    "\n",
    "\n",
    "def predict(user, word_counts):\n",
    "    values = []\n",
    "    #note: user_to_word_counts[user] length is equal to the number of movies rated by the user...\n",
    "    #minus one for the user_to_rand_movie_id[user]\n",
    "\n",
    "    #note: user to word counts needs to omit movies with no rating\n",
    "    for counts in user_to_word_counts[user]:\n",
    "        values.append(counts)\n",
    "    \n",
    "    cosine_sim = cosine_similarity(X = values ,Y = [word_counts])\n",
    "    #not sure if reshape is needed???\n",
    "    cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "\n",
    "    ratings = []\n",
    "    #note: user_to_ratings[user] length is equal to the number of movies\n",
    "    #the length of ratings is the number of rated movies -1 for user_to_rand_movie_id[user]\n",
    "    for rating in user_to_ratings[user]:\n",
    "        if(rating != -1 and rating != -2):\n",
    "            ratings.append(rating)\n",
    "\n",
    "\n",
    "    #note: for accuracy purposes, the user should have a certain number of rated movies\n",
    "    #instead of selecting the most similair movie should clusters be used???\n",
    "    #for now this simple method will suffice\n",
    "    #are the ratings lined up with the cossine similarities???\n",
    "    #yes the order of user_to_ratings is used to make user_to_corpus_list...\n",
    "    #which is used to make lst_of_word_to_nof_occurances\n",
    "\n",
    "    combined = zip(cosine_sim, ratings)\n",
    "    combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "    avg = 0\n",
    "    for i in range(0, 10):\n",
    "        avg += combined[i][1]\n",
    "\n",
    "\n",
    "    return float(avg/10.0)\n",
    "\n",
    "\n",
    "#fill in the missing ratings for a user\n",
    "def fill_in_ratings(user):\n",
    "    cnt = 0\n",
    "    pred = 0\n",
    "    #note: it is possible that movies in order includes some movies that are not part of user_to_ratings[user]\n",
    "    #due to the randomly omitted movie for each user in user_to_ratings\n",
    "    \n",
    "    for rating in user_to_ratings[user]:\n",
    "        if rating == -2:\n",
    "            user_to_ratings[user][cnt] = predict(user, user_to_rand_word_counts[user])\n",
    "            pred = user_to_ratings[user][cnt]\n",
    "            break\n",
    "        cnt+=1\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "predicted = []\n",
    "true = []\n",
    "for user in user_to_movie_id_to_corpus_train.keys():\n",
    "    # commented out for testing\n",
    "    predicted.append(fill_in_ratings(user))\n",
    "    true.append(user_to_rating_to_predict[user])\n",
    "\n",
    "# commented out for testing\n",
    "print(r2_score(true, predicted))\n",
    "\n",
    "\n",
    "\n",
    "# commented out for testing\n",
    "# predicted = []\n",
    "# for user in user_to_movie_id_to_corpus_train.keys(): \n",
    "#     if(len(movie_id_to_ratings[user_to_rand_movie_id[user]])==1):\n",
    "#         predicted.append(overall_average)\n",
    "#     else:\n",
    "#         predicted.append(float(((movie_id_to_average_rating[user_to_rand_movie_id[user]]\n",
    "#                         *len(movie_id_to_ratings[user_to_rand_movie_id[user]]))\n",
    "#                         -user_to_movie_id_to_rating[user][user_to_rand_movie_id[user]])\n",
    "#                         /(len(movie_id_to_ratings[user_to_rand_movie_id[user]])-1)))\n",
    "        \n",
    "# nof_overall_avg = 0\n",
    "# for item in predicted:\n",
    "#     if item == overall_average:\n",
    "#         nof_overall_avg += 1\n",
    "# print(nof_overall_avg)\n",
    "# print(r2_score(true, predicted))\n",
    "\n",
    "\n",
    "#now for the user comparison logic (need user to list of movie ratings)\n",
    "#fill in ratings that the user hasn't watched with the method above\n",
    "#then cluster the users by their ratings\n",
    "\n",
    "#note: agglomerative clustering might make more sense here since k-means has random init for centroids...\n",
    "#note: to guess a new users rating requires that none of that users ratings have been used to train the model\n",
    "#The data needs to be split into test and train before modeling the algorithm on the train data\n",
    "\n",
    "#Training process:\n",
    "#split data into test and train data\n",
    "#proceed with train data...\n",
    "#cluster movies by the tokens with range for k\n",
    "#cluster users by the ratings with range for k and (fill in ratings for movies a users hasn't watched with some guess)\n",
    "#guess: this can be obtained by clustering the movies that the user has watched...\n",
    "#for each movie the user hasn't watched find the cluster that it belongs to with the highest possible k value\n",
    "#that the user has at least one movie belonging to one of the clusters and then take the average of those movies\n",
    "#this is exactly like a later training step excpet it is applied to all the movies the user watched\n",
    "\n",
    "#for a single randomly chosen movie from each user in the trainging data...\n",
    "\n",
    "#find the cluster the movie belongs to \n",
    "#find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#take the average score of these movies\n",
    "#find the cluster the user belongs to\n",
    "#find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#train an mlp model with both averages and perhaps some extra statistics as features...\n",
    "#using the given movie ratings as actuals\n",
    "\n",
    "\n",
    "#The process of predicting a rating:\n",
    "#1. find the cluster the movie belongs to \n",
    "#2. find the movies part of that same cluster that the user has scored at the highest possible k value\n",
    "#3. take the average score of these movies\n",
    "#4. find the cluster the user belongs to\n",
    "#5. find the average rating of the movie for users in that cluster at the highest possible k value\n",
    "#6. input into the trained mlp model both averages and perhaps some extra statistics\n",
    "#7. make predictions and test against the randomly chosen movies actual ratings\n",
    "\n",
    "\n",
    "#summary:\n",
    "#find cluster for movie -> find movies part of the same clusters that the users rated -> average\n",
    "#question: are the clusters unique to the movies the user has watched or to all movies???\n",
    "#what is the technical difference???\n",
    "#is this the same as finding the most simimlair movie the user rated and copying the rating???\n",
    "\n",
    "#find cluster for user -> find the ratings for the movie by people in the same cluster -> average\n",
    "\n",
    "#other avenues considered:\n",
    "#idea 1:\n",
    "#for the first process, instead of averaging the movies that only the user rated, find other users that are...\n",
    "#like the user in question and find the average for that movie cluster\n",
    "#Problem: it is better to get the users raw opionion rather than generalizing it to some like minded users\n",
    "#there is an extra costly step to this\n",
    "#idea 2: \n",
    "#for the second process, instead of finding the average rating for the movie in the same cluster of users...\n",
    "#also find the average rating of movies that are like the movie in question \n",
    "#Problem, it is better to get the movies rating itself as it would be the most accurate indicator\n",
    "#there is an extra costly step to this\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test with overall averges:\n",
    "#note: users have between 50 and 75 ratings each\n",
    "#note: there are 250 users who are taken into account \n",
    "#note: seed int is one for above cells (cells part of creating the csv file)\n",
    "\n",
    "#with seed int == 3, 4 taking overall averages: 0.09565753948597455\n",
    "#with seed_int == 1, 2 taking overall averages: 0.070404868516315\n",
    "#with seed_int == 2, 5 taking overall averages: 0.11310085954932936\n",
    "#with seed_int == 4, 6 taking overall averages: 0.07125374341347135\n",
    "#with seed_int == 5, 4 taking overall averages: 0.17736444913943628\n",
    "#compute time: 11 minutes\n",
    "\n",
    "\n",
    "#test with users related movies:\n",
    "#is there a magic proportion of movies to average???\n",
    "#note: this is taking around the same time as the above tests meaning \n",
    "#there could be more users to include in analysis with little increase in runtime\n",
    "#k fold cross validation could be effective\n",
    "#https://www.youtube.com/watch?v=TIgfjmp-4BA&ab_channel=Udacity\n",
    "\n",
    "#effect of choosing a random seed...\n",
    "#https://towardsdatascience.com/how-to-use-random-seeds-effectively-54a4cd855a79\n",
    "\n",
    "#try tinkering with the number of similair movies to average\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#with number of movies to average: 20\n",
    "#upper seed_int = 1\n",
    "#lower seed_int = 2\n",
    "#with less train users...\n",
    "#0.13183385544822857\n",
    "#with 350 train users...\n",
    "#0.04683280709667714\n",
    "#why does accuracy drastically plumet with more users???\n",
    "#among users with 50+ ratings\n",
    "#0.07241396266206268\n",
    "\n",
    "#with number of movies to average: 10...\n",
    "#upper seed_int = 1\n",
    "#lower seed_int = 2\n",
    "#among users with 50+ ratings\n",
    "#200 users\n",
    "#0.012429588430375227\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
