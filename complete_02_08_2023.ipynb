{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20404\n",
      "Int64Index([    1,     2,     3,     4,     5,     6,     7,     8,     9,\n",
      "               10,\n",
      "            ...\n",
      "            45437, 45438, 45439, 45441, 45449, 45454, 45456, 45458, 45461,\n",
      "            45463],\n",
      "           dtype='int64', length=20404)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[306, 347, 432, 473, 477, 496, 559, 566, 613, 770, 821, 977, 1027, 1076, 1103, 1105, 1254, 1315, 1562, 1617, 1657, 1962, 1964, 1980, 2020, 2118, 2127, 2156, 2177, 2247, 2367, 2425, 2448, 2466, 2582, 2650, 2723, 2847, 2860, 2928, 2998, 3022, 3023, 3038, 3063, 3124, 3148, 3202, 3259, 3410, 3467, 3556, 3757, 3790, 3971, 4034, 4053, 4086, 4089, 4118, 4144, 4159, 4221, 4236, 4251, 4254, 4277, 4321, 4421, 4461, 4463, 4475, 4476, 4507, 4517, 4558, 4574, 4690, 4882, 4920, 4938, 5099, 5119, 5121, 5124, 5154, 5190, 5205, 5288, 5305, 5316, 5349, 5369, 5403, 5502, 5602, 5673, 5677, 5700, 5718, 5750, 5756, 5790, 5791, 5813, 5844, 5888, 5924, 5937, 5952, 5979, 6285, 6314, 6317, 6409, 6476, 6477, 6491, 6519, 6522, 6524, 6527, 6537, 6543, 6544, 6545, 6548, 6574, 6626, 6633, 6768, 6779, 6780, 6781, 6820, 6843, 6859, 6917, 6982, 7004, 7134, 7141, 7232, 7235, 7240, 7280, 7315, 7316, 7389, 7578, 7655, 7758, 7773, 7892, 7894, 7900, 7957, 7962, 7964, 8003, 8052, 8103, 8170, 8203, 8225, 8261, 8305, 8309, 8355, 8384, 8385, 8551, 8561, 8664, 8705, 8737, 8747, 8764, 8783, 8785, 8789, 8796, 8813, 8846, 8879, 8994, 8999, 9021, 9030, 9041, 9076, 9080, 9110, 9122, 9123, 9146, 9197, 9223, 9237, 9250, 9252, 9254, 9261, 9290, 9308, 9328, 9331, 9361, 9384, 9400, 9409, 9463, 9468, 9496, 9519, 9527, 9528, 9560, 9616, 9621, 9640, 9641, 9644, 9656, 9662, 9679, 9682, 9683, 9707, 9719, 9734, 9740, 9782, 9807, 9816, 9823, 9836, 9851, 9864, 9870, 9874, 9877, 9891, 9912, 9915, 9966, 9970, 9971, 9974, 9997, 10074, 10077, 10080, 10094, 10102, 10125, 10171, 10185, 10194, 10211, 10218, 10246, 10251, 10259, 10268, 10310, 10377, 10379, 10403, 10434, 10499, 10501, 10522, 10527, 10534, 10551, 10570, 10573, 10603, 10653, 10670, 10791, 10798, 10855, 10860, 10862, 10877, 10883, 10907, 10945, 10953, 10992, 11016, 11036, 11042, 11076, 11088, 11101, 11126, 11147, 11153, 11154, 11175, 11181, 11211, 11220, 11226, 11240, 11265, 11297, 11338, 11341, 11367, 11368, 11384, 11396, 11420, 11447, 11450, 11451, 11496, 11506, 11510, 11515, 11524, 11526, 11533, 11552, 11566, 11571, 11574, 11581, 11584, 11616, 11622, 11624, 11630, 11638, 11644, 11665, 11680, 11697, 11703, 11754, 11756, 11778, 11788, 11794, 11798, 11803, 11831, 11841, 11847, 11872, 11885, 11894, 11904, 11909, 11912, 11950, 11982, 11999, 12012, 12029, 12037, 12069, 12072, 12075, 12088, 12103, 12115, 12117, 12122, 12125, 12153, 12161, 12167, 12175, 12197, 12207, 12213, 12243, 12247, 12263, 12357, 12380, 12397, 12407, 12409, 12433, 12435, 12442, 12458, 12466, 12475, 12480, 12520, 12523, 12527, 12528, 12531, 12532, 12533, 12538, 12550, 12565, 12607, 12619, 12630, 12631, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12667, 12676, 12697, 12698, 12726, 12738, 12745, 12756, 12764, 12770, 12780, 12781, 12783, 12795, 12796, 12798, 12804, 12822, 12823, 12831, 12834, 12837, 12841, 12852, 12859, 12861, 12865, 12880, 12887, 12902, 12903, 12937, 12947, 12958, 12962, 12971, 12989, 13002, 13005, 13022, 13039, 13081, 13160, 13164, 13166, 13168, 13169, 13176, 13226, 13247, 13301, 13338, 13348, 13361, 13363, 13407, 13424, 13438, 13453, 13454, 13461, 13478, 13483, 13515, 13533, 13538, 13551, 13634, 13661, 13668, 13709, 13716, 13729, 13730, 13731, 13763, 13766, 13781, 13782, 13789, 13821, 13823, 13895, 13897, 13912, 13917, 13925, 13933, 13939, 13962, 13988, 13990, 14000, 14024, 14027, 14029, 14033, 14057, 14081, 14090, 14116, 14123, 14129, 14140, 14150, 14158, 14182, 14233, 14241, 14248, 14254, 14273, 14284, 14286, 14325, 14330, 14338, 14350, 14364, 14371, 14381, 14385, 14393, 14418, 14429, 14432, 14437, 14440, 14443, 14444, 14446, 14454, 14459, 14460, 14466, 14471, 14474, 14479, 14513, 14517, 14522, 14524, 14534, 14559, 14569, 14579, 14600, 14626, 14636, 14643, 14646, 14652, 14659, 14660, 14674, 14678, 14682, 14691, 14709, 14715, 14716, 14749, 14753, 14756, 14775, 14783, 14791, 14803, 14808, 14864, 14905, 14906, 14912, 14930, 14933, 14944, 14962, 14967, 14982, 14986, 14989, 14998, 15001, 15002, 15016, 15033, 15035, 15038, 15042, 15052, 15056, 15068, 15072, 15076, 15095, 15109, 15124, 15135, 15140, 15147, 15187, 15204, 15207, 15210, 15238, 15240, 15245, 15246, 15249, 15253, 15259, 15275, 15309, 15329, 15359, 15367, 15382, 15427, 15429, 15431, 15434, 15444, 15456, 15492, 15497, 15509, 15517, 15518, 15525, 15533, 15537, 15546, 15547, 15550, 15551, 15556, 15560, 15566, 15574, 15590, 15591, 15601, 15603, 15607, 15614, 15620, 15632, 15651, 15654, 15673, 15674, 15677, 15686, 15690, 15691, 15697, 15698, 15700, 15703, 15704, 15705, 15711, 15718, 15744, 15747, 15785, 15786, 15787, 15788, 15789, 15793, 15805, 15808, 15818, 15819, 15842, 15865, 15885, 15891, 15909, 15923, 15928, 15936, 15939, 15949, 15953, 15957, 15959, 15963, 15981, 15992, 15996, 15997, 15998, 16005, 16006, 16008, 16012, 16017, 16023, 16024, 16027, 16037, 16044, 16060, 16069, 16070, 16071, 16085, 16096, 16098, 16099, 16103, 16104, 16122, 16125, 16146, 16148, 16151, 16152, 16164, 16185, 16186, 16198, 16212, 16237, 16238, 16258, 16266, 16268, 16285, 16300, 16303, 16306, 16308, 16313, 16323, 16335, 16348, 16370, 16373, 16381, 16388, 16391, 16400, 16416, 16417, 16420, 16422, 16427, 16436, 16440, 16441, 16447, 16460, 16462, 16464, 16470, 16471, 16472, 16474, 16476, 16486, 16487, 16488, 16490, 16502, 16507, 16511, 16512, 16517, 16522, 16526, 16540, 16546, 16547, 16557, 16562, 16579, 16580, 16589, 16592, 16613, 16614, 16629, 16642, 16666, 16667, 16677, 16685, 16687, 16690, 16695, 16702, 16708, 16721, 16729, 16735, 16743, 16753, 16776, 16784, 16792, 16798, 16800, 16813, 16814, 16841, 16848, 16865, 16870, 16873, 16897, 16902, 16922, 16949, 16970, 16979, 16988, 16989, 16994, 16999, 17001, 17026, 17027, 17028, 17046, 17051, 17063, 17077, 17080, 17094, 17095, 17118, 17145, 17149, 17157, 17162, 17163, 17165, 17179, 17181, 17208, 17214, 17215, 17221, 17250, 17252, 17253, 17264, 17267, 17271, 17277, 17310, 17315, 17323, 17339, 17345, 17350, 17352, 17356, 17363, 17370, 17371, 17385, 17388, 17390, 17393, 17394, 17399, 17401, 17405, 17412, 17415, 17423, 17426, 17430, 17431, 17433, 17434, 17435, 17441, 17448, 17458, 17475, 17476, 17483, 17484, 17494, 17495, 17500, 17503, 17505, 17517, 17536, 17540, 17548, 17550, 17558, 17560, 17562, 17567, 17570, 17581, 17583, 17587, 17589, 17590, 17600, 17607, 17608, 17614, 17629, 17634, 17640, 17641, 17643, 17646, 17659, 17662, 17669, 17670, 17680, 17683, 17685, 17689, 17692, 17722, 17730, 17739, 17741, 17770, 17779, 17780, 17788, 17793, 17797, 17798, 17802, 17803, 17809, 17812, 17813, 17826, 17837, 17844, 17858, 17859, 17892, 17893, 17904, 17907, 17909, 17910, 17911, 17914, 17921, 17927, 17929, 17933, 17937, 17945, 17950, 17951, 17953, 17957, 17958, 17960, 17967, 17968, 17969, 17973, 17974, 17975, 17976, 17978, 17981, 17986, 17998, 18010, 18014, 18015, 18026, 18037, 18039, 18040, 18044, 18052, 18057, 18074, 18075, 18082, 18085, 18090, 18093, 18096, 18097, 18103, 18111, 18117, 18121, 18139, 18153, 18162, 18174, 18184, 18193, 18198, 18215, 18221, 18229, 18231, 18233, 18248, 18250, 18251, 18254, 18261, 18278, 18279, 18280, 18303, 18308, 18309, 18327, 18360, 18368, 18370, 18391, 18414, 18420, 18425, 18429, 18439, 18442, 18445, 18447, 18448, 18457, 18460, 18464, 18475, 18481, 18485, 18486, 18518, 18521, 18531, 18539, 18542, 18558, 18561, 18564, 18566, 18569, 18574, 18581, 18582, 18584, 18597, 18598, 18611, 18619, 18626, 18631, 18632, 18635, 18680, 18694, 18696, 18697, 18698, 18704, 18708, 18711, 18747, 18764, 18767, 18770, 18775, 18778, 18787, 18814, 18822, 18847, 18848, 18851, 18854, 18862, 18864, 18867, 18870, 18873, 18883, 18885, 18922, 18928, 18932, 18935, 18941, 18944, 18949, 18960, 18977, 18986, 18991, 18993, 19003, 19026, 19042, 19055, 19060, 19062, 19067, 19068, 19074, 19075, 19076, 19092, 19116, 19117, 19126, 19132, 19143, 19149, 19164, 19179, 19184, 19195, 19202, 19222, 19235, 19241, 19249, 19262, 19263, 19264, 19273, 19280, 19281, 19299, 19302, 19315, 19317, 19329, 19335, 19365, 19370, 19371, 19392, 19409, 19410, 19415, 19427, 19438, 19443, 19451, 19465, 19475, 19481, 19483, 19505, 19510, 19524, 19526, 19527, 19531, 19533, 19536, 19537, 19562, 19577, 19578, 19600, 19622, 19632, 19633, 19641, 19645, 19647, 19652, 19658, 19662, 19664, 19665, 19678, 19679, 19689, 19692, 19704, 19711, 19712, 19728, 19747, 19768, 19770, 19773, 19785, 19793, 19801, 19803, 19813, 19816, 19840, 19849, 19855, 19865, 19866, 19870, 19876, 19877, 19878, 19879, 19885, 19899, 19900, 19909, 19911, 19912, 19922, 19923, 19925, 19933, 19934, 19936, 19943, 19944, 19950, 19962, 19965, 19983, 19997, 20000, 20001, 20002, 20009, 20011, 20018, 20019, 20031, 20037, 20044, 20053, 20101, 20105, 20116, 20131, 20147, 20153, 20156, 20157, 20159, 20164, 20169, 20170, 20188, 20192, 20194, 20202, 20205, 20213, 20229, 20231, 20237, 20238, 20239, 20243, 20252, 20253, 20255, 20258, 20275, 20296, 20305, 20311, 20312, 20313, 20317, 20319, 20323, 20327, 20329, 20343, 20344, 20346, 20349, 20354, 20365, 20380, 20395] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\repos\\movie_rec_proj\\complete_02_08_2023.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m   \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# movies_full = movies_full.drop(index=[1,3])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m movies_full \u001b[39m=\u001b[39m movies_full\u001b[39m.\u001b[39;49mdrop(index\u001b[39m=\u001b[39;49mdrop_rows, axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(movies_full))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/repos/movie_rec_proj/complete_02_08_2023.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m ratings \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mlarge_source_data/ratings.csv\u001b[39m\u001b[39m'\u001b[39m, usecols \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m\"\u001b[39m), dtype\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[1;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\pandas\\core\\frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5251\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   5252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   5253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5261\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5262\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5263\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5264\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5397\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5398\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5399\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5400\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5401\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5402\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5403\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5404\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5405\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5406\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5407\u001b[0m     )\n",
      "File \u001b[1;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4503\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4504\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4505\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4507\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4544\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4545\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4546\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4547\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4549\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4550\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6932\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6933\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6934\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6935\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6936\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: '[306, 347, 432, 473, 477, 496, 559, 566, 613, 770, 821, 977, 1027, 1076, 1103, 1105, 1254, 1315, 1562, 1617, 1657, 1962, 1964, 1980, 2020, 2118, 2127, 2156, 2177, 2247, 2367, 2425, 2448, 2466, 2582, 2650, 2723, 2847, 2860, 2928, 2998, 3022, 3023, 3038, 3063, 3124, 3148, 3202, 3259, 3410, 3467, 3556, 3757, 3790, 3971, 4034, 4053, 4086, 4089, 4118, 4144, 4159, 4221, 4236, 4251, 4254, 4277, 4321, 4421, 4461, 4463, 4475, 4476, 4507, 4517, 4558, 4574, 4690, 4882, 4920, 4938, 5099, 5119, 5121, 5124, 5154, 5190, 5205, 5288, 5305, 5316, 5349, 5369, 5403, 5502, 5602, 5673, 5677, 5700, 5718, 5750, 5756, 5790, 5791, 5813, 5844, 5888, 5924, 5937, 5952, 5979, 6285, 6314, 6317, 6409, 6476, 6477, 6491, 6519, 6522, 6524, 6527, 6537, 6543, 6544, 6545, 6548, 6574, 6626, 6633, 6768, 6779, 6780, 6781, 6820, 6843, 6859, 6917, 6982, 7004, 7134, 7141, 7232, 7235, 7240, 7280, 7315, 7316, 7389, 7578, 7655, 7758, 7773, 7892, 7894, 7900, 7957, 7962, 7964, 8003, 8052, 8103, 8170, 8203, 8225, 8261, 8305, 8309, 8355, 8384, 8385, 8551, 8561, 8664, 8705, 8737, 8747, 8764, 8783, 8785, 8789, 8796, 8813, 8846, 8879, 8994, 8999, 9021, 9030, 9041, 9076, 9080, 9110, 9122, 9123, 9146, 9197, 9223, 9237, 9250, 9252, 9254, 9261, 9290, 9308, 9328, 9331, 9361, 9384, 9400, 9409, 9463, 9468, 9496, 9519, 9527, 9528, 9560, 9616, 9621, 9640, 9641, 9644, 9656, 9662, 9679, 9682, 9683, 9707, 9719, 9734, 9740, 9782, 9807, 9816, 9823, 9836, 9851, 9864, 9870, 9874, 9877, 9891, 9912, 9915, 9966, 9970, 9971, 9974, 9997, 10074, 10077, 10080, 10094, 10102, 10125, 10171, 10185, 10194, 10211, 10218, 10246, 10251, 10259, 10268, 10310, 10377, 10379, 10403, 10434, 10499, 10501, 10522, 10527, 10534, 10551, 10570, 10573, 10603, 10653, 10670, 10791, 10798, 10855, 10860, 10862, 10877, 10883, 10907, 10945, 10953, 10992, 11016, 11036, 11042, 11076, 11088, 11101, 11126, 11147, 11153, 11154, 11175, 11181, 11211, 11220, 11226, 11240, 11265, 11297, 11338, 11341, 11367, 11368, 11384, 11396, 11420, 11447, 11450, 11451, 11496, 11506, 11510, 11515, 11524, 11526, 11533, 11552, 11566, 11571, 11574, 11581, 11584, 11616, 11622, 11624, 11630, 11638, 11644, 11665, 11680, 11697, 11703, 11754, 11756, 11778, 11788, 11794, 11798, 11803, 11831, 11841, 11847, 11872, 11885, 11894, 11904, 11909, 11912, 11950, 11982, 11999, 12012, 12029, 12037, 12069, 12072, 12075, 12088, 12103, 12115, 12117, 12122, 12125, 12153, 12161, 12167, 12175, 12197, 12207, 12213, 12243, 12247, 12263, 12357, 12380, 12397, 12407, 12409, 12433, 12435, 12442, 12458, 12466, 12475, 12480, 12520, 12523, 12527, 12528, 12531, 12532, 12533, 12538, 12550, 12565, 12607, 12619, 12630, 12631, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12667, 12676, 12697, 12698, 12726, 12738, 12745, 12756, 12764, 12770, 12780, 12781, 12783, 12795, 12796, 12798, 12804, 12822, 12823, 12831, 12834, 12837, 12841, 12852, 12859, 12861, 12865, 12880, 12887, 12902, 12903, 12937, 12947, 12958, 12962, 12971, 12989, 13002, 13005, 13022, 13039, 13081, 13160, 13164, 13166, 13168, 13169, 13176, 13226, 13247, 13301, 13338, 13348, 13361, 13363, 13407, 13424, 13438, 13453, 13454, 13461, 13478, 13483, 13515, 13533, 13538, 13551, 13634, 13661, 13668, 13709, 13716, 13729, 13730, 13731, 13763, 13766, 13781, 13782, 13789, 13821, 13823, 13895, 13897, 13912, 13917, 13925, 13933, 13939, 13962, 13988, 13990, 14000, 14024, 14027, 14029, 14033, 14057, 14081, 14090, 14116, 14123, 14129, 14140, 14150, 14158, 14182, 14233, 14241, 14248, 14254, 14273, 14284, 14286, 14325, 14330, 14338, 14350, 14364, 14371, 14381, 14385, 14393, 14418, 14429, 14432, 14437, 14440, 14443, 14444, 14446, 14454, 14459, 14460, 14466, 14471, 14474, 14479, 14513, 14517, 14522, 14524, 14534, 14559, 14569, 14579, 14600, 14626, 14636, 14643, 14646, 14652, 14659, 14660, 14674, 14678, 14682, 14691, 14709, 14715, 14716, 14749, 14753, 14756, 14775, 14783, 14791, 14803, 14808, 14864, 14905, 14906, 14912, 14930, 14933, 14944, 14962, 14967, 14982, 14986, 14989, 14998, 15001, 15002, 15016, 15033, 15035, 15038, 15042, 15052, 15056, 15068, 15072, 15076, 15095, 15109, 15124, 15135, 15140, 15147, 15187, 15204, 15207, 15210, 15238, 15240, 15245, 15246, 15249, 15253, 15259, 15275, 15309, 15329, 15359, 15367, 15382, 15427, 15429, 15431, 15434, 15444, 15456, 15492, 15497, 15509, 15517, 15518, 15525, 15533, 15537, 15546, 15547, 15550, 15551, 15556, 15560, 15566, 15574, 15590, 15591, 15601, 15603, 15607, 15614, 15620, 15632, 15651, 15654, 15673, 15674, 15677, 15686, 15690, 15691, 15697, 15698, 15700, 15703, 15704, 15705, 15711, 15718, 15744, 15747, 15785, 15786, 15787, 15788, 15789, 15793, 15805, 15808, 15818, 15819, 15842, 15865, 15885, 15891, 15909, 15923, 15928, 15936, 15939, 15949, 15953, 15957, 15959, 15963, 15981, 15992, 15996, 15997, 15998, 16005, 16006, 16008, 16012, 16017, 16023, 16024, 16027, 16037, 16044, 16060, 16069, 16070, 16071, 16085, 16096, 16098, 16099, 16103, 16104, 16122, 16125, 16146, 16148, 16151, 16152, 16164, 16185, 16186, 16198, 16212, 16237, 16238, 16258, 16266, 16268, 16285, 16300, 16303, 16306, 16308, 16313, 16323, 16335, 16348, 16370, 16373, 16381, 16388, 16391, 16400, 16416, 16417, 16420, 16422, 16427, 16436, 16440, 16441, 16447, 16460, 16462, 16464, 16470, 16471, 16472, 16474, 16476, 16486, 16487, 16488, 16490, 16502, 16507, 16511, 16512, 16517, 16522, 16526, 16540, 16546, 16547, 16557, 16562, 16579, 16580, 16589, 16592, 16613, 16614, 16629, 16642, 16666, 16667, 16677, 16685, 16687, 16690, 16695, 16702, 16708, 16721, 16729, 16735, 16743, 16753, 16776, 16784, 16792, 16798, 16800, 16813, 16814, 16841, 16848, 16865, 16870, 16873, 16897, 16902, 16922, 16949, 16970, 16979, 16988, 16989, 16994, 16999, 17001, 17026, 17027, 17028, 17046, 17051, 17063, 17077, 17080, 17094, 17095, 17118, 17145, 17149, 17157, 17162, 17163, 17165, 17179, 17181, 17208, 17214, 17215, 17221, 17250, 17252, 17253, 17264, 17267, 17271, 17277, 17310, 17315, 17323, 17339, 17345, 17350, 17352, 17356, 17363, 17370, 17371, 17385, 17388, 17390, 17393, 17394, 17399, 17401, 17405, 17412, 17415, 17423, 17426, 17430, 17431, 17433, 17434, 17435, 17441, 17448, 17458, 17475, 17476, 17483, 17484, 17494, 17495, 17500, 17503, 17505, 17517, 17536, 17540, 17548, 17550, 17558, 17560, 17562, 17567, 17570, 17581, 17583, 17587, 17589, 17590, 17600, 17607, 17608, 17614, 17629, 17634, 17640, 17641, 17643, 17646, 17659, 17662, 17669, 17670, 17680, 17683, 17685, 17689, 17692, 17722, 17730, 17739, 17741, 17770, 17779, 17780, 17788, 17793, 17797, 17798, 17802, 17803, 17809, 17812, 17813, 17826, 17837, 17844, 17858, 17859, 17892, 17893, 17904, 17907, 17909, 17910, 17911, 17914, 17921, 17927, 17929, 17933, 17937, 17945, 17950, 17951, 17953, 17957, 17958, 17960, 17967, 17968, 17969, 17973, 17974, 17975, 17976, 17978, 17981, 17986, 17998, 18010, 18014, 18015, 18026, 18037, 18039, 18040, 18044, 18052, 18057, 18074, 18075, 18082, 18085, 18090, 18093, 18096, 18097, 18103, 18111, 18117, 18121, 18139, 18153, 18162, 18174, 18184, 18193, 18198, 18215, 18221, 18229, 18231, 18233, 18248, 18250, 18251, 18254, 18261, 18278, 18279, 18280, 18303, 18308, 18309, 18327, 18360, 18368, 18370, 18391, 18414, 18420, 18425, 18429, 18439, 18442, 18445, 18447, 18448, 18457, 18460, 18464, 18475, 18481, 18485, 18486, 18518, 18521, 18531, 18539, 18542, 18558, 18561, 18564, 18566, 18569, 18574, 18581, 18582, 18584, 18597, 18598, 18611, 18619, 18626, 18631, 18632, 18635, 18680, 18694, 18696, 18697, 18698, 18704, 18708, 18711, 18747, 18764, 18767, 18770, 18775, 18778, 18787, 18814, 18822, 18847, 18848, 18851, 18854, 18862, 18864, 18867, 18870, 18873, 18883, 18885, 18922, 18928, 18932, 18935, 18941, 18944, 18949, 18960, 18977, 18986, 18991, 18993, 19003, 19026, 19042, 19055, 19060, 19062, 19067, 19068, 19074, 19075, 19076, 19092, 19116, 19117, 19126, 19132, 19143, 19149, 19164, 19179, 19184, 19195, 19202, 19222, 19235, 19241, 19249, 19262, 19263, 19264, 19273, 19280, 19281, 19299, 19302, 19315, 19317, 19329, 19335, 19365, 19370, 19371, 19392, 19409, 19410, 19415, 19427, 19438, 19443, 19451, 19465, 19475, 19481, 19483, 19505, 19510, 19524, 19526, 19527, 19531, 19533, 19536, 19537, 19562, 19577, 19578, 19600, 19622, 19632, 19633, 19641, 19645, 19647, 19652, 19658, 19662, 19664, 19665, 19678, 19679, 19689, 19692, 19704, 19711, 19712, 19728, 19747, 19768, 19770, 19773, 19785, 19793, 19801, 19803, 19813, 19816, 19840, 19849, 19855, 19865, 19866, 19870, 19876, 19877, 19878, 19879, 19885, 19899, 19900, 19909, 19911, 19912, 19922, 19923, 19925, 19933, 19934, 19936, 19943, 19944, 19950, 19962, 19965, 19983, 19997, 20000, 20001, 20002, 20009, 20011, 20018, 20019, 20031, 20037, 20044, 20053, 20101, 20105, 20116, 20131, 20147, 20153, 20156, 20157, 20159, 20164, 20169, 20170, 20188, 20192, 20194, 20202, 20205, 20213, 20229, 20231, 20237, 20238, 20239, 20243, 20252, 20253, 20255, 20258, 20275, 20296, 20305, 20311, 20312, 20313, 20317, 20319, 20323, 20327, 20329, 20343, 20344, 20346, 20349, 20354, 20365, 20380, 20395] not found in axis'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#Data source:\n",
    "#https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv\n",
    "\n",
    "#This code is for combining certain data from the necessary csv files into a single dataframe (complete)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_full = pd.read_csv('large_source_data/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={\"tagline\": \"string\", \"id\":\"string\", 'genres':\"string\", \"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\", \"production_companies\" :\"string\"})\n",
    "\n",
    "#condition is checked for: \"genres\",\"production_companies\",\"keywords\", \"cast\"\n",
    "#from movies_full: \"genres\", \"production_companies\"\n",
    "#from keywords: keywords\n",
    "#from credits: cast\n",
    "\n",
    "movies_full = movies_full.dropna()\n",
    "\n",
    "#for testing:\n",
    "\n",
    "#this explaines how to reindex the data frame to acess by rows and delete rows\n",
    "#https://stackoverflow.com/questions/13807758/how-to-delete-a-row-in-a-pandas-dataframe-and-relabel-the-index\n",
    "\n",
    "\n",
    "print(len(movies_full))\n",
    "\n",
    "\n",
    "drop_rows = []\n",
    "for i in range(len(movies_full)):\n",
    "    len_1 = len(movies_full.iloc[i].loc[\"genres\"])                   \n",
    "    if(movies_full.iloc[i].loc[\"genres\"][len_1 -2:] == \"[]\"):\n",
    "        drop_rows.append(i)\n",
    "        continue\n",
    "\n",
    "    len_2 = len(movies_full.iloc[i].loc[\"production_companies\"])\n",
    "    if(movies_full.iloc[i].loc[\"production_companies\"][len_2 -2:] == \"[]\"):\n",
    "        drop_rows.append(i)    \n",
    "        continue   \n",
    "\n",
    "\n",
    "\n",
    "# movies_full = movies_full.drop(index=[1,3])\n",
    "movies_full = movies_full.drop(index=drop_rows, axis = 1)\n",
    "print(len(movies_full))\n",
    "\n",
    "\n",
    "\n",
    "ratings = pd.read_csv('large_source_data/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"), dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})\n",
    "ratings = ratings.rename(columns={\"movieId\": \"id\"})\n",
    "\n",
    "keywords = pd.read_csv('large_source_data/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})\n",
    "credits = pd.read_csv(\"large_source_data/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})\n",
    "\n",
    "\n",
    "#what exactly does merge do here?\n",
    "#default is inner: this only keeps movies that have the id in both dataframes...\n",
    "\n",
    "complete =  pd.merge(movies_full, ratings, on =\"id\")\n",
    "complete =  pd.merge(complete,keywords, on =\"id\")\n",
    "complete  = pd.merge(complete,credits, on =\"id\")\n",
    "\n",
    "\n",
    "complete = complete.sort_values(by = 'userId')\n",
    "\n",
    "complete  = complete.dropna()\n",
    "\n",
    "complete  = complete.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "\n",
    "print(complete.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users: 261306\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 0)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32md:\\repos\\movie_rec_proj\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3460\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[5], line 119\u001b[0m\n    transformed = provide_data(complete_array[j])\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[5], line 60\u001b[0m in \u001b[0;35mprovide_data\u001b[0m\n    movie_data.append(populate_names(array[5]))\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[5], line 43\u001b[0m in \u001b[0;35mpopulate_names\u001b[0m\n    temp_dict = ast.literal_eval(item)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mC:\\python\\lib\\ast.py:62\u001b[0m in \u001b[0;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mC:\\python\\lib\\ast.py:50\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    return compile(source, filename, mode, flags,\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<unknown>\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "# seed for consistent results across runtime\n",
    "# not needed if the previous cell is run\n",
    "# seed_int = 2\n",
    "# random.seed(seed_int)\n",
    "\n",
    "#used to filter out the rows of data with empty entries\n",
    "def condition(array):\n",
    "    #is this necessary with the dropNa function\n",
    "    #answer: yes\n",
    "    length = len(array[4])\n",
    "    if(array[4][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[5])\n",
    "    if(array[5][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[6])\n",
    "    if(array[6][length-2:] == \"[]\"):\n",
    "        return False\n",
    "    length = len(array[7])\n",
    "    if(array[7][length-2:] == \"[]\"):\n",
    "        return False   \n",
    "    \n",
    "    #this is not needed due to the dropNa function used above...\n",
    "    # length = len(array[8])\n",
    "    # if(array[8][length-4:]==\"<NA>\"):\n",
    "    #     return False\n",
    "    # length = len(array[9])\n",
    "    # if(array[9][length-4:]==\"<NA>\"):\n",
    "    #     return False \n",
    "    return True\n",
    "\n",
    "\n",
    "#used to extract names\n",
    "def populate_names(item):\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    cnt = 0\n",
    "    for item in jsons:\n",
    "        if(cnt == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        cnt += 1\n",
    "    return names\n",
    "\n",
    "#extract data from row of complete_array\n",
    "def provide_data(array):\n",
    "    movie_data = []\n",
    "    movie_data.append(int(array[0]))\n",
    "    movie_data.append(int(array[1]))\n",
    "    movie_data.append(float(array[2]))\n",
    "    movie_data.append(array[3])  \n",
    "\n",
    "    movie_data.append(populate_names(array[4]))\n",
    "    movie_data.append(populate_names(array[5]))\n",
    "    movie_data.append(populate_names(array[6]))\n",
    "    movie_data.append(populate_names(array[7]))\n",
    "\n",
    "    movie_data.append(str(array[8]))\n",
    "    movie_data.append(str(array[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "#transform dataframe data fram into numpy array\n",
    "complete_array = complete.to_numpy()\n",
    "\n",
    "#a list of the unique user ids\n",
    "list_of_user_ids = list(complete[\"userId\"].unique())\n",
    "\n",
    "#a dictionary of user ids to the number of ratings with that id\n",
    "counts = complete['userId'].value_counts()\n",
    "\n",
    "#gaps is a list of the number of movies each user rated where a user corresponds with the index\n",
    "gaps = [counts[id] for id in list_of_user_ids]\n",
    "\n",
    "index  = 0\n",
    "\n",
    "#this is how the filtered data is formated\n",
    "user_to_data = []\n",
    "\n",
    "#this is the total number of users in the whole dataset\n",
    "#total number of users: 261306\n",
    "total_nof_users = len(list_of_user_ids)\n",
    "print(\"Total number of users:\", total_nof_users)\n",
    "\n",
    "#this is the number of desired users before filtering...\n",
    "#it controls the frequencey of any given user being tested \n",
    "desired_nof_users_before_filter = 61306\n",
    "\n",
    "#this is the minimum number of ratings a user must have to be tested\n",
    "#this can be altered to fully test more realistic senarios\n",
    "#for instance: what if test users dont have 100 ratings???\n",
    "min_number_of_users = 100\n",
    "\n",
    "#this is collected for insite\n",
    "avg = 0.0\n",
    "cnt = 0.0\n",
    "\n",
    "#populate user_to_data from complete_array\n",
    "for i in range(0, total_nof_users):\n",
    "    #generate a random float to determine a pass for the user\n",
    "    if (random.random()<float(desired_nof_users_before_filter/total_nof_users)):\n",
    "        # another example condition:\n",
    "        # if(gaps[i] >= 50 and gaps[i]<=75):\n",
    "        # if(gaps[i] >= min_number_of_users):\n",
    "        if(gaps[i] <= 10):\n",
    "            user_to_data.append([])\n",
    "            last_index = len(user_to_data) -1\n",
    "            for j in range(index, len(complete_array)):\n",
    "                if complete_array[j][0] == list_of_user_ids[i]:\n",
    "                    #condition is checked for complete_array[j] to move onto the \"append data\" step\n",
    "                    if(condition(complete_array[j])):\n",
    "                        #this is where data is tranformed\n",
    "                        transformed = provide_data(complete_array[j])\n",
    "                        user_to_data[last_index].append(transformed)    \n",
    "                else:\n",
    "                    avg += len(user_to_data[last_index])\n",
    "                    cnt+=1\n",
    "                    index = j\n",
    "                    break\n",
    "        else:\n",
    "            index += gaps[i]             \n",
    "    else:\n",
    "        index += gaps[i]\n",
    "\n",
    "\n",
    "#this is the number of users that dont have any viable ratings for movies\n",
    "nof_faults =0\n",
    "\n",
    "#Go through user_to_data and re-index the users in list order\n",
    "#this is for simplicity and readability \n",
    "#also checks for and remove users with no viable movies based on the condition check in the loop above \n",
    "#(note: this is unlikely to happen with a minimum number of user rating of 100)\n",
    "index = 0\n",
    "for i in range(len(user_to_data)):\n",
    "    if len(user_to_data[index]) == 0:\n",
    "        del user_to_data[index]\n",
    "        nof_faults+=1\n",
    "        index -= 1\n",
    "    else:\n",
    "        for j in range(len(user_to_data[index])):\n",
    "            user_to_data[index][j][0] = index\n",
    "    index+=1\n",
    "\n",
    "\n",
    "print(\"Number of faults:\", nof_faults)\n",
    "\n",
    "#How many users pass the conditions in the loop\n",
    "print(\"Number of users left after filtering:\", len(user_to_data))\n",
    "\n",
    "#average number of ratings per users\n",
    "print(\"Average number of ratings for the filtered user:\", float(avg/cnt))\n",
    "\n",
    "\n",
    "#note: another method to complete the above is to remove movies from the data set that dont have all the required data before completing this step for every user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in a file so that cells below can run without running this cell and above\n",
    "\n",
    "#question: would renaming the user ids as indexes in their order be helpful???\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"constructed_data/constructed_data.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    for i in range(len(user_to_data)):\n",
    "        writer.writerows(user_to_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a starting point if the data is already saved to the constructed_data.csv file\n",
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructed_data/constructed_data.csv\", 'r', encoding=\"utf-8\") as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#seed for consistent results across runtime\n",
    "#used with every random function except for the last cell where a certain number of models are tested and accumulated with identiacal test and train data\n",
    "seed_int = 2\n",
    "random.seed(seed_int)\n",
    "\n",
    "#user to data rows \n",
    "user_to_data = []\n",
    "user_to_data_train = []\n",
    "user_to_data_test = []\n",
    "user_id = -1\n",
    "\n",
    "#note: works when row[0] is also an index\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        user_id = row[0]\n",
    "        user_to_data.append([row])\n",
    "    else:\n",
    "        user_to_data[int(row[0])].append(row)\n",
    "\n",
    "\n",
    "#these both can be increased for consistency as long as there is enough data\n",
    "#with the current configuration there are 4204 users\n",
    "#this can be increased by increasing the desired_nof_users_before_filter parameter above\n",
    "for i in range(4000):\n",
    "    index = random.randint(0, len(user_to_data)-1)\n",
    "    user_to_data_train.append(user_to_data[index])\n",
    "    del user_to_data[index]\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    "    index = random.randint(0, len(user_to_data_train)-1)\n",
    "    user_to_data_test.append(user_to_data_train[index])\n",
    "    del user_to_data_train[index]\n",
    "\n",
    "\n",
    "del user_to_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.130624693639974, 3.0286685310067956, 3.573263428822902, 3.3463300049429403, 2.991701485136822]\n",
      "[4.0, 2.5, 4.0, 4.0, 3.5]\n",
      "[3.4069034044135935, 3.5264684110064706, 3.0600993899167994, 2.8262482790149606, 2.716596377538777]\n",
      "[1.0, 3.5, 2.5, 3.0, 4.5]\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "import json\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#the linalg is used from numpy instea of scipy\n",
    "import numpy as np\n",
    "#the version from numpy is used instead\n",
    "from scipy import linalg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.linalg import sqrtm\n",
    "import math\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "class user_type_vars():\n",
    "    def __init__(self):\n",
    "        #for each user of the user type, a dictionary of movie_id to the movies rating for each movie the user watched\n",
    "        self.user_to_movie_id_to_rating = [] \n",
    "\n",
    "        #for each user, a random choice of movie_id from all the movies the user watched to represent the target movie\n",
    "        self.user_to_target_movie_id = [] \n",
    "\n",
    "        #for each user, this is the index of the users target movie in the order of movies_in_order\n",
    "        #(train_users only)\n",
    "        self.user_to_target_index_full = [] \n",
    "\n",
    "        #for each user, includes ratings for all the movies in the entire train set \n",
    "        #missing ratings and target movie ratings are set to that movies average rating\n",
    "        #(train_users only)\n",
    "        self.user_to_ratings_full = [] \n",
    "\n",
    "        #for each user, includes ratings for all the movies in the entire train set\n",
    "        #the movies mean rating is subtracted from each rating\n",
    "        #missing ratings and target movie ratings are set to zero\n",
    "        #(train_users only)\n",
    "        self.user_to_ratings_full_transform = []\n",
    "\n",
    "        #for every movie watched by the user_type, a list of ratings\n",
    "        self.movie_id_to_ratings = dict()\n",
    "\n",
    "        #this is a set of every unique target movie for the user_type\n",
    "        self.target_movies = set()\n",
    "\n",
    "        #all the movies in order of the movies ratings for each user of the user type\n",
    "        self.movies_in_order = OrderedSet()\n",
    "\n",
    "        #model input features x\n",
    "        self.feature_1 = []\n",
    "        self.feature_2 = []\n",
    "        self.feature_3 = []\n",
    "\n",
    "        #model output feature y\n",
    "        self.user_to_target_rating  = [] \n",
    "\n",
    "\n",
    "#for most of the variables above a train and test version is used\n",
    "train_users = user_type_vars()\n",
    "test_users = user_type_vars()\n",
    "\n",
    "\n",
    "#This is the users average rating not including the chosen target movie\n",
    "#this is for all train users in order followed by the test users in order\n",
    "#(this not being used currently)\n",
    "user_to_average_rating = []\n",
    "\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "def load_feature_1_and_2(target_movies, movies_in_order, user_to_data, movie_id_to_ratings, user_to_movie_id_to_rating, user_to_target_movie_id, user_to_target_rating, feature_1, feature_2):\n",
    "   \n",
    "    #these are used to calculate the overall train rating\n",
    "    #this is used to fill in rating for movies that are only target movies (they dont have ratings)\n",
    "    overall_rating_sum = 0\n",
    "    overall_rating_count = 0\n",
    "\n",
    "    for i in range(len(user_to_data)):\n",
    "        movie_id_to_words = dict()\n",
    "        movie_id_to_rating = dict()\n",
    "        cnt = 0\n",
    "        total =0\n",
    "        rand_int = random.randint(0, len(user_to_data[i])-1)\n",
    "        for movie_data in user_to_data[i]:\n",
    "            if cnt == rand_int:    \n",
    "                target_movies.add(movie_data[1])\n",
    "                user_to_target_movie_id.append(movie_data[1])\n",
    "            else:\n",
    "                overall_rating_sum += float(movie_data[2])\n",
    "                overall_rating_count += 1\n",
    "                total += float(movie_data[2])\n",
    "\n",
    "                #this only runs when the movie is not the target movie because\n",
    "                #the target movies are thought to be the movies whose rating is to be predicted...\n",
    "                #not ratings that are already on record\n",
    "                if movie_data[1] in movie_id_to_ratings.keys():\n",
    "                    movie_id_to_ratings[movie_data[1]].append(float(movie_data[2]))\n",
    "                else:\n",
    "                    movie_id_to_ratings[movie_data[1]] = [float(movie_data[2])]\n",
    "\n",
    "            movie_string = \"\"\n",
    "\n",
    "            #use this to apply all the text data and combine in to a single list of words (repeats allowed):\n",
    "            # for index in range (3,len(movie_data)):\n",
    "            #     if(index!= len(movie_data)-1):\n",
    "            #         movie_string+= movie_data[index]+\" \"\n",
    "            #     else:\n",
    "            #         movie_string+= movie_data[index]\n",
    "\n",
    "\n",
    "            #all of the text columns and a few combinations of certain text columns were tested but they were not helpful in...\n",
    "            #increasing model perfromance (see below)\n",
    "\n",
    "\n",
    "            #Use this truncated code to only include the genre column strings:\n",
    "            movie_string = movie_data[4]\n",
    "\n",
    "            #lematization and conversion to lists\n",
    "            cleaned = remove_stopwords(movie_string)\n",
    "            cleaned = [wnl.lemmatize(word) for word in cleaned.split(\" \")]\n",
    "            cleaned = [word[:-1] for word in cleaned if word.endswith(\".\")] + [word for word in cleaned if not word.endswith(\".\")]\n",
    "\n",
    "            movie_id_to_words[movie_data[1]] = cleaned\n",
    "            movie_id_to_rating[movie_data[1]] = float(movie_data[2])\n",
    "            movies_in_order.add(movie_data[1])\n",
    "            cnt+=1\n",
    "\n",
    "        user_to_movie_id_to_rating.append(movie_id_to_rating)\n",
    "        user_to_average_rating.append(float(total/(cnt-1)))\n",
    "\n",
    "        #the current users list of words from all the movies they rated\n",
    "        users_words_in_order = OrderedSet()\n",
    "        for movie_id in movie_id_to_words.keys():\n",
    "            for word in movie_id_to_words[movie_id]:\n",
    "                users_words_in_order.add(word)\n",
    "\n",
    "\n",
    "        word_counts = [] #list of word counts for the users_words_in_order for each movie (excluding target)\n",
    "        target_word_counts = [] #word counts for the users_words_in_order for the target movie\n",
    "\n",
    "        #these are the scaled versions of variables directly above\n",
    "        #these are only relevant with user averages scalings opposed to movie average scaling...\n",
    "        #note: scaling also happens automatically below\n",
    "        word_counts_transformed = []\n",
    "        target_word_counts_transformed = []\n",
    "\n",
    "        #word count sums for each word in users_words_in_order for each user\n",
    "        sums = dict()\n",
    "\n",
    "        #for each movie the user watched record the wordcount for each word in users_words_in_order\n",
    "        for movie_id in movie_id_to_words.keys():\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "                temp_list = []\n",
    "                # sum = 0\n",
    "                for word in users_words_in_order:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                        # sum+=temp_dict[word]\n",
    "                        if word in sums.keys():\n",
    "                            sums[word] += temp_dict[word] \n",
    "                        else:\n",
    "                            sums[word] = temp_dict[word] \n",
    "                    else:\n",
    "                        temp_list.append(0) \n",
    "                        if word not in sums.keys():\n",
    "                            sums[word] = 0  \n",
    "\n",
    "                word_counts.append(temp_list)  \n",
    "\n",
    "                # append to word_counts_transformed:\n",
    "                # avg = float(sum/len(users_words_in_order))\n",
    "                # word_counts_transformed.append([x - avg for x in temp_list])\n",
    "            else:\n",
    "\n",
    "                temp_dict = Counter(movie_id_to_words[movie_id])\n",
    "                temp_list = []\n",
    "                # sum = 0\n",
    "                for word in users_words_in_order:\n",
    "                    if word in temp_dict.keys():\n",
    "                        temp_list.append(temp_dict[word])\n",
    "                        # sum+=temp_dict[word]\n",
    "                        if word in sums.keys():\n",
    "                            sums[word] += temp_dict[word] \n",
    "                        else:\n",
    "                            sums[word] = temp_dict[word]             \n",
    "                    else:\n",
    "                        temp_list.append(0) \n",
    "                        if word not in sums.keys():\n",
    "                            sums[word] = 0 \n",
    "\n",
    "                target_word_counts = temp_list\n",
    "\n",
    "                # set target_word_counts_transformed:\n",
    "                # avg = float(sum/len(users_words_in_order))\n",
    "                # target_word_counts_transformed = [x - avg for x in temp_list]\n",
    "        \n",
    "\n",
    "        complete_word_counts = word_counts.copy()\n",
    "        complete_word_counts.append(target_word_counts)\n",
    "        transformed_word_counts = TfidfTransformer().fit_transform(complete_word_counts).toarray()\n",
    "\n",
    "\n",
    "        #populate ratings with the exception of the target rating \n",
    "        #also record the users target movie rating \n",
    "        ratings = []\n",
    "        for movie_id in movie_id_to_rating.keys():\n",
    "            if movie_id != user_to_target_movie_id[-1]:\n",
    "                ratings.append(movie_id_to_rating[movie_id])\n",
    "            else:\n",
    "                #this signifies the ratings to be predicted by the model\n",
    "                user_to_target_rating.append(movie_id_to_rating[movie_id])\n",
    "        \n",
    "\n",
    "        #potential functions of predict:\n",
    "        #return the average ratings from movies that are a like the target movie with cosine similairity\n",
    "        #unweighted average of all of the users movies\n",
    "        #weighted average of all the users movies (weights are based on cossine similarity)\n",
    "        def predict():\n",
    "            item_1 = 0 \n",
    "            item_2 = 0\n",
    "\n",
    "            # option 1: \n",
    "            # cosine_sim = linear_kernel(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "            #or\n",
    "            #cosine_sim = cosine_similarity(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "            # cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            # combined = zip(cosine_sim, ratings)\n",
    "            # combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "            # avg = 0\n",
    "            # nof = 10.0\n",
    "            # for i in range(int(nof)):\n",
    "            #     avg += combined[i][1]\n",
    "            # item_2 =  float(avg/nof)\n",
    "\n",
    "            #option 2:\n",
    "            #note: item 1 is a higher performing feature than any of the other methods in the function\n",
    "            sum = 0\n",
    "            for i in range(len(ratings)):\n",
    "                sum += ratings[i]\n",
    "            item_1 = float(sum/len(ratings))\n",
    "\n",
    "            #option 3:\n",
    "            #when the svd function is used:\n",
    "            # cosine_sim = cosine_similarity(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "\n",
    "            #when the svd function is not used:\n",
    "            cosine_sim = linear_kernel(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "            cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "            numerator = 0\n",
    "            denominator = 0\n",
    "            item_2 = item_1\n",
    "            for i in range(len(ratings)):\n",
    "                numerator += float(cosine_sim[i]*ratings[i])\n",
    "                denominator += cosine_sim[i]\n",
    "            \n",
    "            if denominator != 0:\n",
    "                item_2 = float(numerator/denominator)\n",
    "        \n",
    "            return (item_1, item_2)\n",
    "        \n",
    "        \n",
    "        items = predict()\n",
    "\n",
    "        feature_1.append(items[0])\n",
    "        feature_2.append(items[1])\n",
    "            \n",
    "        \n",
    "    return float(overall_rating_sum/overall_rating_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pre_svd(movie_id_to_average_rating, movies_in_order, user_to_ratings_full_transform, user_to_ratings_full, user_to_target_index_full, \n",
    "               user_to_movie_id_to_rating, user_to_target_movie_id):\n",
    "    for i in range(len(user_to_movie_id_to_rating)):\n",
    "        ratings = []\n",
    "        transformed_ratings = []\n",
    "        index = 0\n",
    "\n",
    "\n",
    "        #what if there is no movie_id == user_to_target_movie_id[i]\n",
    "        #this can happen when a test users target movie is not in the train_users.movies_in_order...\n",
    "\n",
    "        #solution:\n",
    "\n",
    "        #this could be run once with only train_movies\n",
    "        #and then used to populate the train svd\n",
    "        #and then extract the prediction to train the model\n",
    "\n",
    "\n",
    "        #then again with all movies train_movies + test_movies\n",
    "        #then used to populate the full svd\n",
    "        #and then extract the prediction to test model\n",
    "\n",
    "\n",
    "        #note: movie_id_to_average_rating_train shouold onyl be used for the train run of this function\n",
    "        #for the test version of the this movie_id_to_average_rating_full should be used\n",
    "\n",
    "        for movie_id in movies_in_order:\n",
    "            if movie_id == user_to_target_movie_id[i]:\n",
    "                user_to_target_index_full.append(index)\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(movie_id_to_average_rating[movie_id] - movie_id_to_average_rating[movie_id]) \n",
    "\n",
    "            #note: It should not matter that user_to_movie_id_to_rating includes movie id equal to user_to_target_movie_id[i] since the above condition will flag before this condition\n",
    "            elif movie_id in user_to_movie_id_to_rating[i].keys():\n",
    "                ratings.append(user_to_movie_id_to_rating[i][movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(user_to_movie_id_to_rating[i][movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            else:\n",
    "                ratings.append(movie_id_to_average_rating[movie_id])\n",
    "                #note: per item averages are being subtracted here instead of per user averages\n",
    "                transformed_ratings.append(movie_id_to_average_rating[movie_id] - movie_id_to_average_rating[movie_id])\n",
    "            index +=1\n",
    "        user_to_ratings_full.append(ratings)\n",
    "        user_to_ratings_full_transform.append(transformed_ratings)\n",
    "\n",
    "\n",
    "#note: before passing to this function the data is normalized about the average movie ratings (not average user ratings)\n",
    "#each user train and test users have a single rating that needs to be trained against in the train case\n",
    "#and predicted in the test case\n",
    "\n",
    "#the svd can be applied to the combined data of the train and test sets\n",
    "#both movies that the user didn't watch and movies that should be guesses are...\n",
    "#transformed to have a value of zero before svd\n",
    "\n",
    "#the movie columns are taken from the train dataset...\n",
    "#senario: suppose a test user has a rating of a movie not part of the train set and it is not the target movie (ignore it)\n",
    "#senario: suppose a test user has a rating of a movie not part of the train set and it is the target movie (guess the rating instead of using svd)\n",
    "\n",
    "#...Once the UsV is created...\n",
    "#take the rating from the new UsV for the user row and movie column for the target movie\n",
    "#other option: cossine similairty on the U ignoring other test users\n",
    "\n",
    "\n",
    "def svd_full(user_to_ratings_full_transform, n, movie_id_to_average_rating):\n",
    "    #is this the source the random variation???\n",
    "    U, s, V = np.linalg.svd(user_to_ratings_full_transform, full_matrices=False)\n",
    "    \n",
    "    #simplify ratings to n features\n",
    "    s=np.diag(s)\n",
    "    s=s[0:n,0:n]\n",
    "    U=U[:,0:n]\n",
    "    V=V[0:n,:]\n",
    "\n",
    "    #reconstrcut to a new array\n",
    "    Us = np.dot(U,s)\n",
    "    UsV = np.dot(Us,V)\n",
    "    \n",
    "\n",
    "    #the keys of movie_id_to_ratings is in the same order of movies_in_order and therefore so is movie_id_to_average_rating_train\n",
    "    x = np.tile(list(movie_id_to_average_rating.values()), (UsV.shape[0],1))\n",
    "\n",
    "    #this tranforms the UsV row by row into the original rating scale (1-5)\n",
    "    UsV = UsV + x\n",
    "\n",
    "    #be consistent with data structures...\n",
    "    return list(UsV)\n",
    "\n",
    "\n",
    "\n",
    "overall_average_train = load_feature_1_and_2(train_users.target_movies, train_users.movies_in_order, user_to_data_train, train_users.movie_id_to_ratings, train_users.user_to_movie_id_to_rating, \n",
    "                                                         train_users.user_to_target_movie_id, train_users.user_to_target_rating, train_users.feature_1, train_users.feature_2)\n",
    "\n",
    "\n",
    "load_feature_1_and_2(test_users.target_movies, test_users.movies_in_order, user_to_data_test, test_users.movie_id_to_ratings, test_users.user_to_movie_id_to_rating, \n",
    "               test_users.user_to_target_movie_id,\n",
    "               test_users.user_to_target_rating, test_users.feature_1, test_users.feature_2)\n",
    "\n",
    "\n",
    "\n",
    "#Unlike the other feature loading functions it only makes sense to run this once since...\n",
    "#there is significantly difference processes for train and test data\n",
    "def load_feature_3():\n",
    "\n",
    "    movie_id_to_average_rating_train = dict()\n",
    "    movie_id_to_average_rating_full = dict()\n",
    "\n",
    "    #is all_movies_in_order still in order???\n",
    "    all_movies_in_order = train_users.movies_in_order|test_users.movies_in_order\n",
    "\n",
    "\n",
    "    #this is used to populate movie_id_to_average_rating_train and movie_id_to_average_rating_full...\n",
    "    #without skippig the movies are target movies  and not in (movies not in test_users.movie_id_to_ratings or train_users.movie_id_to_ratings)\n",
    "    for movie in all_movies_in_order:\n",
    "        temp = 0\n",
    "        if(movie in train_users.movie_id_to_ratings and movie in test_users.movie_id_to_ratings):\n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_train[movie] = float(temp/len(train_users.movie_id_to_ratings[movie])) \n",
    "\n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_full[movie] = float(temp/(len(train_users.movie_id_to_ratings[movie])+len(test_users.movie_id_to_ratings[movie])))  \n",
    "\n",
    "        elif(movie in train_users.movie_id_to_ratings):\n",
    "            for rating in train_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_train[movie] = float(temp/len(train_users.movie_id_to_ratings[movie]))\n",
    "            movie_id_to_average_rating_full[movie] = movie_id_to_average_rating_train[movie]\n",
    "\n",
    "        elif(movie in test_users.movie_id_to_ratings):\n",
    "            #is the movie a target movie in the train set that isn't in train_users.movies_id_to_ratings???         \n",
    "            if(movie in train_users.target_movies):\n",
    "                movie_id_to_average_rating_train[movie] = overall_average_train\n",
    "\n",
    "            for rating in test_users.movie_id_to_ratings[movie]:\n",
    "                temp+=rating\n",
    "            movie_id_to_average_rating_full[movie] = float(temp/len(test_users.movie_id_to_ratings[movie]))\n",
    "        else:\n",
    "            #is the movie a target movie in the train set that isn't in train_users.movie_id_to_ratings???\n",
    "            #is the movie a target movie in the test set that isn't in test_users.movie_id_to_ratings???\n",
    "            if(movie in train_users.target_movies):\n",
    "                movie_id_to_average_rating_train[movie] = overall_average_train\n",
    "                movie_id_to_average_rating_full[movie] = overall_average_train\n",
    "            else:\n",
    "                movie_id_to_average_rating_full[movie] = overall_average_train\n",
    "   \n",
    "\n",
    "    #for all users in train and then test order\n",
    "    full_user_to_ratings_full_transform = []\n",
    "    full_user_to_ratings_full = []\n",
    "    full_user_to_target_index_full = []\n",
    "\n",
    "\n",
    "    #this makes a comprehensive list of the train data followed by the test users data\n",
    "    full_user_to_movie_id_to_rating  = train_users.user_to_movie_id_to_rating + test_users.user_to_movie_id_to_rating\n",
    "    full_user_to_target_movie_id = train_users.user_to_target_movie_id + test_users.user_to_target_movie_id\n",
    "\n",
    "\n",
    "    #This is used to scale the ratings and store in train_users.user_to_ratings_full_transform and full_user_to_ratings_full_transform\n",
    "    #This will transform the target movie ratings and unrated movies to zero\n",
    "\n",
    "    #run once with only train data to train model\n",
    "    #run again with train and test data to evaluate model...\n",
    "\n",
    "    pre_svd(movie_id_to_average_rating_train, train_users.movies_in_order, train_users.user_to_ratings_full_transform, train_users.user_to_ratings_full, train_users.user_to_target_index_full, \n",
    "                train_users.user_to_movie_id_to_rating, train_users.user_to_target_movie_id)\n",
    "\n",
    "    pre_svd(movie_id_to_average_rating_full, all_movies_in_order, full_user_to_ratings_full_transform, full_user_to_ratings_full, full_user_to_target_index_full, \n",
    "                full_user_to_movie_id_to_rating, full_user_to_target_movie_id)\n",
    "\n",
    "\n",
    "    #In practice, there is a train and a test set, the train set is what the database has on record\n",
    "    #the test data will usually be data that hasn't been seen before that can include any number of test users\n",
    "\n",
    "    #When train_users.user_to_ratings_full_transform is used as the input of the svd function, \n",
    "    #svd_out_train is used to produce predictions used to train the model\n",
    "\n",
    "    #When full_user_to_ratings_full_transform is used as the input of the svd function,\n",
    "    #svd_out_full is used to produce predictions used to test the model\n",
    "    \n",
    "\n",
    "    #n = 20 proved to be close to the highest performing constant for the above configuration\n",
    "    svd_out_train = svd_full(train_users.user_to_ratings_full_transform, 20, movie_id_to_average_rating_train)\n",
    "    svd_out_full = svd_full(full_user_to_ratings_full_transform, 20, movie_id_to_average_rating_full)\n",
    "\n",
    "    #here the smaller svd provides predictions used to train the mlp model\n",
    "    for i in range(len(train_users.user_to_ratings_full_transform)):\n",
    "        train_users.feature_3.append(svd_out_train[i][train_users.user_to_target_index_full[i]])\n",
    "\n",
    "    #here the larger svd provides predictions used to test the mlp model\n",
    "    for i in range(len(full_user_to_ratings_full_transform) - len(train_users.user_to_ratings_full_transform)):\n",
    "        test_users.feature_3.append(svd_out_full[i+len(train_users.user_to_ratings_full_transform)][full_user_to_target_index_full[i+len(train_users.user_to_ratings_full_transform)]])\n",
    "\n",
    "load_feature_3()\n",
    "\n",
    "print(train_users.feature_3[0:5])\n",
    "print(train_users.user_to_target_rating[0:5])\n",
    "\n",
    "print(test_users.feature_3[0:5])\n",
    "print(test_users.user_to_target_rating[0:5])\n",
    "\n",
    "\n",
    "#used to see what the text data looks like...\n",
    "# not applicable with dict to list change...\n",
    "# file = open(\"test_dicts_1.txt\", 'w', encoding=\"utf-8\")\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_train))\n",
    "# file.close()\n",
    "\n",
    "# file = open(\"test_dicts_2.txt\", 'w', encoding=\"utf-8\")\n",
    "# file.write(json.dumps(user_to_movie_id_to_corpus_test))\n",
    "# file.close()\n",
    "\n",
    "\n",
    "#this meight not be worth the deletion!!!\n",
    "# del user_to_data_train\n",
    "# del user_to_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07655955 0.54884398]\n",
      "[0.07251724 0.53422953]\n",
      "[0.07398554 0.53950872]\n",
      "[0.07893114 0.53707882]\n",
      "[0.07790385 0.54515307]\n",
      "[0.07621005 0.54280122]\n",
      "[0.07408352 0.54923166]\n",
      "[0.0758433  0.54757472]\n",
      "[0.07909377 0.54351962]\n",
      "[0.07422675 0.55054701]\n",
      "[0.0754062  0.55380725]\n",
      "[0.0735775  0.54133645]\n",
      "[0.07306207 0.54227272]\n",
      "[0.07318415 0.55481386]\n",
      "[0.07866763 0.54122154]\n",
      "[0.07641959 0.55014952]\n",
      "[0.07510819 0.53854389]\n",
      "[0.07709212 0.54089504]\n",
      "[0.07791481 0.55187469]\n",
      "[0.0770309  0.54681115]\n",
      "[0.07527401 0.53857398]\n",
      "[0.07125875 0.54996505]\n",
      "[0.07672478 0.54124431]\n",
      "[0.07449761 0.5365991 ]\n",
      "[0.07700223 0.5585927 ]\n",
      "[0.0731843  0.53812009]\n",
      "[0.07340178 0.54033766]\n",
      "[0.07516149 0.54115591]\n",
      "[0.07465853 0.56061812]\n",
      "[0.07405626 0.55137907]\n",
      "[0.07400895 0.5419615 ]\n",
      "[0.07677643 0.53409133]\n",
      "[0.07398305 0.55226476]\n",
      "[0.07892379 0.55685729]\n",
      "[0.07238151 0.54825332]\n",
      "[0.07507103 0.54598566]\n",
      "[0.07428719 0.5476295 ]\n",
      "[0.07369303 0.55169743]\n",
      "[0.07607859 0.55844905]\n",
      "[0.0772086 0.5402641]\n",
      "[0.07822428 0.54304877]\n",
      "[0.07551385 0.53977194]\n",
      "[0.07305404 0.52771885]\n",
      "[0.07705965 0.53734755]\n",
      "[0.0740848 0.5494893]\n",
      "[0.0730521  0.55338704]\n",
      "[0.08045907 0.55214487]\n",
      "[0.07661559 0.53917979]\n",
      "[0.07044912 0.54721127]\n",
      "[0.07311216 0.5466872 ]\n",
      "[0.07474687 0.54062667]\n",
      "[0.07806905 0.5572739 ]\n",
      "[0.07259565 0.53451983]\n",
      "[0.07563702 0.55483081]\n",
      "[0.07563761 0.55031664]\n",
      "[0.07662178 0.5463821 ]\n",
      "[0.07202406 0.53513469]\n",
      "[0.07400453 0.54504599]\n",
      "[0.07919974 0.54791187]\n",
      "[0.07677905 0.54089787]\n",
      "[0.07125206 0.53132603]\n",
      "[0.07347896 0.54100651]\n",
      "[0.07901239 0.54139018]\n",
      "[0.0738979  0.54457802]\n",
      "[0.0797384  0.55291781]\n",
      "[0.07837002 0.55017778]\n",
      "[0.07412571 0.54737291]\n",
      "[0.07464886 0.55650282]\n",
      "[0.07806069 0.54478607]\n",
      "[0.07693732 0.5427919 ]\n",
      "[0.07854382 0.54261958]\n",
      "[0.07568792 0.54679238]\n",
      "[0.07562559 0.54990028]\n",
      "[0.07785919 0.55125317]\n",
      "[0.07372391 0.55175167]\n",
      "[0.07841402 0.54835309]\n",
      "[0.07477702 0.546495  ]\n",
      "[0.07588283 0.53968873]\n",
      "[0.07449332 0.54454733]\n",
      "[0.07348797 0.53869026]\n",
      "[0.07671418 0.55318521]\n",
      "[0.07454098 0.54192265]\n",
      "[0.07387343 0.5380318 ]\n",
      "[0.07740367 0.54338081]\n",
      "[0.07557237 0.55439168]\n",
      "[0.0814212  0.55390882]\n",
      "[0.07621395 0.55075399]\n",
      "[0.07442755 0.54626076]\n",
      "[0.07315792 0.54760006]\n",
      "[0.07424171 0.534985  ]\n",
      "[0.07250829 0.53819614]\n",
      "[0.0760646  0.54567087]\n",
      "[0.07196198 0.55383264]\n",
      "[0.07423601 0.55212625]\n",
      "[0.07488291 0.53460995]\n",
      "[0.07406652 0.5278335 ]\n",
      "[0.07883989 0.53965175]\n",
      "[0.07832415 0.55240479]\n",
      "[0.07699448 0.54155194]\n",
      "[0.0719714 0.5474528]\n",
      "(0.3692252282147528, 0.35771948109887597)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "#average the performance results for a number of models with identical inputs\n",
    "def test_parameters(nof_runs, layers, train_input_features, test_input_features):\n",
    "    train_inputs = [list(pair) for pair in train_input_features]\n",
    "    test_inputs = [list(pair) for pair in test_input_features]\n",
    "    return average_results(nof_runs, layers, train_inputs, test_inputs)\n",
    "    \n",
    "\n",
    "def average_results(nof_runs, layers, train_inputs, text_inputs):\n",
    "    no_rounding = 0\n",
    "    rounding = 0\n",
    "    for _ in range(nof_runs):\n",
    "        #best performance analysis is analysis_1\n",
    "        pair = analysis_1(layers, train_inputs, text_inputs)\n",
    "        no_rounding+=pair[0]\n",
    "        rounding+=pair[1]\n",
    "    return float(no_rounding/nof_runs), float(rounding/nof_runs)\n",
    "\n",
    "\n",
    "#no scaling (best performance):\n",
    "def analysis_1(layers, train_inputs, test_inputs):\n",
    "    # build and train model\n",
    "    # nn model (worse performance)\n",
    "    # reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    # linear regression (better performance)\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(train_inputs, train_users.user_to_target_rating)\n",
    "\n",
    "    #show importance of different inputs features to the model\n",
    "    results = permutation_importance(reg, train_inputs, train_users.user_to_target_rating)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "    #make predictions\n",
    "    predictions = reg.predict(test_inputs)\n",
    "\n",
    "    #test with and without roundings...\n",
    "    #in a sense this is logical sense becasue the actual ratings a user makes must be divisable by .5 \n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#scale inputs and targets:\n",
    "def analysis_2(layers, train_inputs, test_inputs):\n",
    "    #scale input features\n",
    "    train_inputs_scaled = StandardScaler().fit_transform(train_inputs)\n",
    "\n",
    "    #scale target values\n",
    "    target_scalar = StandardScaler()\n",
    "    true_rating_train_scaled = target_scalar.fit_transform(np.reshape(train_users.user_to_target_rating, (-1, 1)))\n",
    "    true_rating_train_scaled = np.reshape(true_rating_train_scaled, len(true_rating_train_scaled))\n",
    "\n",
    "    #build and train model\n",
    "    reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    reg.fit(train_inputs_scaled, true_rating_train_scaled)\n",
    "\n",
    "    #show importance of different inputs features...\n",
    "    results = permutation_importance(reg, train_inputs_scaled,true_rating_train_scaled)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "    #scale inputs features\n",
    "    test_inputs_scaled = StandardScaler().fit_transform(test_inputs)\n",
    "\n",
    "    #predict the scaled verison of ouptuts\n",
    "    scaled_predictions = reg.predict(test_inputs_scaled)\n",
    "\n",
    "    #get actual predictions from scaled predictions...\n",
    "    predictions = target_scalar.inverse_transform(scaled_predictions.reshape(-1, 1))\n",
    "    predictions = list(predictions.reshape(len(predictions)))\n",
    "\n",
    "    #test with and without roundings...\n",
    "    #in a sense this is logical sense becasue the actual ratings a user makes must be divisable by .5 \n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "#only scale inputs:\n",
    "def analysis_3(layers, train_inputs, test_inputs):\n",
    "    #scale input features\n",
    "    train_inputs_scaled = StandardScaler().fit_transform(train_inputs)\n",
    "\n",
    "    #build and train model\n",
    "    reg = MLPRegressor(hidden_layer_sizes = layers, solver = \"adam\",  max_iter = 1000)\n",
    "    reg.fit(train_inputs_scaled, train_users.user_to_target_rating)\n",
    "\n",
    "    #show importance of different inputs features...\n",
    "    results = permutation_importance(reg, train_inputs_scaled, train_users.user_to_target_rating)\n",
    "    print(results[\"importances_mean\"])\n",
    "\n",
    "    #scale inputs features\n",
    "    test_inputs_scaled = StandardScaler().fit_transform(test_inputs)\n",
    "\n",
    "    #predict the scaled verison of ouptuts\n",
    "    predictions = reg.predict(test_inputs_scaled)\n",
    "\n",
    "    #test with and without roundings...\n",
    "    #in a sense this is logical sense becasue the actual ratings a user makes must be divisable by .5 \n",
    "    rounded_predictions = []\n",
    "    for item in predictions:\n",
    "        rounded_predictions.append(float(round(item*2)/2.0))\n",
    "\n",
    "    #evaluation metric 1:\n",
    "    return(r2_score(test_users.user_to_target_rating, predictions), \n",
    "        r2_score(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "    #evaluation metric 2:\n",
    "    # return(mean_squared_error(test_users.user_to_target_rating, predictions), \n",
    "    #         mean_squared_error(test_users.user_to_target_rating, rounded_predictions))\n",
    "\n",
    "\n",
    "\n",
    "#the current test is the average of the r2 scores for 100 different models trained on the same input\n",
    "#the hidden layers are (10,10,10) and the best combinatio of inputs features(feature_2 and feature_3) are used\n",
    "print(test_parameters(100, (10,10,10), \n",
    "    zip(train_users.feature_1, train_users.feature_3),\n",
    "      zip(test_users.feature_1, test_users.feature_3)))\n",
    "\n",
    "\n",
    "#this shows the side by side comparision between all the features and the actual rating\n",
    "#each feature provides a reasonable guess of the target rating\n",
    "#the combination of the feature used above (feature_2 and feature_3) proves stronger than any feature alone and any other combination of features\n",
    "# print(test_users.feature_1)\n",
    "# print(test_users.feature_2)\n",
    "# print(test_users.feature_3)\n",
    "# print(test_users.user_to_target_rating)\n",
    "\n",
    "\n",
    "\n",
    "#with linear regression:\n",
    "#with cossine similarity:\n",
    "\n",
    "#feature_2 and feature_3:\n",
    "#(0.3749823647027071, 0.348993902575555)\n",
    "#(0.3749823647027071, 0.348993902575555)\n",
    "\n",
    "#feature_1 and feature_3: \n",
    "#(0.37665923268552526, 0.35436777366278627)\n",
    "#(0.37665923268552526, 0.35436777366278627)\n",
    "\n",
    "\n",
    "#with linear regression:\n",
    "#with linear_kernel:\n",
    "\n",
    "#feature_2 and feature_3:\n",
    "#(0.3749823647027071, 0.348993902575555)...\n",
    "\n",
    "#feature_1 and feature_3: \n",
    "#(0.37665923268552526, 0.35436777366278627)...\n",
    "#(0.3692252282147528, 0.35771948109887597)\n",
    "\n",
    "\n",
    "\n",
    "#with nn model:\n",
    "#feature_2 and feature_3:\n",
    "#(0.368986238493678, 0.34709385529828507)\n",
    "#feature_1 and feature_3: \n",
    "#(0.3692192733203262, 0.34905915672447185)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
