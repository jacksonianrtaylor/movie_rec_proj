{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data source\n",
    "\n",
    "* Download the data needed for this jupyter notebook from kaggle and store it in a new folder (the-movies-dataset) in the current directory.\n",
    "\n",
    "\n",
    "* Upon running this cell, the user will be asked for their username and key which can be found in a fresh api token from kaggle.\n",
    "\n",
    "* Instructions to get api token to authenticate the data request (Note: kaggle account required):\n",
    "    1. Sign into kaggle.\n",
    "    2. Go to the 'Account' tab of your user profile and select 'Create New Token'. \n",
    "    3. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
    "\n",
    "* If the folder has been created and the files are already in that folder, than this cell does nothing and requires no credentials.\n",
    "\n",
    "* Data Source Information: https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=movies_metadata.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Kaggle Key:Downloading the-movies-dataset.zip to ./the-movies-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228M/228M [00:23<00:00, 10.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Raw Data\n",
    "\n",
    "Combining certain data from the necessary csv files into a single dataframe (complete_df).\n",
    "\n",
    "* Rows are removed from each dataframe when they do not have sufficent data for a column or the data from a column does not exist.\n",
    "* This kind of row removal is done before multiple copies of the same movie data becomes present in multple rows, to save time and space.\n",
    "* Iteration through rows of a dataframe at this level is inefficient compared to list iteration.\n",
    "* This is why the dataframes are converted into lists before iteration and then back again to dataframes, so the merge function can be applied to combine the data into a single dataframe (complete_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3276\n",
      "3276\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "movies_df = pd.read_csv('./the-movies-dataset/movies_metadata.csv',usecols=(\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"),\n",
    "                          dtype={'genres':\"string\",\"id\":\"string\",\"title\": \"string\", \"tagline\": \"string\",\"overview\":\"string\",\n",
    "                                    \"production_companies\" :\"string\"})[[\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"]]\n",
    "movies_df.dropna(inplace = True)\n",
    "movies_lst = [row for row in movies_df.values.tolist() if not (row[0][len(row[0])  - 2:] == \"[]\" or row[5][len(row[5]) - 2:] == \"[]\")]\n",
    "movies_df = pd.DataFrame(movies_lst, columns = (\"genres\",\"id\" ,\"title\",\"tagline\", \"overview\",\"production_companies\"), dtype = str)\n",
    "\n",
    "\n",
    "\n",
    "ratings_df = pd.read_csv('./the-movies-dataset/ratings.csv', usecols = (\"userId\", \"movieId\", \"rating\"),\n",
    "                       dtype={\"userId\": \"string\",\"movieId\": \"string\",\"rating\": \"string\"})[[\"userId\", \"movieId\", \"rating\"]]\n",
    "ratings_df.rename(columns={\"movieId\": \"id\"}, inplace = True)\n",
    "ratings_df.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Question: What if the removal of duplicate movie ids per user was processed here instead of the cell below???\n",
    "# Answer: The duplicate removal function can be ran here,...\n",
    "# but the complete_list in the cell below can also be iterated over with relative complexity in order to remove duplicates.\n",
    "# The iteration in the next cell also populates the gap list...\n",
    "# which is critical to be ran directly before the function that determines bounds for users rated movies.\n",
    "# So, omitting the no duplicate function in this cell and making it run in the next cell avoids redundant iteration.\n",
    "\n",
    "\n",
    "# Question: What if the test and train ratings bounds was enforced here instead of the cell below???\n",
    "# Answer: The merge functions below needs to be executed before determining test and train users, because merge will remove rows and ratings from users...\n",
    "# before enforcing the users to be in a certain bounds for the number of their ratings. \n",
    "# The current timing of this function will ensure that the final users are within the set train or test bounds.\n",
    "\n",
    "\n",
    "keywords_df = pd.read_csv('./the-movies-dataset/keywords.csv', usecols = (\"id\", \"keywords\"), dtype={\"id\": \"string\",\"keywords\":\"string\"})[[\"id\", \"keywords\"]]\n",
    "keywords_df.dropna(inplace = True)\n",
    "keywords_lst = [row for row in keywords_df.values.tolist() if not (row[1][len(row[1])  - 2:] == \"[]\")]\n",
    "keywords_df = pd.DataFrame(keywords_lst, columns = (\"id\", \"keywords\"), dtype = str)\n",
    "\n",
    "\n",
    "credits_df = pd.read_csv(\"./the-movies-dataset/credits.csv\", usecols = (\"cast\", \"id\"), dtype={\"cast\": \"string\", \"id\": \"string\"})[[\"cast\", \"id\"]]\n",
    "credits_df.dropna(inplace = True)\n",
    "credits_lst = [row for row in credits_df.values.tolist() if (not row[0][len(row[0])  - 2:] == \"[]\")]\n",
    "credits_df = pd.DataFrame(credits_lst, columns = (\"cast\", \"id\"), dtype = str)\n",
    "\n",
    "\n",
    "# Default merge is inner: This only keeps movies that have the id existing in both dataframes.\n",
    "complete_df =  pd.merge(movies_df, ratings_df, on =\"id\")\n",
    "complete_df =  pd.merge(complete_df,keywords_df, on =\"id\")\n",
    "complete_df  = pd.merge(complete_df,credits_df, on =\"id\")\n",
    "\n",
    "\n",
    "print(len(complete_df[\"id\"].unique()))\n",
    "\n",
    "\n",
    "\n",
    "complete_df.sort_values(by = 'userId', inplace = True)\n",
    "\n",
    "\n",
    "# Master dataframe: For each (user id, movie id) row combination there is the combined movie data from movies_df, ratings_df, keywords_df, and credits_df for the movie id in question.\n",
    "# The columns are reordered.\n",
    "complete_df  = complete_df.loc[:,['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\" ]]\n",
    "\n",
    "\n",
    "print(len(complete_df[\"id\"].unique()))\n",
    "# print(complete_df.tail())\n",
    "\n",
    "\n",
    "# For testing:\n",
    "# print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "# print(complete_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# Tested on personal machine:\n",
    "# Old run with dataframe iteration (old code): 1 minute and 5.7 seconds\n",
    "# New run with list conversion before iteration (current code): 37.1 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Selection\n",
    "1. Select data from users that have a number of ratings within a certain bounds.\n",
    "2. Select a random subset of this data and simplify it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete number of users: 260788\n",
      "Minutes taken: 0.7658499757448832\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "import time\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import hist\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# LOOK: To make a fair comparison to the best possible implementation of the netflix data\n",
    "# a closer distribution of user ratings to the netflix data should be selected \n",
    "# perhaps simalir proportion of users in each increment of 10 ratings\n",
    "# problem: this does not correctly represent the population of the non-netflix dataset so it would have weaker applciaiton!!!\n",
    "# But, another question is, is the netflix data biased???\n",
    "# was the whole problem statment more theoretical then practical???\n",
    "# how is it that the two datasets are vastly different???\n",
    "\n",
    "\n",
    "\n",
    "# Also, since these are different datasets, a higher distribution is more extreme for this dataset than the netflix dataset\n",
    "# meaning there is a higher selection bias for this dataset \n",
    "\n",
    "# the right distibution of users ratings should be selected with trial and error\n",
    "\n",
    "\n",
    "# Note: in the netflix data, the distribution of nof user ratings does not change for users tested and users not tested\n",
    "# this should be mimicked with this data\n",
    "\n",
    "\n",
    "\n",
    "SEED_INT = 0\n",
    "# Seed for consistent results across runtimes:\n",
    "random.seed(SEED_INT)\n",
    "\n",
    "\n",
    "def populate_names(item):\n",
    "    \"\"\"Extract names from the syntax of certain data entries:\"\"\"\n",
    "    string  = item[1:-1]\n",
    "    jsons = string.split(\"}, \")   \n",
    "    names = \"\"\n",
    "    index = 0\n",
    "    for item in jsons:\n",
    "        if(index == len(jsons)-1):\n",
    "            temp_dict = ast.literal_eval(item)\n",
    "            names+=str(temp_dict[\"name\"])\n",
    "        else:\n",
    "            temp_dict = ast.literal_eval(item+\"}\")\n",
    "            names+=str(str(temp_dict[\"name\"])+\" \")\n",
    "        index += 1\n",
    "    return names\n",
    "\n",
    "\n",
    "def provide_data(row):\n",
    "    \"\"\"Extract data from row of complete_list:\"\"\"\n",
    "    movie_data = []\n",
    "    movie_data.append(int(row[0]))\n",
    "    movie_data.append(int(row[1]))\n",
    "    movie_data.append(float(row[2]))\n",
    "    movie_data.append(row[3])  \n",
    "\n",
    "    movie_data.append(populate_names(row[4]))\n",
    "    movie_data.append(populate_names(row[5]))\n",
    "    movie_data.append(populate_names(row[6]))\n",
    "    movie_data.append(populate_names(row[7]))\n",
    "\n",
    "    movie_data.append(str(row[8]))\n",
    "    movie_data.append(str(row[9]))\n",
    "    return movie_data\n",
    "    \n",
    "\n",
    "\n",
    "# The list of rows with users id, the users rating for the movie, and raw data for the movie:\n",
    "# Note: It is sorted by user_id.\n",
    "complete_list = complete_df.values.tolist()\n",
    "\n",
    "print(\"Complete number of users:\", len(list(complete_df[\"userId\"].unique()))) # 260788\n",
    "\n",
    "# The complete list of user rows without ratings of the same movie more than once for a given user:\n",
    "complete_list_no_dups = []\n",
    "\n",
    "# Distinquish the user the row belongs to:\n",
    "last_id = complete_list[0][0]\n",
    "\n",
    "# The set of movies that a user has rated:\n",
    "# It is used to omit later ratings of a movie that the user has already rated.\n",
    "movie_set = set()\n",
    "\n",
    "# The number of rows of movie data a single user takes up for each user:\n",
    "gaps = []\n",
    "\n",
    "# Appended to gaps when all of a users rows of movie data have been counted:\n",
    "gap_len = 0\n",
    "\n",
    "\n",
    "# Populates gaps and complete_list_no_dups by omitting movies that already have a rating in respect to each user:\n",
    "# Note: This code is faster than using dataframe methods.\n",
    "# Example: Filter data by user and then remove duplicate movie ids for each user.\n",
    "# This avoids slow dataframe iteration, but the filter method is also slow.\n",
    "for row in complete_list:\n",
    "    if last_id != row[0]:\n",
    "        movie_set= set()\n",
    "        complete_list_no_dups.append(row)\n",
    "        movie_set.add(row[1])\n",
    "        gaps.append(gap_len)\n",
    "        gap_len = 1\n",
    "    else:\n",
    "        if row[1] not in movie_set:\n",
    "            complete_list_no_dups.append(row)\n",
    "            gap_len+=1\n",
    "            movie_set.add(row[1])\n",
    "    last_id = row[0]\n",
    "\n",
    "# Add the last gap_len:\n",
    "gaps.append(gap_len)\n",
    "\n",
    "\n",
    "\n",
    "# Index in the complete_list_no_dups list:\n",
    "full_index = 0 \n",
    "bounds = [] \n",
    "\n",
    "\n",
    "\n",
    "# Populates bounds_train and bounds_test by testing each user if they are a valid train or test user:\n",
    "for user_index in range(len(gaps)):\n",
    "    bounds.append([full_index, full_index+gaps[user_index]])\n",
    "    full_index+=gaps[user_index]    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Normal sampling:\n",
    "# random.shuffle(bounds)\n",
    "# bounds_train = bounds[0:800]\n",
    "# bounds_test = bounds[800:1200]\n",
    "\n",
    "\n",
    "#Cherry picking train users:\n",
    "#Note: The number of train ratings wont be normally distributed between 30 and 50 \n",
    "#it will lean heavily towards 30\n",
    "# random.shuffle(bounds)\n",
    "# bounds_test = bounds[0:800]\n",
    "# bounds_train = []\n",
    "# for item in bounds[800:]:\n",
    "#     if item[1]-item[0] >=30 and item[1]-item[0] <=40:\n",
    "#         bounds_train.append(item)\n",
    "#         if len(bounds_train) == 800:\n",
    "#             print(\"met\")\n",
    "#             break\n",
    "            \n",
    "\n",
    "#cherry picking train and test users:\n",
    "random.shuffle(bounds)\n",
    "nof_train_users = 1400\n",
    "nof_test_users = 400\n",
    "last_index = -1\n",
    "bounds_train = []\n",
    "bounds_test = []\n",
    "\n",
    "index = 0\n",
    "for item in bounds:\n",
    "    # keep this if trainbounds are required\n",
    "    if item[1]-item[0] >=20 and item[1]-item[0] <=30:\n",
    "        bounds_train.append(item)\n",
    "        if len(bounds_train) == nof_train_users:\n",
    "            last_index = index\n",
    "            break\n",
    "    index+=1\n",
    "\n",
    "for item in bounds[last_index+1:]:\n",
    "    if item[1]-item[0] >=5 and item[1]-item[0] <=10:\n",
    "        bounds_test.append(item)\n",
    "        if len(bounds_test) == nof_test_users:\n",
    "            break\n",
    "\n",
    "\n",
    "# Transformed data of the selected train users and test users (in that order):\n",
    "sampled_data = []\n",
    "\n",
    "\n",
    "for bound in bounds_train:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        sampled_data.append(movie_data)\n",
    "\n",
    "\n",
    "\n",
    "for bound in bounds_test:\n",
    "    for movie in complete_list_no_dups[bound[0]:bound[1]]:\n",
    "        movie_data = provide_data(movie)\n",
    "        sampled_data.append(movie_data)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Minutes taken:\", (time.time()-start_time)/60)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data\n",
    "\n",
    "Save selected data in constructed_data.csv file so that cells below it can run without running this cell and above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "final_directory = os.path.join(current_directory, 'constructed_data')\n",
    "if not os.path.exists(final_directory):\n",
    "   os.makedirs(final_directory)\n",
    "\n",
    "with open(\"constructed_data/constructed_data_2.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['userId','id','rating',\"title\", \"genres\",\"production_companies\",\"keywords\", \"cast\", \"tagline\", \"overview\"])\n",
    "    writer.writerows(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "This is the starting cell to run if the data is already saved to the constructed_data.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data_list =[]\n",
    "\n",
    "with open(\"constructed_data/constructed_data_2.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    data_list = list(csv_reader)\n",
    "\n",
    "data_list = data_list[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format and re-sample Data:\n",
    "\n",
    "Format the data into a list of movie data rows for each movie rated for the user for each user. Then, select a subset of that data for each user type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "SEED_INT = 0\n",
    "\n",
    "random.seed(SEED_INT)\n",
    "\n",
    "user_to_data_train = []\n",
    "user_to_data_test = []\n",
    "\n",
    "user_id = data_list[0][0]\n",
    "ratings = []\n",
    "user_index = 0\n",
    "\n",
    "\n",
    "for row in data_list:\n",
    "    if (row[0]!=user_id):\n",
    "        if(user_index<1400):\n",
    "            user_to_data_train.append(ratings)\n",
    "        else:\n",
    "            user_to_data_test.append(ratings)\n",
    "        user_id = row[0]\n",
    "        ratings = [row]\n",
    "        user_index+=1\n",
    "    else:\n",
    "        ratings.append(row)\n",
    "\n",
    "user_to_data_test.append(ratings)\n",
    "\n",
    "\n",
    "#sample and relabel user and movie indices\n",
    "user_to_data_train = random.sample(user_to_data_train, 1200)\n",
    "user_to_data_test = random.sample(user_to_data_test, 400)\n",
    "\n",
    "\n",
    "old_to_new  = dict()\n",
    "last_index = 0\n",
    "cnt = 0\n",
    "\n",
    "for user in user_to_data_train:\n",
    "    for row in user: \n",
    "        if(row[1] in old_to_new.keys()):\n",
    "            row[1] = old_to_new[row[1]]\n",
    "        else:\n",
    "            old_to_new[row[1]] = last_index\n",
    "            row[1] = last_index\n",
    "            last_index+=1      \n",
    "        row[0] = cnt\n",
    "    cnt+=1\n",
    "\n",
    "\n",
    "for user in user_to_data_test:\n",
    "    for row in user: \n",
    "        if(row[1] in old_to_new.keys()):\n",
    "            row[1] = old_to_new[row[1]]\n",
    "        else:\n",
    "            old_to_new[row[1]] = last_index\n",
    "            row[1] = last_index\n",
    "            last_index+=1      \n",
    "        row[0] = cnt\n",
    "    cnt+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features and Target values\n",
    "\n",
    "* The train and test version of feature 1,2, and 3 are populated and in the final cell some subset of (feature 1, 2 and 3) is used to train and test the final model.\n",
    "* The target values are ratings for each user from the randomly selected movie that they rated. They are also either train or test ratings used to train or test the the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jackson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse 1.0471745323712158\n",
      "rmse 1.0645831771455982\n",
      "rmse 1.035909899577545\n",
      "{'factor': [150], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
      "best params: [150, 30, 0.03, 0.0075]\n",
      "best seed 2\n",
      "\n",
      "cell runtime: 84.1490249633789\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import random\n",
    "from ordered_set import OrderedSet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from surprise import SVD,Dataset,Reader\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "N_VALUE = 100\n",
    "\n",
    "SEED_INT = 0\n",
    "\n",
    "# Seed for consistent results across runtimes:\n",
    "random.seed(SEED_INT)\n",
    "np.random.seed(SEED_INT)\n",
    "\n",
    "movies_order = OrderedSet()\n",
    "movie_ratings_sum_dict = dict()\n",
    "movie_ratings_count_dict = dict()\n",
    "overall_average = 0\n",
    "cnt = 0\n",
    "\n",
    "train_user_to_movie_to_rating = [] \n",
    "\n",
    "for user in user_to_data_train:\n",
    "    movie_to_rating  = dict()\n",
    "    for movie in user:\n",
    "        movies_order.add(movie[1])\n",
    "        movie_to_rating[movie[1]] = float(movie[2])\n",
    "        if(movie[1] in movie_ratings_sum_dict.keys()):\n",
    "            movie_ratings_sum_dict[movie[1]] += float(movie[2])\n",
    "            movie_ratings_count_dict[movie[1]] += 1\n",
    "        else:\n",
    "            movie_ratings_sum_dict[movie[1]] = float(movie[2])\n",
    "            movie_ratings_count_dict[movie[1]] = 1\n",
    "        overall_average+=float(movie[2])\n",
    "        cnt += 1\n",
    "    train_user_to_movie_to_rating.append(movie_to_rating)\n",
    "\n",
    "\n",
    "test_user_to_movie_to_rating = [] \n",
    "target_movie = []\n",
    "target_rating = []\n",
    "\n",
    "for user in user_to_data_test:\n",
    "    rand_num  = random.randint(0, len(user)-1)\n",
    "    index = 0\n",
    "    movie_to_rating  = dict()\n",
    "    for movie in user:\n",
    "        movies_order.add(movie[1])\n",
    "        if(index == rand_num):\n",
    "            target_movie.append(movie[1])\n",
    "            target_rating.append(float(movie[2]))\n",
    "        else:\n",
    "            if(movie[1] in movie_ratings_sum_dict.keys()):\n",
    "                movie_ratings_sum_dict[movie[1]] += float(movie[2])\n",
    "                movie_ratings_count_dict[movie[1]] += 1\n",
    "            else:\n",
    "                movie_ratings_sum_dict[movie[1]] = float(movie[2])\n",
    "                movie_ratings_count_dict[movie[1]] = 1\n",
    "            movie_to_rating[movie[1]] = float(movie[2])\n",
    "            overall_average+=float(movie[2])\n",
    "            cnt += 1\n",
    "        index+=1\n",
    "    test_user_to_movie_to_rating.append(movie_to_rating)\n",
    "\n",
    "overall_average  = overall_average/cnt\n",
    "\n",
    "movie_ratings_avg_list = []\n",
    "\n",
    "\n",
    "\n",
    "for movie in movies_order:\n",
    "    if movie in movie_ratings_sum_dict.keys():\n",
    "        movie_ratings_avg_list.append(movie_ratings_sum_dict[movie]/movie_ratings_count_dict[movie])\n",
    "    else:\n",
    "        movie_ratings_avg_list.append(overall_average)\n",
    "    \n",
    "\n",
    "train_user_averages = []\n",
    "test_user_averages = []\n",
    "for user in train_user_to_movie_to_rating:\n",
    "    if len(user)==0:\n",
    "        train_user_averages.append(overall_average)\n",
    "    else:\n",
    "        train_user_averages.append(sum([user[key] for key in user.keys()])/len(user))\n",
    "for user in test_user_to_movie_to_rating:\n",
    "    if len(user)==0:\n",
    "        test_user_averages.append(overall_average)\n",
    "    else:\n",
    "        test_user_averages.append(sum([user[key] for key in user.keys()])/len(user))\n",
    "        \n",
    "user_averages = train_user_averages + test_user_averages\n",
    "\n",
    "\n",
    "train_users_to_movie_ratings_transformed = []\n",
    "test_users_to_movie_ratings_transformed = []\n",
    "\n",
    "train_users_to_movie_ratings = []\n",
    "test_users_to_movie_ratings = []\n",
    "\n",
    "\n",
    "user_movie = []\n",
    "overall_average_scaled = 0\n",
    "cnt = 0\n",
    "\n",
    "for i in range(len(user_to_data_train)):\n",
    "    j = 0\n",
    "    lst_1 = []\n",
    "    lst_2 = []\n",
    "    for movie in movies_order: \n",
    "        if movie in train_user_to_movie_to_rating[i].keys():\n",
    "            lst_1.append(train_user_to_movie_to_rating[i][movie] - movie_ratings_avg_list[j])\n",
    "            lst_2.append(train_user_to_movie_to_rating[i][movie])\n",
    "            overall_average_scaled+=train_user_to_movie_to_rating[i][movie] - movie_ratings_avg_list[j]\n",
    "            cnt+=1\n",
    "            user_movie.append([i, j])\n",
    "        else:\n",
    "            lst_1.append(0)\n",
    "            lst_2.append(movie_ratings_avg_list[j])\n",
    "        j += 1\n",
    "    train_users_to_movie_ratings_transformed.append(lst_1)\n",
    "    train_users_to_movie_ratings.append(lst_2)\n",
    "\n",
    "target_movie_index = []\n",
    "\n",
    "for i in range(len(user_to_data_test)):\n",
    "    j = 0\n",
    "    lst_1 = []\n",
    "    lst_2 = []\n",
    "    for movie in movies_order: \n",
    "        if(target_movie[i] == movie):\n",
    "            lst_1.append(0)\n",
    "            lst_2.append(movie_ratings_avg_list[j])\n",
    "            target_movie_index.append(j)\n",
    "        elif movie in test_user_to_movie_to_rating[i].keys():\n",
    "            lst_1.append(test_user_to_movie_to_rating[i][movie] - movie_ratings_avg_list[j])\n",
    "            lst_2.append(test_user_to_movie_to_rating[i][movie])\n",
    "            overall_average_scaled+=test_user_to_movie_to_rating[i][movie] - movie_ratings_avg_list[j]\n",
    "            cnt+=1\n",
    "            user_movie.append([len(user_to_data_train)+i, j])\n",
    "        else:\n",
    "            lst_1.append(0)\n",
    "            lst_2.append(movie_ratings_avg_list[j])\n",
    "        j += 1\n",
    "    test_users_to_movie_ratings_transformed.append(lst_1)\n",
    "    test_users_to_movie_ratings.append(lst_2)\n",
    "\n",
    "overall_average_scaled = overall_average_scaled/cnt\n",
    "\n",
    "users_to_movie_ratings_transformed = train_users_to_movie_ratings_transformed + test_users_to_movie_ratings_transformed\n",
    "users_to_movie_ratings = train_users_to_movie_ratings + test_users_to_movie_ratings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Slower train\n",
    "def train(user_to_ratings, n, overall_average, user_movie):\n",
    "\n",
    "    rt = .02\n",
    "    lr = .005\n",
    "    epochs  = 20\n",
    "\n",
    "    R = np.array(user_to_ratings)\n",
    "    q = np.random.normal(0, .1, (R.shape[1], n))\n",
    "    p = np.random.normal(0, .1, (R.shape[0], n))\n",
    "\n",
    "    b1 = [0]*R.shape[0]\n",
    "    b2 = [0]*R.shape[1]\n",
    "\n",
    "    R_hat = np.zeros((R.shape[0], R.shape[1]))\n",
    "    eui = np.zeros((R.shape[0], R.shape[1]))\n",
    "\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for u,i in user_movie:\n",
    "            R_hat[u][i] = overall_average+b1[u]+b2[i]+np.dot(p[u],q[i])\n",
    "            eui[u][i] = R[u][i]-R_hat[u][i]\n",
    "            b1[u] += lr*(eui[u][i]- rt*b1[u])\n",
    "            b2[i] += lr*(eui[u][i]- rt*b2[i])\n",
    "            temp = lr*(eui[u][i]*q[i] -rt*p[u])\n",
    "            q[i] += lr*(eui[u][i]*p[u] -rt*q[i])\n",
    "            p[u] += temp\n",
    "\n",
    "\n",
    "    return b1, b2, p, q\n",
    "\n",
    "\n",
    "@njit\n",
    "def epoch(train_list, b1, b2, p, q, overall_average, lr, rt):\n",
    "    for row in train_list:\n",
    "        #conversions needed because numpy array converts to decimal\n",
    "        u = int(row[0])\n",
    "        i = int(row[1])\n",
    "        r = row[2]\n",
    "        \n",
    "        pred = overall_average+b1[u]+b2[i]+np.dot(p[u],q[i])\n",
    "        error = r-pred\n",
    "        b1[u] += lr*(error- rt*b1[u])\n",
    "        b2[i] += lr*(error- rt*b2[i])\n",
    "        temp = lr*(error*q[i] -rt*p[u])\n",
    "        q[i] += lr*(error*p[u] -rt*q[i])\n",
    "        p[u] += temp\n",
    "\n",
    "\n",
    "\n",
    "#Faster train\n",
    "def train_v2(train_list, n, overall_average, nof_users, nof_movies):\n",
    "\n",
    "    rt = .02\n",
    "    lr = .005\n",
    "    epochs  = 20\n",
    "    q = np.random.normal(0, .1, (nof_movies, n))\n",
    "    p = np.random.normal(0, .1, (nof_users, n))\n",
    "\n",
    "    b1 = np.zeros(nof_users)\n",
    "    b2 = np.zeros(nof_movies)\n",
    "\n",
    "    np_array = np.array(train_list)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        epoch(np_array, b1, b2, p, q, overall_average, lr, rt)\n",
    "\n",
    "    return b1, b2, p, q\n",
    "\n",
    "\n",
    "def train_v3(train_list, n, epochs, rt, lr, overall_average, nof_users, nof_movies):\n",
    "    \n",
    "    q = np.random.normal(0, .1, (nof_movies, n))\n",
    "    p = np.random.normal(0, .1, (nof_users, n))\n",
    "\n",
    "    b1 = np.zeros(nof_users)\n",
    "    b2 = np.zeros(nof_movies)\n",
    "\n",
    "    np_array = np.array(train_list)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        epoch(np_array, b1, b2, p, q, overall_average, lr, rt)\n",
    "\n",
    "    return b1, b2, p, q\n",
    "\n",
    "\n",
    "train_list = []\n",
    "\n",
    "index =0\n",
    "for user_ratings, user_dict in zip(train_users_to_movie_ratings, train_user_to_movie_to_rating):\n",
    "    for rating, movie_id in zip(user_ratings, list(movies_order)):\n",
    "        if movie_id in user_dict.keys():\n",
    "            train_list.append((index, int(movie_id), rating))\n",
    "    index+=1\n",
    "\n",
    "i = 0\n",
    "user_movies_to_predict = []\n",
    "for user_ratings, user_dict in zip(test_users_to_movie_ratings, test_user_to_movie_to_rating):\n",
    "    for rating, movie_id in zip(user_ratings, list(movies_order)):\n",
    "        if movie_id in user_dict.keys():\n",
    "            train_list.append((index, int(movie_id), rating))\n",
    "        elif(movie_id == target_movie[i]):\n",
    "            user_movies_to_predict.append((index, int(movie_id)))\n",
    "    index+=1\n",
    "    i+=1\n",
    "\n",
    "random.shuffle(train_list)\n",
    "\n",
    "reader = Reader()\n",
    "train_df = pd.DataFrame(train_list, columns=['userId', 'movieId', 'rating'])\n",
    "dataset = Dataset.load_from_df(train_df, reader)\n",
    "trainset = dataset.build_full_trainset()\n",
    "\n",
    "svd_model = SVD(random_state = SEED_INT)\n",
    "svd_model_trained = svd_model.fit(trainset)\n",
    "\n",
    "predictions_0 = []\n",
    "\n",
    "for item in user_movies_to_predict:\n",
    "    predictions_0.append(svd_model_trained.predict(item[0], item[1], verbose=False).est)\n",
    "\n",
    "\n",
    "print(\"rmse\",mean_squared_error(target_rating , predictions_0, squared = False))\n",
    "# print(r2_score(target_rating , predictions_0))\n",
    "\n",
    "\n",
    "#normalized version:\n",
    "# time_start_3 = time.time()\n",
    "# b1, b2, p, q = train(users_to_movie_ratings_transformed, N_VALUE, overall_average_scaled, user_movie)\n",
    "# predictions_1 = [movie_ratings_avg_list[target_movie_index[u]]+ overall_average_scaled \n",
    "#                  + b1[len(user_to_data_train)+u]+b2[target_movie_index[u]]\n",
    "#                  +np.dot(p[len(user_to_data_train)+u],q[target_movie_index[u]]) for u in range(len(user_to_data_test))]\n",
    "\n",
    "# print(mean_absolute_error(target_rating , predictions_1))\n",
    "# print(mean_squared_error(target_rating , predictions_1, squared = False))\n",
    "# # print(r2_score(target_rating , predictions_1))\n",
    "# print(time.time()- time_start_3)\n",
    "\n",
    "\n",
    "#non-normalized version:\n",
    "#normalization does not improve performance\n",
    "# time_start_4 = time.time()\n",
    "# b1, b2, p, q = train(users_to_movie_ratings, N_VALUE, overall_average, user_movie)\n",
    "# predictions_2 = [overall_average + b1[len(user_to_data_train)+u]+b2[target_movie_index[u]]\n",
    "#                  +np.dot(p[len(user_to_data_train)+u],q[target_movie_index[u]]) for u in range(len(user_to_data_test))]\n",
    "\n",
    "# print(mean_absolute_error(target_rating , predictions_2))\n",
    "# print(mean_squared_error(target_rating , predictions_2, squared = False))\n",
    "# # print(r2_score(target_rating , predictions_2))\n",
    "# print(time.time() - time_start_4)\n",
    "\n",
    "\n",
    "b1, b2, p, q = train_v2(train_list, N_VALUE, overall_average, len(users_to_movie_ratings), len(list(movies_order)))\n",
    "predictions_3 = [overall_average + b1[pair[0]]+b2[pair[1]]\n",
    "                 +np.dot(p[pair[0]],q[pair[1]]) for pair in user_movies_to_predict]\n",
    "print(\"rmse\", mean_squared_error(target_rating , predictions_3, squared = False))\n",
    "\n",
    "#now introduce a version that optimizes with these parameters\n",
    "# rt, lr , epochs , factors\n",
    "\n",
    "# this is the first combination of hyperparameters:\n",
    "factors = [50, 100, 150]\n",
    "epochs = [10, 20, 30]\n",
    "rts = [.01, .02, .03]\n",
    "lrs = [.0025, .005, .0075]\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {'factor': factors, 'nof_epoch': epochs, 'rt': rts, 'lr': lrs}\n",
    "direction = {'factor': None, 'nof_epoch': None, 'rt': None, 'lr': None}\n",
    "\n",
    "best_seed = None\n",
    "\n",
    "best_error = np.inf \n",
    "\n",
    "best_params = []\n",
    "\n",
    "\n",
    "for _ in range(4):\n",
    "    grid = ParameterGrid(param_grid)\n",
    "\n",
    "    if len(grid)== 1:\n",
    "        break\n",
    "\n",
    "    for params in grid:\n",
    "        for j in range(0,3):\n",
    "            random.seed(j)\n",
    "            np.random.seed(j)\n",
    "\n",
    "            b1, b2, p, q = train_v3(train_list, params[\"factor\"],params[\"nof_epoch\"],params[\"rt\"],params[\"lr\"],\n",
    "                                    overall_average, len(users_to_movie_ratings), len(list(movies_order)))\n",
    "            predictions_4 = [overall_average + b1[pair[0]]+b2[pair[1]]\n",
    "                            +np.dot(p[pair[0]],q[pair[1]]) for pair in user_movies_to_predict]\n",
    "            \n",
    "\n",
    "            current_error = mean_squared_error(target_rating , predictions_4, squared = False)\n",
    "            if(best_error > current_error):\n",
    "                best_error = current_error\n",
    "                best_params = [params[\"factor\"],params[\"nof_epoch\"],params[\"rt\"],params[\"lr\"]]\n",
    "                best_seed = j\n",
    "\n",
    "\n",
    "\n",
    "    for param, key in zip(best_params, param_grid.keys()):\n",
    "        if len(param_grid[key])==3:\n",
    "            if(param == param_grid[key][0]):\n",
    "                param_grid[key] = [param/1.5,param]\n",
    "                direction[key] = \"left\"\n",
    "            elif(param == param_grid[key][1]):\n",
    "                param_grid[key] = [param]\n",
    "                direction[key] = None\n",
    "            elif(param == param_grid[key][2]):\n",
    "                param_grid[key] = [param, param*1.5]\n",
    "                direction[key] = \"right\"\n",
    "        \n",
    "        if len(param_grid[key])==2:\n",
    "            if direction[key] == \"left\":\n",
    "                if param == param_grid[key][0]:\n",
    "                    param_grid[key] = [param/1.5,param]\n",
    "                    direction[key] = \"left\"\n",
    "                elif param == param_grid[key][1]:\n",
    "                    param_grid[key] = [param]\n",
    "                    direction[key] = None\n",
    "            elif direction[key] == \"right\":\n",
    "                if param == param_grid[key][1]:\n",
    "                    param_grid[key] = [param,param*1.5]\n",
    "                    direction[key] = \"right\"\n",
    "                elif param == param_grid[key][0]:               \n",
    "                    param_grid[key] = [param]\n",
    "                    direction[key] = None\n",
    "     \n",
    "\n",
    "print(\"rmse\", best_error)\n",
    "print(param_grid)\n",
    "print(\"best params:\", best_params)\n",
    "print(\"best seed\", best_seed)\n",
    "print()\n",
    "\n",
    "# print(r2_score(target_rating , predictions_3))\n",
    "\n",
    "\n",
    "print(\"cell runtime:\", time.time()- start_time)\n",
    "\n",
    "\n",
    "\n",
    "#LOOK: The point of this entire notebook should be to find the best hyper parameters and then they can be used inthe final model.\n",
    "\n",
    "#LOOK: \n",
    "#sample code: https://www.kaggle.com/code/ankitahankare/collaborative-filtering-based-recommender-system\n",
    "#A difference between his and my implementation is that in mine the target values for test users are never included in training\n",
    "#this is simply because, in a realistic implementation of the model the users dont have these ratings\n",
    "#Another difference is that the target ratings in his are randomly chosen (user,movie) pairs, where in mine they are one per test user\n",
    "\n",
    "\n",
    "#LOOK: compiler optimizations???:\n",
    "\n",
    "\n",
    "#need a way to optimize this list of parameters\n",
    "# rt, lr , epochs , factors \n",
    "# bounds for the nof train users\n",
    "# bounds for the nof test users\n",
    "# nof train users\n",
    "# nof test users\n",
    "# seed_int\n",
    "# hyper parameter tuning: https://www.analyticsvidhya.com/blog/2022/02/a-comprehensive-guide-on-hyperparameter-tuning-and-its-techniques/\n",
    "# estimator: https://scikit-learn.org/stable/developers/develop.html\n",
    "\n",
    "\n",
    "# LOOK: Test number of train ratings in increments of 50...\n",
    "# LOOK: problem... finding the best seed helps with the init of p and q but it also...\n",
    "# may create a bias with the specific test data rather than general data\n",
    "# this bias is relinguishee when test data gets larger\n",
    "\n",
    "\n",
    "#LOOK: the nof test users actually matters alot in this notebook\n",
    "\n",
    "\n",
    "# train: 30-40, test: 5-10\n",
    "# 600 train 800 test\n",
    "# rmse 1.0285562949958984\n",
    "# rmse 1.0298450523327378\n",
    "# rmse 1.0171920974614017\n",
    "# {'factor': [50], 'nof_epoch': [20], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [50, 20, 0.03, 0.0075]\n",
    "# best seed 1\n",
    "\n",
    "# 800 train 800 test\n",
    "# rmse 1.0216396731064734\n",
    "# rmse 1.02912853453398\n",
    "# rmse 0.9992378859916795\n",
    "# {'factor': [50], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [50, 30, 0.03, 0.0075]\n",
    "# best seed 0\n",
    "\n",
    "# 1000 train 800 test\n",
    "# rmse 1.0410596238370267\n",
    "# rmse 1.0366831532021172\n",
    "# rmse 1.0308335939737594\n",
    "# {'factor': [100], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.005]}\n",
    "# best params: [100, 30, 0.03, 0.005]\n",
    "# best seed 0\n",
    "\n",
    "\n",
    "# train: 40-50, test: 5-10\n",
    "# 600 train 800 test\n",
    "# rmse 1.0843314763291703\n",
    "# rmse 1.073772719644768\n",
    "# rmse 1.0609136230467695\n",
    "# {'factor': [100], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [100, 30, 0.03, 0.0075]\n",
    "# best seed 0\n",
    "\n",
    "# 800 train 800 test\n",
    "# rmse 1.0342831232727059\n",
    "# rmse 1.038799933169063\n",
    "# rmse 1.0148020092992955\n",
    "# {'factor': [100], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [100, 30, 0.03, 0.0075]\n",
    "# best seed 1\n",
    "\n",
    "# 1000 train 800 test\n",
    "# rmse 1.1124632720416288\n",
    "# rmse 1.1124768160972383\n",
    "# rmse 1.089061849194386\n",
    "# {'factor': [150], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [150, 30, 0.03, 0.0075]\n",
    "# best seed 1\n",
    "\n",
    "\n",
    "# train: 20-30, test: 5-10\n",
    "# 600 train 800 test\n",
    "# rmse 1.080531756985698\n",
    "# rmse 1.0717684526411904\n",
    "# rmse 1.0633037451901133\n",
    "# {'factor': [150], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [150, 30, 0.03, 0.0075]\n",
    "# best seed 1\n",
    "\n",
    "# 800 train 800 test\n",
    "# rmse 1.0337050093038436\n",
    "# rmse 1.0392269125002738\n",
    "# rmse 1.017842620676386\n",
    "# {'factor': [50], 'nof_epoch': [20], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [50, 20, 0.03, 0.0075]\n",
    "# best seed 2\n",
    "\n",
    "# 1000 train 800 test\n",
    "# rmse 0.9922240015842242\n",
    "# rmse 0.984335042573686\n",
    "# rmse 0.969646265617763\n",
    "# {'factor': [50], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [50, 30, 0.03, 0.0075]\n",
    "# best seed 0\n",
    "\n",
    "\n",
    "# test: 5-10\n",
    "# 600 train 800 test\n",
    "# rmse 1.0349265114936685\n",
    "# rmse 1.0349413414401287\n",
    "# rmse 1.0255965085938568\n",
    "# {'factor': [50], 'nof_epoch': [20], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [50, 20, 0.03, 0.0075]\n",
    "# best seed 0\n",
    "\n",
    "# 800 train 800 test\n",
    "# rmse 1.0687741467278165\n",
    "# rmse 1.0757357704893762\n",
    "# rmse 1.0565687644970598\n",
    "# {'factor': [100], 'nof_epoch': [20], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [100, 20, 0.03, 0.0075]\n",
    "# best seed 1\n",
    "\n",
    "# 1000 train 800 test\n",
    "# rmse 1.0228895862175953\n",
    "# rmse 1.0215296457628693\n",
    "# rmse 1.0035794333443069\n",
    "# {'factor': [50], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [50, 30, 0.03, 0.0075]\n",
    "# best seed 1\n",
    "\n",
    "\n",
    "# test: 5-10 train 10-20\n",
    "# 600 train 800 test\n",
    "# rmse 1.0554163955517797\n",
    "# rmse 1.0512485348587202\n",
    "# rmse 1.0409988362019327\n",
    "# {'factor': [100], 'nof_epoch': [20], 'rt': [0.02], 'lr': [0.0075]}\n",
    "# best params: [100, 20, 0.02, 0.0075]\n",
    "# best seed 2\n",
    "\n",
    "# 800 train 800 test\n",
    "# rmse 1.0880280898716237\n",
    "# rmse 1.0979012183414683\n",
    "# rmse 1.0764857155928829\n",
    "# {'factor': [100], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [100, 30, 0.03, 0.0075]\n",
    "# best seed 2\n",
    "\n",
    "# 1000 train 800 test\n",
    "# rmse 1.0545327372157678\n",
    "# rmse 1.0501710861392046\n",
    "# rmse 1.0368243876960928\n",
    "# {'factor': [100], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.005]}\n",
    "# best params: [100, 30, 0.03, 0.005]\n",
    "# best seed 1\n",
    "\n",
    "\n",
    "# test: 5-10 train 25-35\n",
    "# 600 train 800 test\n",
    "# rmse 1.0241811428451262\n",
    "# rmse 1.0310511419318278\n",
    "# rmse 1.0060674892082313\n",
    "# {'factor': [50], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [50, 30, 0.03, 0.0075]\n",
    "# best seed 1\n",
    "\n",
    "# 800 train 800 test\n",
    "# rmse 1.0241811428451262\n",
    "# rmse 1.0310511419318278\n",
    "# rmse 1.0060674892082313\n",
    "# {'factor': [50], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [50, 30, 0.03, 0.0075]\n",
    "# best seed 1\n",
    "\n",
    "# 1000 train 800 test\n",
    "# rmse 1.0744857213514982\n",
    "# rmse 1.0700793823523764\n",
    "# rmse 1.0547991259599134\n",
    "# {'factor': [150], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [150, 30, 0.03, 0.0075]\n",
    "# best seed 2\n",
    "\n",
    "\n",
    "# rmse 0.9742081570728105\n",
    "# rmse 0.9755648980583311\n",
    "# rmse 0.9491276901725388\n",
    "# {'factor': [150], 'nof_epoch': [30], 'rt': [0.03], 'lr': [0.0075]}\n",
    "# best params: [150, 30, 0.03, 0.0075]\n",
    "# best seed 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
