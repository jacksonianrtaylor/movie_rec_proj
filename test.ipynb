{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "movies_order_test = copy.deepcopy(movies_order_svd)\n",
    "overall_average_test = overall_average_svd \n",
    "cnt_test = cnt_svd\n",
    "test_rating_to_predict = []\n",
    "\n",
    "for user in user_to_data_test:\n",
    "    rand_num  = random.randint(0, len(user)-1)\n",
    "    index = 0\n",
    "    user_rating_sum = 0\n",
    "    usr_rating_count = 0\n",
    "\n",
    "\n",
    "    #These three are used to build feature_2_test\n",
    "    words = OrderedSet()\n",
    "    list_of_movie_to_count_dict = []\n",
    "    target_word_counts = dict()\n",
    "    non_target_ratings = []\n",
    "\n",
    "    for movie in user:\n",
    "\n",
    "        movies_order_test.add(movie[1])\n",
    "\n",
    "        #corpus extraction:\n",
    "        movie_string = \"\"\n",
    "        for j in range (3,len(movie)):\n",
    "            if(j!= len(movie)-1):\n",
    "                movie_string+= movie[j]+\" \"\n",
    "            else:\n",
    "                movie_string+= movie[j]\n",
    "\n",
    "        #LOOK: this declaration may need to be outside the loop\n",
    "        wnl = WordNetLemmatizer()\n",
    "        cleaned_string = remove_stopwords(movie_string)\n",
    "        cleaned_list = [wnl.lemmatize(word) for word in cleaned_string.split(\" \")]\n",
    "        cleaned_list = [word[:-1] for word in cleaned_list if word.endswith(\".\")] + [word for word in cleaned_list if not word.endswith(\".\")]\n",
    "\n",
    "\n",
    "        #For each user there needs to be an ordered set of words/terms in movies...\n",
    "        words.update(cleaned_list)\n",
    "\n",
    "        if(index == rand_num):\n",
    "            test_rating_to_predict.append([int(movie[0]), int(movie[1])])\n",
    "            target_rating_test.append(float(movie[2]))\n",
    "\n",
    "            #for every movie there needs to be a dictionary of words to counts\n",
    "            target_word_counts = Counter(cleaned_list)\n",
    "        else:\n",
    "            overall_average_test+=float(movie[2])\n",
    "            cnt_test += 1\n",
    "            user_rating_sum+=float(movie[2])\n",
    "            usr_rating_count +=1\n",
    "            test_list.append([int(movie[0]), int(movie[1]), float(movie[2])])\n",
    "\n",
    "            #for every movie there needs to be a dictionary of words to counts\n",
    "            list_of_movie_to_count_dict.append(Counter(cleaned_list))\n",
    "            non_target_ratings.append(float(movie[2]))\n",
    "\n",
    "        index+=1\n",
    "\n",
    "    #LOOK: Now predict with average ratings:\n",
    "    if(user_rating_sum==0):\n",
    "        feature_1_test.append(-1)\n",
    "    else: \n",
    "        feature_1_test.append(user_rating_sum/usr_rating_count)\n",
    "\n",
    "    #LOOK: now predict with weighted rating...\n",
    "    #LOOK: May want to use list comprehension here...\n",
    "    complete_word_counts_in_order = []\n",
    "    target_word_counts_in_order = []\n",
    "\n",
    "    for temp_dict in list_of_movie_to_count_dict:\n",
    "        word_count = []\n",
    "        for movie in words:\n",
    "            if movie in temp_dict.keys():\n",
    "                word_count.append(temp_dict[movie])\n",
    "            else:\n",
    "                word_count.append(0)\n",
    "        complete_word_counts_in_order.append(word_count)\n",
    "\n",
    "    for movie in words:\n",
    "        if movie in target_word_counts.keys():\n",
    "            target_word_counts_in_order.append(target_word_counts[movie])\n",
    "        else:\n",
    "            target_word_counts_in_order.append(0)\n",
    "\n",
    "\n",
    "    complete_word_counts = complete_word_counts_in_order.copy()\n",
    "    complete_word_counts.append(target_word_counts_in_order)\n",
    "    transformed_word_counts = TfidfTransformer().fit_transform(complete_word_counts).toarray()\n",
    "\n",
    "\n",
    "    cosine_sim = linear_kernel(X = transformed_word_counts[0:-1],Y = [transformed_word_counts[-1]])\n",
    "    cosine_sim = np.reshape(cosine_sim,  (len(cosine_sim)))\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "\n",
    "\n",
    "    #LOOK: are cosine_sim and non_target_ratings in the same order???(yes)\n",
    "    for i in range(len(non_target_ratings)):\n",
    "        numerator += cosine_sim[i]*non_target_ratings[i]\n",
    "        denominator += cosine_sim[i]\n",
    "\n",
    "    # populate feature_2_test handeling fringe cases\n",
    "    if denominator == 0:\n",
    "        if(feature_1_test[-1] == -1):\n",
    "            if(overall_average_test == 0):\n",
    "                feature_2_test.append(3)\n",
    "            else:\n",
    "                feature_2_test.append(overall_average_test)\n",
    "        else:\n",
    "            feature_2_test.append(feature_1_test[-1])\n",
    "    else:\n",
    "        feature_2_test.append(numerator/denominator)\n",
    "\n",
    "\n",
    "\n",
    "overall_average_test = overall_average_test/cnt_test\n",
    "\n",
    "for i in range(len(feature_1_test)):\n",
    "    if feature_1_test[i] ==-1:\n",
    "        feature_1_test[i] = overall_average_test\n",
    "\n",
    "random.shuffle(test_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
